From a5bb159e37d76e079de75045d4ea5cb725ac1b82 Mon Sep 17 00:00:00 2001
From: Yan Markman <ymarkman@marvell.com>
Date: Mon, 15 Oct 2018 14:52:27 +0300
Subject: [PATCH 0726/1051] net: mvpp2: rx Icache optimization by splitting
 budget loop

An original MVPP2 RX driver procedure has a loop for up to
NAPI_POLL_WEIGHT (N) packets, making:
- fetch data from hw,
- build SKB,
- call for napi_gro_receive net-stack procedure.
The whole sequence is done PER-PACKET.
The napi_gro_receive stack has a lot of code (especially for TCP)
and with high probability Instruction-Cache could be overloaded
which leads to performance downgrade.

Let's loop over all N received packets with fetch/build
SKBs but with saving SKBs without napi_gro_receive (loop-1)
After this call for napi_gro_receive in another loop (loop-2)

This guaranties the I-cache consistency over all N packets in loop-1
and then in loop-2.

Change-Id: Icb68c72ad6246dbdc2116fa6fa768a4a92c91481
Signed-off-by: Yan Markman <ymarkman@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/60518
Reviewed-by: Igal Liberman <igall@marvell.com>
Tested-by: iSoC Platform CI <ykjenk@marvell.com>
[Kevin: The original patch got from Marvell sdk10.0_19.06]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c | 12 +++++++-----
 1 file changed, 7 insertions(+), 5 deletions(-)

diff --git a/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c b/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
index 89e8d92e8213..77a16c9be063 100644
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
@@ -3427,8 +3427,9 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 	struct net_device *dev = port->dev;
 	int rx_received;
 	int rx_done = 0;
-	u32 rcvd_pkts = 0;
+	u32 rcvd_pkts = 0, i = 0;
 	u32 rcvd_bytes = 0;
+	struct sk_buff *skb_all[rx_todo];
 
 	/* Get number of received packets and clamp the to-do */
 	rx_received = mvpp2_rxq_received(port, rxq->id);
@@ -3503,17 +3504,18 @@ static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
 			goto err_drop_frame;
 		}
 refill_done:
-		rcvd_pkts++;
-		rcvd_bytes += rx_bytes;
-
 		skb_reserve(skb, MVPP2_MH_SIZE + NET_SKB_PAD);
 		skb_put(skb, rx_bytes);
 		skb->protocol = eth_type_trans(skb, dev);
 		mvpp2_rx_csum(port, rx_status, skb);
 
-		napi_gro_receive(napi, skb);
+		skb_all[rcvd_pkts++] = skb;
+		rcvd_bytes += rx_bytes;
 	}
 
+	while (i < rcvd_pkts)
+		napi_gro_receive(napi, skb_all[i++]);
+
 	if (rcvd_pkts) {
 		struct mvpp2_pcpu_stats *stats = this_cpu_ptr(port->stats);
 
-- 
2.17.1

