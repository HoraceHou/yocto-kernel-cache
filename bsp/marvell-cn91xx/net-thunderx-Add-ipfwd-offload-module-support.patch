From 4119e8883ab081b2c3f5803a8b92c3e42b275997 Mon Sep 17 00:00:00 2001
From: Abhijit Ayarekar <abhijit.ayarekar@caviumnetworks.com>
Date: Fri, 19 Oct 2018 10:05:13 +0300
Subject: [PATCH 0329/1051] net: thunderx: Add ipfwd offload module support.

Signed-off-by: Abhijit Ayarekar <abhijit.ayarekar@caviumnetworks.com>
Signed-off-by: Yury Norov <ynorov@marvell.com>
Signed-off-by: Yury Norov <ynorov@caviumnetworks.com>
[Kevin: The original patch got from Marvell sdk10.0_19.06.
Just some minor context mods in order to port to wrlinux]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 .../net/ethernet/cavium/thunder/nicvf_main.c  | 22 +++++++
 .../ethernet/cavium/thunder/nicvf_queues.c    | 60 ++++++++++++++++++-
 .../ethernet/cavium/thunder/nicvf_queues.h    | 14 +++++
 3 files changed, 95 insertions(+), 1 deletion(-)

diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_main.c b/drivers/net/ethernet/cavium/thunder/nicvf_main.c
index a08ca75194e3..91680420e15f 100644
--- a/drivers/net/ethernet/cavium/thunder/nicvf_main.c
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_main.c
@@ -881,6 +881,25 @@ static void nicvf_rcv_pkt_handler(struct net_device *netdev,
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
 				       ntohs((__force __be16)cqe_rx->vlan_tci));
 
+#ifdef CONFIG_CAVIUM_IPFWD_OFFLOAD
+	/* apply rps for packets that do not have l4 hash */
+#ifdef CONFIG_RPS
+	if (!skb->l4_hash && static_key_false(&rps_needed)) {
+		struct rps_dev_flow voidflow, *rflow = &voidflow;
+		int cpu = get_rps_cpu(skb->dev, skb, &rflow);
+
+		if (cpu >= 0) {
+			enqueue_to_backlog(skb, cpu, &rflow->last_qtail);
+			return;
+		}
+	}
+#endif
+	if (cvm_ipfwd_rx_hook) {
+		if (!cvm_ipfwd_rx_hook(skb))
+			return;
+	}
+#endif
+
 	if (napi && (netdev->features & NETIF_F_GRO))
 		napi_gro_receive(napi, skb);
 	else
@@ -2326,6 +2345,9 @@ static int nicvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 		if (!err && i < NIC_MAX_PKIND)
 			netdev_ethx[i] = netdev;
 	}
+#ifdef CONFIG_CAVIUM_IPFWD_OFFLOAD
+	netdev->is_cvm_dev = 1;
+#endif
 	return 0;
 
 err_unregister_interrupts:
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_queues.c b/drivers/net/ethernet/cavium/thunder/nicvf_queues.c
index 7b2a5f1bd717..52ea0d591006 100644
--- a/drivers/net/ethernet/cavium/thunder/nicvf_queues.c
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_queues.c
@@ -213,10 +213,19 @@ static inline int nicvf_alloc_rcv_buffer(struct nicvf *nic, struct rbdr *rbdr,
 		*rbuf = pgcache->dma_addr;
 	} else {
 		/* HW will ensure data coherency, CPU sync not required */
+
+#ifdef CONFIG_CAVIUM_IPFWD_OFFLOAD
+		*rbuf = (u64)dma_map_page_attrs(&nic->pdev->dev, nic->rb_page,
+			nic->rb_page_offset + CAVIUM_IPFWD_OFFLOAD_HEADROOM,
+						buf_len,
+						DMA_FROM_DEVICE,
+						DMA_ATTR_SKIP_CPU_SYNC);
+#else
 		*rbuf = (u64)dma_map_page_attrs(&nic->pdev->dev, nic->rb_page,
 						nic->rb_page_offset, buf_len,
 						DMA_FROM_DEVICE,
 						DMA_ATTR_SKIP_CPU_SYNC);
+#endif
 		if (dma_mapping_error(&nic->pdev->dev, (dma_addr_t)*rbuf)) {
 			if (!nic->rb_page_offset)
 				__free_pages(nic->rb_page, 0);
@@ -240,6 +249,14 @@ static struct sk_buff *nicvf_rb_ptr_to_skb(struct nicvf *nic,
 
 	data = phys_to_virt(rb_ptr);
 
+#ifdef CONFIG_CAVIUM_IPFWD_OFFLOAD
+	/* This extra headroom has been allocated in nicvf_alloc_rcv_buffer
+	 * and memory pointer has been moved up already.
+	 * Move the pointer down so extra headroom can be usd.
+	 */
+	data -= CAVIUM_IPFWD_OFFLOAD_HEADROOM;
+#endif
+
 	/* Now build an skb to give to stack */
 	skb = build_skb(data, RCV_FRAG_LEN);
 	if (!skb) {
@@ -247,6 +264,10 @@ static struct sk_buff *nicvf_rb_ptr_to_skb(struct nicvf *nic,
 		return NULL;
 	}
 
+#ifdef CONFIG_CAVIUM_IPFWD_OFFLOAD
+	skb_reserve(skb, CAVIUM_IPFWD_OFFLOAD_HEADROOM);
+#endif
+
 	prefetch(skb->data);
 	return skb;
 }
@@ -751,6 +772,14 @@ static void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,
 	struct rcv_queue *rq;
 	struct rq_cfg rq_cfg;
 
+#ifdef CONFIG_CAVIUM_IPFWD_OFFLOAD
+#ifdef CONFIG_RPS
+	struct netdev_rx_queue *rxqueue;
+	struct rps_map *map, *oldmap;
+	struct cpumask rps_mask;
+	int i, cpu;
+#endif
+#endif
 	rq = &qs->rq[qidx];
 	rq->enable = enable;
 
@@ -805,7 +834,7 @@ static void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,
 		 * Also allow IPv6 pkts with zero UDP checksum.
 		 */
 		nicvf_queue_reg_write(nic, NIC_QSET_RQ_GEN_CFG, 0,
-				      (BIT(24) | BIT(23) | BIT(21) | BIT(20)));
+				(BIT(24) | BIT(23) | BIT(21) | BIT(20)));
 		nicvf_config_vlan_stripping(nic, nic->netdev->features);
 	}
 
@@ -814,6 +843,35 @@ static void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,
 	rq_cfg.ena = 1;
 	rq_cfg.tcp_ena = 0;
 	nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, *(u64 *)&rq_cfg);
+
+#ifdef CONFIG_CAVIUM_IPFWD_OFFLOAD
+#ifdef CONFIG_RPS
+	/* Set RPS CPU map */
+	cpumask_copy(&rps_mask, cpu_online_mask);
+	cpumask_clear_cpu(cpumask_first(nic->affinity_mask[0]), &rps_mask);
+
+	rxqueue = nic->netdev->_rx + qidx;
+	oldmap = rcu_dereference(rxqueue->rps_map);
+	map = kzalloc(max_t(unsigned int,
+			    RPS_MAP_SIZE(cpumask_weight(&rps_mask)),
+			    L1_CACHE_BYTES), GFP_KERNEL);
+	if (!map)
+		return;
+
+	i = 0;
+	for_each_cpu_and(cpu, &rps_mask, cpu_online_mask)
+		map->cpus[i++] = cpu;
+	map->len = i;
+
+	static_key_slow_inc(&rps_needed);
+	rcu_assign_pointer(rxqueue->rps_map, map);
+
+	if (oldmap) {
+		kfree_rcu(oldmap, rcu);
+		static_key_slow_dec(&rps_needed);
+	}
+#endif
+#endif
 }
 
 /* Configures completion queue */
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_queues.h b/drivers/net/ethernet/cavium/thunder/nicvf_queues.h
index a722f2feb1b5..fb426600e2bb 100644
--- a/drivers/net/ethernet/cavium/thunder/nicvf_queues.h
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_queues.h
@@ -90,9 +90,23 @@
 #define MAX_RCV_BUF_COUNT	(1ULL << (RBDR_SIZE6 + 13))
 #define RBDR_THRESH		(RCV_BUF_COUNT / 2)
 #define DMA_BUFFER_LEN		1536 /* In multiples of 128bytes */
+
+#ifdef CONFIG_CAVIUM_IPFWD_OFFLOAD
+
+/* Extra 128 bytes in skb headroom to accommodate protocol headers like ppp */
+#define CAVIUM_IPFWD_OFFLOAD_HEADROOM        128
+
+#define RCV_FRAG_LEN    (SKB_DATA_ALIGN(DMA_BUFFER_LEN + NET_SKB_PAD) + \
+			SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) + \
+					CAVIUM_IPFWD_OFFLOAD_HEADROOM)
+
+#else
+
 #define RCV_FRAG_LEN	 (SKB_DATA_ALIGN(DMA_BUFFER_LEN + NET_SKB_PAD) + \
 			 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
 
+#endif
+
 #define MAX_CQES_FOR_TX		((SND_QUEUE_LEN / MIN_SQ_DESC_PER_PKT_XMIT) * \
 				 MAX_CQE_PER_PKT_XMIT)
 
-- 
2.17.1

