From d66b53590b057f4f10fcdee344acf8818712d856 Mon Sep 17 00:00:00 2001
From: Geetha sowjanya <gakula@marvell.com>
Date: Mon, 13 Aug 2018 23:46:09 +0530
Subject: [PATCH 0125/1051] net: octeontx2: Support to use single pool for all
 RQs

This patch adds support to use single NPA pool for all
NIX RQs i.e all RQs NPA Auras point to same pool.
This will reduce the context related cache replacements
in NPA's NDC.

Signed-off-by: Geetha sowjanya <gakula@marvell.com>
[Kevin: The original patch got from Marvell sdk10.0_19.06]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 .../ethernet/marvell/octeontx2/otx2_common.c  | 26 ++++++++++++++-----
 .../ethernet/marvell/octeontx2/otx2_common.h  |  1 +
 .../net/ethernet/marvell/octeontx2/otx2_pf.c  |  1 +
 3 files changed, 22 insertions(+), 6 deletions(-)

diff --git a/drivers/net/ethernet/marvell/octeontx2/otx2_common.c b/drivers/net/ethernet/marvell/octeontx2/otx2_common.c
index b2580b3f85fb..9af5b11cd942 100644
--- a/drivers/net/ethernet/marvell/octeontx2/otx2_common.c
+++ b/drivers/net/ethernet/marvell/octeontx2/otx2_common.c
@@ -492,7 +492,7 @@ static int otx2_cq_init(struct otx2_nic *pfvf, u16 qidx)
 	struct otx2_qset *qset = &pfvf->qset;
 	struct nix_aq_enq_req *aq;
 	struct otx2_cq_queue *cq;
-	int err;
+	int err, pool_id;
 
 	cq = &qset->cq[qidx];
 	cq->cqe_cnt = Q_COUNT(Q_SIZE_4K);
@@ -506,6 +506,12 @@ static int otx2_cq_init(struct otx2_nic *pfvf, u16 qidx)
 	/* Save CQE CPU base for faster reference */
 	cq->cqe_base = cq->cqe->base;
 	cq->rbpool = &qset->pool[qidx];
+	/* In case where all RQs auras point to single pool,
+	 * all CQs receive buffer pool also point to same pool.
+	 */
+	pool_id = ((qidx < pfvf->hw.rx_queues) &&
+		   (pfvf->hw.rqpool_cnt != pfvf->hw.rx_queues)) ? 0 : qidx;
+	cq->rbpool = &qset->pool[pool_id];
 	cq->cq_idx = qidx;
 
 	/* Get memory to put this msg */
@@ -600,7 +606,7 @@ void otx2_free_aura_ptr(struct otx2_nic *pfvf, int type)
 	}
 	if (type == NIX_AQ_CTYPE_RQ) {
 		pool_start = 0;
-		pool_end = pfvf->hw.rx_queues;
+		pool_end = pfvf->hw.rqpool_cnt;
 	}
 
 	/* Free SQB and RQB pointers from the aura pool */
@@ -670,6 +676,11 @@ static int otx2_aura_init(struct otx2_nic *pfvf, int aura_id,
 	aq->aura.pool_caching = 1;
 	aq->aura.shift = ilog2(numptrs) - 8;
 	aq->aura.count = numptrs;
+	/* In case where all RQ's auras points to a single pool,
+	 * buffer pointers are freed to Aura 0 only.
+	 */
+	if (pool_id != aura_id)
+		aq->aura.count = 0;
 	aq->aura.limit = numptrs;
 	aq->aura.ena = 1;
 	aq->aura.fc_ena = 1;
@@ -790,8 +801,8 @@ int otx2_sq_aura_pool_init(struct otx2_nic *pfvf)
 
 int otx2_rq_aura_pool_init(struct otx2_nic *pfvf)
 {
+	int stack_pages, pool_id, aura_id;
 	struct otx2_hw *hw = &pfvf->hw;
-	int stack_pages, pool_id;
 	struct otx2_pool *pool;
 	int err, ptr;
 	s64 bufptr;
@@ -799,12 +810,15 @@ int otx2_rq_aura_pool_init(struct otx2_nic *pfvf)
 	stack_pages =
 		(RQ_QLEN + hw->stack_pg_ptrs - 1) / hw->stack_pg_ptrs;
 
-	for (pool_id = 0; pool_id < hw->rx_queues; pool_id++) {
+	for (aura_id = 0; aura_id < hw->rx_queues; aura_id++) {
+		pool_id = (hw->rqpool_cnt == hw->rx_queues) ? aura_id : 0;
 		/* Initialize aura context */
-		err = otx2_aura_init(pfvf, pool_id, pool_id, RQ_QLEN);
+		err = otx2_aura_init(pfvf, aura_id, pool_id, RQ_QLEN);
 		if (err)
 			goto fail;
+	}
 
+	for (pool_id = 0; pool_id < hw->rqpool_cnt; pool_id++) {
 		err = otx2_pool_init(pfvf, pool_id, stack_pages,
 				     RQ_QLEN, RCV_FRAG_LEN);
 		if (err)
@@ -817,7 +831,7 @@ int otx2_rq_aura_pool_init(struct otx2_nic *pfvf)
 		goto fail;
 
 	/* Allocate pointers and free them to aura/pool */
-	for (pool_id = 0; pool_id < hw->rx_queues; pool_id++) {
+	for (pool_id = 0; pool_id < hw->rqpool_cnt; pool_id++) {
 		pool = &pfvf->qset.pool[pool_id];
 		for (ptr = 0; ptr < RQ_QLEN; ptr++) {
 			bufptr = otx2_alloc_rbuf(pfvf, pool);
diff --git a/drivers/net/ethernet/marvell/octeontx2/otx2_common.h b/drivers/net/ethernet/marvell/octeontx2/otx2_common.h
index 1db8c35942ae..df888e209e64 100644
--- a/drivers/net/ethernet/marvell/octeontx2/otx2_common.h
+++ b/drivers/net/ethernet/marvell/octeontx2/otx2_common.h
@@ -116,6 +116,7 @@ struct otx2_hw {
 	cpumask_var_t           *affinity_mask;
 
 	u8			cint_cnt; /* CQ interrupt count */
+	u16			rqpool_cnt;
 	u16		txschq_list[NIX_TXSCH_LVL_CNT][MAX_TXSCHQ_PER_FUNC];
 
 	/* For TSO segmentation */
diff --git a/drivers/net/ethernet/marvell/octeontx2/otx2_pf.c b/drivers/net/ethernet/marvell/octeontx2/otx2_pf.c
index 47bdf32c7dee..e2a1aad1a8a2 100644
--- a/drivers/net/ethernet/marvell/octeontx2/otx2_pf.c
+++ b/drivers/net/ethernet/marvell/octeontx2/otx2_pf.c
@@ -736,6 +736,7 @@ static int otx2_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	hw->rx_queues = qcount;
 	hw->tx_queues = qcount;
 	hw->max_queues = qcount;
+	hw->rqpool_cnt = qcount;
 
 	/* Map CSRs */
 	pf->reg_base = pcim_iomap(pdev, PCI_CFG_REG_BAR_NUM, 0);
-- 
2.17.1

