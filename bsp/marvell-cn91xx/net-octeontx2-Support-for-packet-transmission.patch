From b7942060d896820ffaa7f664e13264a091889199 Mon Sep 17 00:00:00 2001
From: Geetha sowjanya <gakula@marvell.com>
Date: Thu, 16 Aug 2018 17:13:31 +0530
Subject: [PATCH 0110/1051] net: octeontx2: Support for packet transmission

This patch adds support for packet transmission and also
for processing transmit notifications.

One SQE can fit a max of 9 fragments, if SKB has
more number of fragments then SKB is linearized.

Signed-off-by: Geetha sowjanya <gakula@marvell.com>
Signed-off-by: Sunil Goutham <sgoutham@marvell.com>
Signed-off-by: Linu Cherian <lcherian@marvell.com>
[Kevin: The original patch got from Marvell sdk10.0_19.06]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 .../ethernet/marvell/octeontx2/otx2_common.c  |  26 +++
 .../net/ethernet/marvell/octeontx2/otx2_pf.c  |  43 ++++
 .../ethernet/marvell/octeontx2/otx2_struct.h  |  92 +++++++++
 .../ethernet/marvell/octeontx2/otx2_txrx.c    | 192 ++++++++++++++++++
 .../ethernet/marvell/octeontx2/otx2_txrx.h    |  26 +++
 5 files changed, 379 insertions(+)

diff --git a/drivers/net/ethernet/marvell/octeontx2/otx2_common.c b/drivers/net/ethernet/marvell/octeontx2/otx2_common.c
index c28ac27adf10..8347ad899d89 100644
--- a/drivers/net/ethernet/marvell/octeontx2/otx2_common.c
+++ b/drivers/net/ethernet/marvell/octeontx2/otx2_common.c
@@ -163,7 +163,33 @@ static int otx2_rq_init(struct otx2_nic *pfvf, u16 qidx)
 
 static int otx2_sq_init(struct otx2_nic *pfvf, u16 qidx)
 {
+	int pool_id = pfvf->hw.rx_queues + qidx;
+	struct otx2_qset *qset = &pfvf->qset;
+	struct otx2_snd_queue *sq;
 	struct nix_aq_enq_req *aq;
+	struct otx2_pool *pool;
+	int err;
+
+	pool = &pfvf->qset.pool[pool_id];
+	sq = &qset->sq[qidx];
+	sq->sqe_size = NIX_SQESZ_W16 ? 64 : 128;
+
+	err = qmem_alloc(pfvf->dev, &sq->sqe, 1, sq->sqe_size);
+	if (err)
+		return err;
+
+	sq->sqe_base = sq->sqe->base;
+	sq->sg = kcalloc((SQ_QLEN + 1), sizeof(struct sg_list), GFP_KERNEL);
+	if (!sq->sg)
+		return -ENOMEM;
+
+	sq->head = 0;
+	sq->num_sqbs = (pfvf->hw.sqb_size / sq->sqe_size) - 1;
+	sq->num_sqbs = (SQ_QLEN + sq->num_sqbs) / sq->num_sqbs;
+	sq->aura_id = pool_id;
+	sq->aura_fc_addr = pool->fc_addr->base;
+	sq->lmt_addr = (__force u64 *)(pfvf->reg_base + LMT_LF_LMTLINEX(qidx));
+	sq->io_addr = (__force u64)(pfvf->reg_base + NIX_LF_OP_SENDX(0));
 
 	/* Get memory to put this msg */
 	aq = otx2_mbox_alloc_msg_NIX_AQ_ENQ(&pfvf->mbox);
diff --git a/drivers/net/ethernet/marvell/octeontx2/otx2_pf.c b/drivers/net/ethernet/marvell/octeontx2/otx2_pf.c
index 90434ef7e6c6..3598fe2a1209 100644
--- a/drivers/net/ethernet/marvell/octeontx2/otx2_pf.c
+++ b/drivers/net/ethernet/marvell/octeontx2/otx2_pf.c
@@ -331,6 +331,41 @@ static int otx2_init_hw_resources(struct otx2_nic *pf)
 	return 0;
 }
 
+static netdev_tx_t otx2_xmit(struct sk_buff *skb, struct net_device *netdev)
+
+{
+	struct otx2_nic *pf = netdev_priv(netdev);
+	struct otx2_snd_queue *sq;
+	int qidx = skb_get_queue_mapping(skb);
+	struct netdev_queue *txq = netdev_get_tx_queue(netdev, qidx);
+
+	/* Check for minimum packet length */
+	if (skb->len <= ETH_HLEN) {
+		dev_kfree_skb(skb);
+		return NETDEV_TX_OK;
+	}
+
+	sq = &pf->qset.sq[qidx];
+
+	if (!netif_tx_queue_stopped(txq) &&
+	    !otx2_sq_append_skb(netdev, sq, skb, qidx)) {
+		netif_tx_stop_queue(txq);
+
+		/* Barrier, for stop_queue to be visible on other cpus */
+		smp_mb();
+		if ((sq->num_sqbs - *sq->aura_fc_addr) > 1)
+			netif_tx_start_queue(txq);
+		else
+			netdev_warn(netdev,
+				    "%s: No free SQE/SQB, stopping SQ%d\n",
+				     netdev->name, qidx);
+
+		return NETDEV_TX_BUSY;
+	}
+
+	return NETDEV_TX_OK;
+}
+
 static int otx2_open(struct net_device *netdev)
 {
 	struct otx2_nic *pf = netdev_priv(netdev);
@@ -358,6 +393,11 @@ static int otx2_open(struct net_device *netdev)
 	if (!qset->cq)
 		goto freemem;
 
+	qset->sq = kcalloc(pf->hw.tx_queues,
+			   sizeof(struct otx2_snd_queue), GFP_KERNEL);
+	if (!qset->sq)
+		goto freemem;
+
 	err = otx2_init_hw_resources(pf);
 	if (err)
 		goto freemem;
@@ -421,6 +461,7 @@ static int otx2_open(struct net_device *netdev)
 	otx2_disable_napi(pf);
 	otx2_disable_msix(pf);
 freemem:
+	kfree(qset->sq);
 	kfree(qset->cq);
 	kfree(qset->napi);
 	return err;
@@ -453,6 +494,7 @@ static int otx2_stop(struct net_device *netdev)
 	otx2_disable_msix(pf);
 
 	otx2_disable_napi(pf);
+	kfree(qset->sq);
 	kfree(qset->cq);
 	kfree(qset->napi);
 	memset(qset, 0, sizeof(*qset));
@@ -462,6 +504,7 @@ static int otx2_stop(struct net_device *netdev)
 static const struct net_device_ops otx2_netdev_ops = {
 	.ndo_open		= otx2_open,
 	.ndo_stop		= otx2_stop,
+	.ndo_start_xmit		= otx2_xmit,
 };
 
 static int otx2_probe(struct pci_dev *pdev, const struct pci_device_id *id)
diff --git a/drivers/net/ethernet/marvell/octeontx2/otx2_struct.h b/drivers/net/ethernet/marvell/octeontx2/otx2_struct.h
index ceb32fbc1b0a..9264f9c2a717 100644
--- a/drivers/net/ethernet/marvell/octeontx2/otx2_struct.h
+++ b/drivers/net/ethernet/marvell/octeontx2/otx2_struct.h
@@ -17,6 +17,17 @@ enum nix_cqesz_e {
 	NIX_XQESZ_W16 = 0x1,
 };
 
+enum nix_sqes_e {
+	NIX_SQESZ_W16 = 0x0,
+	NIX_SQESZ_W8 = 0x1,
+};
+
+enum nix_send_ldtype {
+	NIX_SEND_LDTYPE_LDD  = 0x0,
+	NIX_SEND_LDTYPE_LDT  = 0x1,
+	NIX_SEND_LDTYPE_LDWB = 0x2,
+};
+
 /* NIX wqe/cqe types */
 enum nix_xqe_type {
 	NIX_XQE_TYPE_INVALID   = 0x0,
@@ -203,4 +214,85 @@ struct nix_rx_sg_s {
 #endif
 };
 
+struct nix_send_comp_s {
+#if defined(__BIG_ENDIAN_BITFIELD)	/* W0 */
+	u64 rsvd_24_63  : 40;
+	u64 sqe_id	: 16;
+	u64 status	: 8;
+#else
+	u64 status	: 8;
+	u64 sqe_id	: 16;
+	u64 rsvd_24_63	: 40;
+#endif
+};
+
+/* NIX SQE header structure */
+struct nix_sqe_hdr_s {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	u64 sq			: 20; /* W0 */
+	u64 pnc			: 1;
+	u64 sizem1		: 3;
+	u64 aura		: 20;
+	u64 df			: 1;
+	u64 reserved_18		: 1;
+	u64 total		: 18;
+#else
+	u64 total		: 18;
+	u64 reserved_18		: 1;
+	u64 df			: 1;
+	u64 aura		: 20;
+	u64 sizem1		: 3;
+	u64 pnc			: 1;
+	u64 sq			: 20;
+#endif
+#if defined(__BIG_ENDIAN_BITFIELD)    /* W1 */
+	u64 sqe_id		:16;
+	u64 il4type		:4;
+	u64 il3type		:4;
+	u64 ol4type		:4;
+	u64 ol3type		:4;
+	u64 il4ptr		:8;
+	u64 il3ptr		:8;
+	u64 ol4ptr		:8;
+	u64 ol3ptr		:8;
+#else
+	u64 ol3ptr		:8;
+	u64 ol4ptr		:8;
+	u64 il3ptr		:8;
+	u64 il4ptr		:8;
+	u64 ol3type		:4;
+	u64 ol4type		:4;
+	u64 il3type		:4;
+	u64 il4type		:4;
+	u64 sqe_id		:16;
+
+#endif
+};
+
+struct nix_sqe_sg_s {
+#if defined(__BIG_ENDIAN_BITFIELD)  /* W0 */
+	u64 subdc	: 4;
+	u64 ld_type	: 2;
+	u64 i3		: 1;
+	u64 i2		: 1;
+	u64 i1		: 1;
+	u64 rsvd_54_50	: 5;
+	u64 segs	: 2;
+	u64 seg3_size	: 16;
+	u64 seg2_size	: 16;
+	u64 seg1_size	: 16;
+#else
+	u64 seg1_size	: 16;
+	u64 seg2_size	: 16;
+	u64 seg3_size	: 16;
+	u64 segs	: 2;
+	u64 rsvd_54_50	: 5;
+	u64 i1		: 1;
+	u64 i2		: 1;
+	u64 i3		: 1;
+	u64 ld_type	: 2;
+	u64 subdc	: 4;
+#endif
+};
+
 #endif /* OTX2_STRUCT_H */
diff --git a/drivers/net/ethernet/marvell/octeontx2/otx2_txrx.c b/drivers/net/ethernet/marvell/octeontx2/otx2_txrx.c
index 2fa9c58dfb32..8fcde319040f 100644
--- a/drivers/net/ethernet/marvell/octeontx2/otx2_txrx.c
+++ b/drivers/net/ethernet/marvell/octeontx2/otx2_txrx.c
@@ -16,6 +16,12 @@
 #include "otx2_struct.h"
 #include "otx2_txrx.h"
 
+/* Flush SQE written to LMT to SQB */
+static inline u64 otx2_lmt_flush(uint64_t addr)
+{
+	return atomic64_fetch_xor_relaxed(0, (atomic64_t *)addr);
+}
+
 static inline u64 otx2_nix_cq_op_status(struct otx2_nic *pfvf, int cq_idx)
 {
 	u64 incr = (u64)cq_idx << 32;
@@ -43,6 +49,69 @@ static inline unsigned int frag_num(unsigned int i)
 #endif
 }
 
+static dma_addr_t otx2_dma_map_skb_frag(struct otx2_nic *pfvf,
+					struct sk_buff *skb, int seg, int *len)
+{
+	const struct skb_frag_struct *frag;
+	struct page *page;
+	int offset;
+
+	/* First segment is always skb->data */
+	if (!seg) {
+		page = virt_to_page(skb->data);
+		offset = offset_in_page(skb->data);
+		*len = skb_headlen(skb);
+	} else {
+		frag = &skb_shinfo(skb)->frags[seg - 1];
+		page = skb_frag_page(frag);
+		offset = frag->page_offset;
+		*len = skb_frag_size(frag);
+	}
+	return dma_map_page_attrs(pfvf->dev, page, offset, *len,
+				  DMA_TO_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);
+}
+
+static void otx2_dma_unmap_skb_frags(struct otx2_nic *pfvf, struct sg_list *sg)
+{
+	int seg;
+
+	for (seg = 0; seg < sg->num_segs; seg++) {
+		dma_unmap_page_attrs(pfvf->dev, sg->dma_addr[seg],
+				     sg->size[seg], DMA_TO_DEVICE,
+				     DMA_ATTR_SKIP_CPU_SYNC);
+	}
+}
+
+static void otx2_snd_pkt_handler(struct otx2_nic *pfvf,
+				 struct otx2_cq_queue *cq, void *cqe,
+				 int budget)
+{
+	struct nix_cqe_hdr_s *cqe_hdr = (struct nix_cqe_hdr_s *)cqe;
+	struct nix_send_comp_s *snd_comp;
+	struct sk_buff *skb = NULL;
+	struct otx2_snd_queue *sq;
+	struct sg_list *sg;
+
+	snd_comp = (struct nix_send_comp_s *)(cqe + sizeof(*cqe_hdr));
+	if (snd_comp->status) {
+		/* tx packet error handling*/
+		dev_info(pfvf->dev, "TX%d: Error in send CQ entry\n",
+			 cq->cint_idx);
+	}
+
+	/* Barrier, so that update to sq by other cpus is visible */
+	smp_mb();
+	sq = &pfvf->qset.sq[cq->cint_idx];
+	sg = &sq->sg[snd_comp->sqe_id];
+
+	skb = (struct sk_buff *)sg->skb;
+	if (skb) {
+		otx2_dma_unmap_skb_frags(pfvf, sg);
+		napi_consume_skb(skb, budget);
+		sg->skb = (u64)NULL;
+	}
+}
+
 static void otx2_skb_add_frag(struct otx2_nic *pfvf,
 			      struct sk_buff *skb, u64 iova, int len)
 {
@@ -198,6 +267,8 @@ static int otx2_napi_handler(struct otx2_cq_queue *cq, struct otx2_nic *pfvf,
 			otx2_rcv_pkt_handler(pfvf, cq, cqe_hdr, &pool_ptrs);
 			workdone++;
 			break;
+		case NIX_XQE_TYPE_SEND:
+			otx2_snd_pkt_handler(pfvf, cq, cqe_hdr, budget);
 		}
 		processed_cqe++;
 	}
@@ -254,3 +325,124 @@ int otx2_poll(struct napi_struct *napi, int budget)
 	}
 	return workdone;
 }
+
+#define MAX_SEGS_PER_SG	3
+/* Add SQE scatter/gather subdescriptor structure */
+static bool otx2_sqe_add_sg(struct otx2_nic *pfvf, struct otx2_snd_queue *sq,
+			    struct sk_buff *skb, int num_segs, int *offset)
+{
+	struct nix_sqe_sg_s *sg = NULL;
+	u64 dma_addr, *iova = NULL;
+	u16 *sg_lens = NULL;
+	int seg, len;
+
+	sq->sg[sq->head].num_segs = 0;
+
+	for (seg = 0; seg < num_segs; seg++) {
+		if ((seg % MAX_SEGS_PER_SG) == 0) {
+			sg = (struct nix_sqe_sg_s *)(sq->sqe_base + *offset);
+			sg->ld_type = NIX_SEND_LDTYPE_LDD;
+			sg->subdc = NIX_SUBDC_SG;
+			sg->segs = 0;
+			sg_lens = (void *)sg;
+			iova = (void *)sg + sizeof(*sg);
+			/* Next subdc always starts at a 16byte boundary.
+			 * So if sg->segs is whether 2 or 3, offset += 16bytes.
+			 */
+			if ((num_segs - seg) >= (MAX_SEGS_PER_SG - 1))
+				*offset += sizeof(*sg) + (2 * sizeof(u64));
+			else
+				*offset += sizeof(*sg) + sizeof(u64);
+		}
+		dma_addr = otx2_dma_map_skb_frag(pfvf, skb, seg, &len);
+		if (dma_mapping_error(pfvf->dev, dma_addr))
+			return false;
+
+		sg_lens[frag_num(seg % MAX_SEGS_PER_SG)] = len;
+		sg->segs++;
+		*iova++ = dma_addr;
+
+		/* Save DMA mapping info for later unmapping */
+		sq->sg[sq->head].dma_addr[seg] = dma_addr;
+		sq->sg[sq->head].size[seg] = len;
+		sq->sg[sq->head].num_segs++;
+	}
+
+	sq->sg[sq->head].skb = (u64)skb;
+	return true;
+}
+
+/* Add SQE header subdescriptor structure */
+static void otx2_sqe_add_hdr(struct otx2_nic *pfvf, struct otx2_snd_queue *sq,
+			     struct nix_sqe_hdr_s *sqe_hdr, int len, u16 qidx)
+{
+	sqe_hdr->total = len;
+	/* Don't free Tx buffers to Aura */
+	sqe_hdr->df = 1;
+	sqe_hdr->aura = sq->aura_id;
+	/* Post a CQE Tx after pkt transmission */
+	sqe_hdr->pnc = 1;
+	sqe_hdr->sq = qidx;
+	/* Set SQE identifier which will be used later for freeing SKB */
+	sqe_hdr->sqe_id = sq->head;
+}
+
+bool otx2_sq_append_skb(struct net_device *netdev, struct otx2_snd_queue *sq,
+			struct sk_buff *skb, u16 qidx)
+{
+	struct otx2_nic *pfvf = netdev_priv(netdev);
+	struct nix_sqe_hdr_s *sqe_hdr;
+	int offset, num_segs;
+	u64 status;
+
+	/* Check if there is room for new SQE.
+	 * 'Num of SQBs freed to SQ's pool - SQ's Aura count'
+	 * will give free SQE count.
+	 */
+	if (!(sq->num_sqbs - *sq->aura_fc_addr))
+		goto fail;
+
+	/* Set SQE's SEND_HDR */
+	memset(sq->sqe_base, 0, sq->sqe_size);
+	sqe_hdr = (struct nix_sqe_hdr_s *)(sq->sqe_base);
+	otx2_sqe_add_hdr(pfvf, sq, sqe_hdr, skb->len, qidx);
+	offset = sizeof(*sqe_hdr);
+
+	num_segs = skb_shinfo(skb)->nr_frags + 1;
+
+	/* If SKB doesn't fit in a single SQE, linearize it.
+	 * TODO: Consider adding JUMP descriptor instead.
+	 */
+	if (num_segs > OTX2_MAX_FRAGS_IN_SQE) {
+		if (__skb_linearize(skb)) {
+			dev_kfree_skb_any(skb);
+			return true;
+		}
+		num_segs = skb_shinfo(skb)->nr_frags + 1;
+	}
+
+	/* Add SG subdesc with data frags */
+	if (!otx2_sqe_add_sg(pfvf, sq, skb, num_segs, &offset)) {
+		otx2_dma_unmap_skb_frags(pfvf, &sq->sg[sq->head]);
+		return false;
+	}
+
+	sqe_hdr->sizem1 = (offset / 16) - 1;
+
+	/* Packet data stores should finish before SQE is flushed to HW */
+	dma_wmb();
+
+	do {
+		memcpy(sq->lmt_addr, sqe_hdr, offset);
+		status = otx2_lmt_flush(sq->io_addr);
+	} while (status == 0);
+
+	sq->head++;
+	sq->head &= (SQ_QLEN - 1);
+
+	return true;
+fail:
+	netdev_warn(pfvf->netdev, "SQ%d full, SQB count %d Aura count %lld\n",
+		    qidx, sq->num_sqbs, *sq->aura_fc_addr);
+	return false;
+}
diff --git a/drivers/net/ethernet/marvell/octeontx2/otx2_txrx.h b/drivers/net/ethernet/marvell/octeontx2/otx2_txrx.h
index 4357b4cbdd11..4d0a2fcdd4be 100644
--- a/drivers/net/ethernet/marvell/octeontx2/otx2_txrx.h
+++ b/drivers/net/ethernet/marvell/octeontx2/otx2_txrx.h
@@ -11,6 +11,7 @@
 #ifndef OTX2_TXRX_H
 #define OTX2_TXRX_H
 
+#include <linux/etherdevice.h>
 #include <linux/iommu.h>
 
 #define RQ_QLEN		1024
@@ -19,6 +20,28 @@
 #define RCV_FRAG_LEN	(SKB_DATA_ALIGN(DMA_BUFFER_LEN + NET_SKB_PAD) + \
 			 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
 
+#define OTX2_MAX_FRAGS_IN_SQE	9
+
+struct sg_list {
+	u16	num_segs;
+	u64	skb;
+	u64	size[OTX2_MAX_FRAGS_IN_SQE];
+	u64	dma_addr[OTX2_MAX_FRAGS_IN_SQE];
+};
+
+struct otx2_snd_queue {
+	u8			aura_id;
+	u16			head;
+	u16			sqe_size;
+	u16			num_sqbs;
+	u64			 io_addr;
+	u64			*aura_fc_addr;
+	u64			*lmt_addr;
+	void			*sqe_base;
+	struct qmem		*sqe;
+	struct sg_list		*sg;
+};
+
 struct otx2_cq_poll {
 	void			*dev;
 #define CINT_INVALID_CQ		255
@@ -54,6 +77,7 @@ struct otx2_qset {
 	struct otx2_pool	*pool;
 	struct otx2_cq_poll	*napi;
 	struct otx2_cq_queue	*cq;
+	struct otx2_snd_queue	*sq;
 };
 
 /* Translate IOVA to physical address */
@@ -66,4 +90,6 @@ static inline u64 otx2_iova_to_phys(void *iommu_domain, dma_addr_t dma_addr)
 }
 
 int otx2_poll(struct napi_struct *napi, int budget);
+bool otx2_sq_append_skb(struct net_device *netdev, struct otx2_snd_queue *sq,
+			struct sk_buff *skb, u16 qidx);
 #endif /* OTX2_TXRX_H */
-- 
2.17.1

