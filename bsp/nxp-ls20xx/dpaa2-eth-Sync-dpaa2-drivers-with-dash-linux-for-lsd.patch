From 6c8f8775e21a71b41c075afb8833f9daa0493f7c Mon Sep 17 00:00:00 2001
From: "G.h. Gao" <guanhua.gao@nxp.com>
Date: Wed, 21 Nov 2018 11:03:59 +0800
Subject: [PATCH 666/767] dpaa2-eth: Sync dpaa2 drivers with dash-linux for
 lsdk1812

Sync dpaa2 drivers with dash-linux for 1812 backport.

Signed-off-by: Guanhua Gao <guanhua.gao@nxp.com>
[Xulin: Original patch taken from NXP LSDK-18.12.]
Signed-off-by: Xulin Sun <xulin.sun@windriver.com>
---
 .../staging/fsl-dpaa2/ethernet/dpaa2-eth.c    | 407 +++++++-------
 .../staging/fsl-dpaa2/ethernet/dpaa2-eth.h    | 211 ++++----
 .../fsl-dpaa2/ethernet/dpaa2-ethtool.c        | 504 +++++++-----------
 drivers/staging/fsl-dpaa2/ethernet/dpni.c     | 166 +++---
 4 files changed, 589 insertions(+), 699 deletions(-)

diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c
index 213fd7cb5ead..acc20c7c3f41 100755
--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c
+++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c
@@ -30,8 +30,6 @@ MODULE_LICENSE("Dual BSD/GPL");
 MODULE_AUTHOR("Freescale Semiconductor, Inc");
 MODULE_DESCRIPTION("Freescale DPAA2 Ethernet Driver");
 
-const char dpaa2_eth_drv_version[] = "0.1";
-
 static void *dpaa2_iova_to_virt(struct iommu_domain *domain,
 				dma_addr_t iova_addr)
 {
@@ -248,6 +246,9 @@ static int dpaa2_eth_xdp_tx(struct dpaa2_eth_priv *priv,
 	return err;
 }
 
+/* Free buffers acquired from the buffer pool or which were meant to
+ * be released in the pool
+ */
 static void free_bufs(struct dpaa2_eth_priv *priv, u64 *buf_array, int count)
 {
 	struct device *dev = priv->net_dev->dev.parent;
@@ -255,7 +256,6 @@ static void free_bufs(struct dpaa2_eth_priv *priv, u64 *buf_array, int count)
 	int i;
 
 	for (i = 0; i < count; i++) {
-		/* Same logic as on regular Rx path */
 		vaddr = dpaa2_iova_to_virt(priv->iommu_domain, buf_array[i]);
 		dma_unmap_single(dev, buf_array[i], DPAA2_ETH_RX_BUF_SIZE,
 				 DMA_BIDIRECTIONAL);
@@ -419,11 +419,13 @@ static void dpaa2_eth_rx(struct dpaa2_eth_priv *priv,
 	/* Get the timestamp value */
 	if (priv->ts_rx_en) {
 		struct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);
-		u64 *ns = dpaa2_get_ts(vaddr, false);
+		__le64 *ts = dpaa2_get_ts(vaddr, false);
+		u64 ns;
 
-		*ns = DPAA2_PTP_NOMINAL_FREQ_PERIOD_NS * le64_to_cpup(ns);
 		memset(shhwtstamps, 0, sizeof(*shhwtstamps));
-		shhwtstamps->hwtstamp = ns_to_ktime(*ns);
+
+		ns = DPAA2_PTP_NOMINAL_FREQ_PERIOD_NS * le64_to_cpup(ts);
+		shhwtstamps->hwtstamp = ns_to_ktime(ns);
 	}
 
 	/* Check if we need to validate the L4 csum */
@@ -500,13 +502,10 @@ static void dpaa2_eth_rx_err(struct dpaa2_eth_priv *priv,
  * make sure we don't accidentally issue another volatile dequeue which would
  * overwrite (leak) frames already in the store.
  *
- * The number of frames is returned using the last 2 output arguments,
- * separately for Rx and Tx confirmations.
- *
  * Observance of NAPI budget is not our concern, leaving that to the caller.
  */
-static bool consume_frames(struct dpaa2_eth_channel *ch, int *rx_cleaned,
-			   int *tx_conf_cleaned)
+static int consume_frames(struct dpaa2_eth_channel *ch,
+			  enum dpaa2_eth_fq_type *type)
 {
 	struct dpaa2_eth_priv *priv = ch->priv;
 	struct dpaa2_eth_fq *fq = NULL;
@@ -536,30 +535,18 @@ static bool consume_frames(struct dpaa2_eth_channel *ch, int *rx_cleaned,
 	} while (!is_last);
 
 	if (!cleaned)
-		return false;
-
-	/* All frames brought in store by a volatile dequeue
-	 * come from the same queue
-	 */
-	if (fq->type == DPAA2_TX_CONF_FQ) {
-		*tx_conf_cleaned += cleaned;
-	} else {
-		*rx_cleaned += cleaned;
-		/* If we processed XDP_REDIRECT frames, flush them now */
-		/* FIXME: Since we don't actually do anything inside
-		 * ndo_xdp_flush, we call it here simply for compliance
-		 * reasons
-		 */
-		if (ch->flush) {
-			xdp_do_flush_map();
-			ch->flush = false;
-		}
-	}
+		return 0;
 
 	fq->stats.frames += cleaned;
 	ch->stats.frames += cleaned;
 
-	return true;
+	/* A dequeue operation only pulls frames from a single queue
+	 * into the store. Return the frame queue type as an out param.
+	 */
+	if (type)
+		*type = fq->type;
+
+	return cleaned;
 }
 
 /* Configure the egress frame annotation for timestamp update */
@@ -737,12 +724,9 @@ static int build_single_fd(struct dpaa2_eth_priv *priv,
  * back-pointed to is also freed.
  * This can be called either from dpaa2_eth_tx_conf() or on the error path of
  * dpaa2_eth_tx().
- * Optionally, return the frame annotation status word (FAS), which needs
- * to be checked if we're on the confirmation path.
  */
-static void free_tx_fd(struct dpaa2_eth_priv *priv,
-		       const struct dpaa2_fd *fd,
-		       bool in_napi)
+static void free_tx_fd(const struct dpaa2_eth_priv *priv,
+		       const struct dpaa2_fd *fd, bool in_napi)
 {
 	struct device *dev = priv->net_dev->dev.parent;
 	dma_addr_t fd_addr;
@@ -793,13 +777,13 @@ static void free_tx_fd(struct dpaa2_eth_priv *priv,
 	/* Get the timestamp value */
 	if (priv->ts_tx_en && skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {
 		struct skb_shared_hwtstamps shhwtstamps;
-		u64 *ns;
+		__le64 *ts = dpaa2_get_ts(buffer_start, true);
+		u64 ns;
 
 		memset(&shhwtstamps, 0, sizeof(shhwtstamps));
 
-		ns = dpaa2_get_ts(buffer_start, true);
- 		*ns = DPAA2_PTP_NOMINAL_FREQ_PERIOD_NS * le64_to_cpup(ns);
-		shhwtstamps.hwtstamp = ns_to_ktime(*ns);
+		ns = DPAA2_PTP_NOMINAL_FREQ_PERIOD_NS * le64_to_cpup(ts);
+		shhwtstamps.hwtstamp = ns_to_ktime(ns);
 		skb_tstamp_tx(skb, &shhwtstamps);
 	}
 
@@ -832,7 +816,7 @@ static netdev_tx_t dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
 	 */
 	if (net_dev->num_tc)
 		prio = net_dev->num_tc - prio - 1;
- 
+
 	queue_mapping %= dpaa2_eth_queue_count(priv);
 	fq = &priv->fq[queue_mapping];
 
@@ -849,7 +833,6 @@ static netdev_tx_t dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
 	percpu_stats = this_cpu_ptr(priv->percpu_stats);
 	percpu_extras = this_cpu_ptr(priv->percpu_extras);
 
-	/* For non-linear skb we don't need a minimum headroom */
 	needed_headroom = dpaa2_eth_needed_headroom(priv, skb);
 	if (skb_headroom(skb) < needed_headroom) {
 		struct sk_buff *ns;
@@ -894,15 +877,15 @@ static netdev_tx_t dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
 		goto err_build_fd;
 	}
 
-	/* Tracing point */
-	trace_dpaa2_tx_fd(net_dev, &fd);
-
 	if (dpaa2_eth_ceetm_is_enabled(priv)) {
 		err = dpaa2_ceetm_classify(skb, net_dev->qdisc, &ch_id, &prio);
 		if (err)
 			goto err_ceetm_classify;
 	}
 
+	/* Tracing point */
+	trace_dpaa2_tx_fd(net_dev, &fd);
+
 	for (i = 0; i < DPAA2_ETH_ENQUEUE_RETRIES; i++) {
 		err = dpaa2_io_service_enqueue_qd(fq->channel->dpio,
 						  priv->tx_qdid, prio,
@@ -933,7 +916,7 @@ static netdev_tx_t dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
 
 /* Tx confirmation frame processing routine */
 static void dpaa2_eth_tx_conf(struct dpaa2_eth_priv *priv,
-			      struct dpaa2_eth_channel *ch,
+			      struct dpaa2_eth_channel *ch __always_unused,
 			      const struct dpaa2_fd *fd,
 			      struct napi_struct *napi __always_unused,
 			      u16 queue_id)
@@ -1190,15 +1173,16 @@ static int pull_channel(struct dpaa2_eth_channel *ch)
 /* NAPI poll routine
  *
  * Frames are dequeued from the QMan channel associated with this NAPI context.
- * Rx and (if configured) Rx error frames count towards the NAPI budget. Tx
- * confirmation frames are limited by a threshold per NAPI poll cycle.
+ * Rx, Tx confirmation and (if configured) Rx error frames all count
+ * towards the NAPI budget.
  */
 static int dpaa2_eth_poll(struct napi_struct *napi, int budget)
 {
 	struct dpaa2_eth_channel *ch;
-	int rx_cleaned = 0, tx_conf_cleaned = 0;
-	bool store_cleaned;
 	struct dpaa2_eth_priv *priv;
+	int rx_cleaned = 0, txconf_cleaned = 0;
+	enum dpaa2_eth_fq_type type = 0;
+	int store_cleaned;
 	int err;
 
 	ch = container_of(napi, struct dpaa2_eth_channel, napi);
@@ -1212,27 +1196,37 @@ static int dpaa2_eth_poll(struct napi_struct *napi, int budget)
 		/* Refill pool if appropriate */
 		refill_pool(priv, ch, priv->bpid);
 
-		store_cleaned = consume_frames(ch, &rx_cleaned,
-					       &tx_conf_cleaned);
+		store_cleaned = consume_frames(ch, &type);
+		if (type == DPAA2_RX_FQ) {
+			rx_cleaned += store_cleaned;
+			/* If these are XDP_REDIRECT frames, flush them now */
+			/* TODO: Do we need this? */
+			if (ch->flush) {
+				xdp_do_flush_map();
+				ch->flush = false;
+			}
+		} else {
+			txconf_cleaned += store_cleaned;
+		}
 
-		/* If we've either consumed the budget with Rx frames,
-		 * or reached the Tx conf threshold, we're done.
+		/* If we either consumed the whole NAPI budget with Rx frames
+		 * or we reached the Tx confirmations threshold, we're done.
 		 */
 		if (rx_cleaned >= budget ||
-		    tx_conf_cleaned >= TX_CONF_PER_NAPI_POLL)
+		    txconf_cleaned >= DPAA2_ETH_TXCONF_PER_NAPI)
 			return budget;
 	} while (store_cleaned);
  
-	/* We didn't consume the entire budget, finish napi and
+	/* We didn't consume the entire budget, so finish napi and
 	 * re-enable data availability notifications
 	 */
-	napi_complete(napi);
+	napi_complete_done(napi, rx_cleaned);
 	do {
 		err = dpaa2_io_service_rearm(ch->dpio, &ch->nctx);
 		cpu_relax();
 	} while (err == -EBUSY);
-		WARN_ONCE(err, "CDAN notifications rearm failed on core %d",
-			  ch->nctx.desired_cpu);
+	WARN_ONCE(err, "CDAN notifications rearm failed on core %d",
+		  ch->nctx.desired_cpu);
 
 	return max(rx_cleaned, 1);
 }
@@ -1261,7 +1255,7 @@ static void disable_ch_napi(struct dpaa2_eth_priv *priv)
 
 static int link_state_update(struct dpaa2_eth_priv *priv)
 {
-	struct dpni_link_state state;
+	struct dpni_link_state state = {0};
 	int err;
 
 	err = dpni_get_link_state(priv->mc_io, 0, priv->mc_token, &state);
@@ -1346,7 +1340,7 @@ static int dpaa2_eth_open(struct net_device *net_dev)
 static int dpaa2_eth_stop(struct net_device *net_dev)
 {
 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
-	int dpni_enabled;
+	int dpni_enabled = 0;
 	int retries = 10, i;
 	int err = 0;
 
@@ -1645,10 +1639,10 @@ static int set_buffer_layout(struct dpaa2_eth_priv *priv)
 		priv->rx_buf_align = DPAA2_ETH_RX_BUF_ALIGN;
 
 	/* tx buffer */
-	buf_layout.pass_timestamp = true;
 	buf_layout.private_data_size = DPAA2_ETH_SWA_SIZE;
-	buf_layout.options = DPNI_BUF_LAYOUT_OPT_TIMESTAMP |
-			     DPNI_BUF_LAYOUT_OPT_PRIVATE_DATA_SIZE;
+	buf_layout.pass_timestamp = true;
+	buf_layout.options = DPNI_BUF_LAYOUT_OPT_PRIVATE_DATA_SIZE |
+			     DPNI_BUF_LAYOUT_OPT_TIMESTAMP;
 	err = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,
 				     DPNI_QUEUE_TX, &buf_layout);
 	if (err) {
@@ -1683,8 +1677,8 @@ static int set_buffer_layout(struct dpaa2_eth_priv *priv)
 	buf_layout.pass_frame_status = true;
 	buf_layout.pass_parser_result = true;
 	buf_layout.data_align = priv->rx_buf_align;
-	buf_layout.private_data_size = 0;
 	buf_layout.data_head_room = dpaa2_eth_rx_headroom(priv);
+	buf_layout.private_data_size = 0;
 	/* If XDP program is attached, reserve extra space for
 	 * potential header expansions
 	 */
@@ -2205,7 +2199,6 @@ static void set_fq_affinity(struct dpaa2_eth_priv *priv)
 			break;
 		case DPAA2_TX_CONF_FQ:
 			fq->target_cpu = txc_cpu;
-
 			txc_cpu = cpumask_next(txc_cpu, &priv->dpio_cpumask);
 			if (txc_cpu >= nr_cpu_ids)
 				txc_cpu = cpumask_first(&priv->dpio_cpumask);
@@ -2419,31 +2412,27 @@ static int setup_dpni(struct fsl_mc_device *ls_dev)
 	if (err)
 		goto close;
 
+	priv->cls_rule = devm_kzalloc(dev, sizeof(struct dpaa2_eth_cls_rule) *
+				      dpaa2_eth_fs_count(priv), GFP_KERNEL);
+	if (!priv->cls_rule)
+		goto close;
+
 	/* Enable congestion notifications for Tx queues */
 	err = setup_tx_congestion(priv);
 	if (err)
 		goto close;
 
-	/* allocate classification rule space */
-	priv->cls_rule = kzalloc(sizeof(*priv->cls_rule) *
-				 dpaa2_eth_fs_count(priv), GFP_KERNEL);
-	if (!priv->cls_rule)
-		goto close;
-
 	/* Enable flow control */
 	cfg.options = DPNI_LINK_OPT_AUTONEG | DPNI_LINK_OPT_PAUSE;
 	priv->tx_pause_frames = true;
 	err = dpni_set_link_cfg(priv->mc_io, 0, priv->mc_token, &cfg);
 	if (err) {
 		dev_err(dev, "dpni_set_link_cfg() failed\n");
-		goto cls_free;
+		goto close;
 	}
 
 	return 0;
 
-cls_free:
-	kfree(priv->cls_rule);
-
 close:
 	dpni_close(priv->mc_io, 0, priv->mc_token);
 
@@ -2462,8 +2451,6 @@ static void free_dpni(struct dpaa2_eth_priv *priv)
 
 	dpni_close(priv->mc_io, 0, priv->mc_token);
 
-	kfree(priv->cls_rule);
-
 	dma_unmap_single(dev, priv->cscn_dma, DPAA2_CSCN_SIZE, DMA_FROM_DEVICE);
 	kfree(priv->cscn_unaligned);
 }
@@ -2693,19 +2680,17 @@ static int setup_rx_err_flow(struct dpaa2_eth_priv *priv,
 }
 #endif
 
-/* default hash key fields */
-static struct dpaa2_eth_dist_fields default_dist_fields[] = {
+/* Supported header fields for Rx hash distribution key */
+static const struct dpaa2_eth_dist_fields dist_fields[] = {
 	{
 		/* L2 header */
 		.rxnfc_field = RXH_L2DA,
 		.cls_prot = NET_PROT_ETH,
 		.cls_field = NH_FLD_ETH_DA,
-		.id = DPAA2_ETH_DIST_ETHDST,
 		.size = 6,
 	}, {
 		.cls_prot = NET_PROT_ETH,
 		.cls_field = NH_FLD_ETH_SA,
-		.id = DPAA2_ETH_DIST_ETHSRC,
 		.size = 6,
 	}, {
 		/* This is the last ethertype field parsed:
@@ -2714,33 +2699,28 @@ static struct dpaa2_eth_dist_fields default_dist_fields[] = {
 		 */
 		.cls_prot = NET_PROT_ETH,
 		.cls_field = NH_FLD_ETH_TYPE,
-		.id = DPAA2_ETH_DIST_ETHTYPE,
 		.size = 2,
 	}, {
 		/* VLAN header */
 		.rxnfc_field = RXH_VLAN,
 		.cls_prot = NET_PROT_VLAN,
 		.cls_field = NH_FLD_VLAN_TCI,
-		.id = DPAA2_ETH_DIST_VLAN,
 		.size = 2,
 	}, {
 		/* IP header */
 		.rxnfc_field = RXH_IP_SRC,
 		.cls_prot = NET_PROT_IP,
 		.cls_field = NH_FLD_IP_SRC,
-		.id = DPAA2_ETH_DIST_IPSRC,
 		.size = 4,
 	}, {
 		.rxnfc_field = RXH_IP_DST,
 		.cls_prot = NET_PROT_IP,
 		.cls_field = NH_FLD_IP_DST,
-		.id = DPAA2_ETH_DIST_IPDST,
 		.size = 4,
 	}, {
 		.rxnfc_field = RXH_L3_PROTO,
 		.cls_prot = NET_PROT_IP,
 		.cls_field = NH_FLD_IP_PROTO,
-		.id = DPAA2_ETH_DIST_IPPROTO,
 		.size = 1,
 	}, {
 		/* Using UDP ports, this is functionally equivalent to raw
@@ -2749,147 +2729,173 @@ static struct dpaa2_eth_dist_fields default_dist_fields[] = {
 		.rxnfc_field = RXH_L4_B_0_1,
 		.cls_prot = NET_PROT_UDP,
 		.cls_field = NH_FLD_UDP_PORT_SRC,
-		.id = DPAA2_ETH_DIST_L4SRC,
 		.size = 2,
 	}, {
 		.rxnfc_field = RXH_L4_B_2_3,
 		.cls_prot = NET_PROT_UDP,
 		.cls_field = NH_FLD_UDP_PORT_DST,
-		.id = DPAA2_ETH_DIST_L4DST,
 		.size = 2,
 	},
 };
 
-static int legacy_config_dist_key(struct dpaa2_eth_priv *priv,
-				  dma_addr_t key_iova)
+/* Configure the Rx hash key using the legacy API */
+static int config_legacy_hash_key(struct dpaa2_eth_priv *priv, dma_addr_t key)
 {
 	struct device *dev = priv->net_dev->dev.parent;
 	struct dpni_rx_tc_dist_cfg dist_cfg;
-	int i, err;
-
-	/* In legacy mode, we can't configure flow steering independently */
-	if (!dpaa2_eth_hash_enabled(priv))
-		return -EOPNOTSUPP;
+	int i, err = 0;
 
 	memset(&dist_cfg, 0, sizeof(dist_cfg));
 
-	dist_cfg.key_cfg_iova = key_iova;
+	dist_cfg.key_cfg_iova = key;
 	dist_cfg.dist_size = dpaa2_eth_queue_count(priv);
-	if (dpaa2_eth_fs_enabled(priv)) {
-		dist_cfg.dist_mode = DPNI_DIST_MODE_FS;
-		dist_cfg.fs_cfg.miss_action = DPNI_FS_MISS_HASH;
-	} else {
-		dist_cfg.dist_mode = DPNI_DIST_MODE_HASH;
-	}
+	dist_cfg.dist_mode = DPNI_DIST_MODE_HASH;
 
 	for (i = 0; i < dpaa2_eth_tc_count(priv); i++) {
-		err = dpni_set_rx_tc_dist(priv->mc_io, 0, priv->mc_token, i,
-					  &dist_cfg);
+		err = dpni_set_rx_tc_dist(priv->mc_io, 0, priv->mc_token,
+					  i, &dist_cfg);
 		if (err) {
 			dev_err(dev, "dpni_set_rx_tc_dist failed\n");
-			return err;
+			break;
 		}
 	}
 
-	return 0;
+	return err;
 }
 
-static int config_hash_key(struct dpaa2_eth_priv *priv, dma_addr_t key_iova)
+/* Configure the Rx hash key using the new API */
+static int config_hash_key(struct dpaa2_eth_priv *priv, dma_addr_t key)
 {
 	struct device *dev = priv->net_dev->dev.parent;
 	struct dpni_rx_dist_cfg dist_cfg;
-	int i, err;
-
-	if (!dpaa2_eth_hash_enabled(priv))
-		return -EOPNOTSUPP;
+	int i, err = 0;
 
 	memset(&dist_cfg, 0, sizeof(dist_cfg));
 
-	dist_cfg.key_cfg_iova = key_iova;
+	dist_cfg.key_cfg_iova = key;
 	dist_cfg.dist_size = dpaa2_eth_queue_count(priv);
-	dist_cfg.enable = true;
+	dist_cfg.enable = 1;
 
 	for (i = 0; i < dpaa2_eth_tc_count(priv); i++) {
 		dist_cfg.tc = i;
-
-		err = dpni_set_rx_hash_dist(priv->mc_io, 0,
-					    priv->mc_token, &dist_cfg);
+		err = dpni_set_rx_hash_dist(priv->mc_io, 0, priv->mc_token,
+					    &dist_cfg);
 		if (err) {
 			dev_err(dev, "dpni_set_rx_hash_dist failed\n");
-			return err;
+			break;
 		}
 	}
 
-	return 0;
+	return err;
 }
 
-static int config_fs_key(struct dpaa2_eth_priv *priv, dma_addr_t key_iova)
+/* Configure the Rx flow classification key */
+static int config_cls_key(struct dpaa2_eth_priv *priv, dma_addr_t key)
 {
 	struct device *dev = priv->net_dev->dev.parent;
 	struct dpni_rx_dist_cfg dist_cfg;
-	int i, err;
-
-	if (!dpaa2_eth_fs_enabled(priv))
-		return -EOPNOTSUPP;
+	int i, err = 0;
 
 	memset(&dist_cfg, 0, sizeof(dist_cfg));
 
-	dist_cfg.key_cfg_iova = key_iova;
+	dist_cfg.key_cfg_iova = key;
 	dist_cfg.dist_size = dpaa2_eth_queue_count(priv);
-	dist_cfg.enable = true;
+	dist_cfg.enable = 1;
 
 	for (i = 0; i < dpaa2_eth_tc_count(priv); i++) {
 		dist_cfg.tc = i;
-
-		err = dpni_set_rx_fs_dist(priv->mc_io, 0,
-					  priv->mc_token, &dist_cfg);
+		err = dpni_set_rx_fs_dist(priv->mc_io, 0, priv->mc_token,
+					  &dist_cfg);
 		if (err) {
 			dev_err(dev, "dpni_set_rx_fs_dist failed\n");
-			return err;
+			break;
 		}
 	}
 
+	return err;
+}
+
+/* Size of the Rx flow classification key */
+int dpaa2_eth_cls_key_size(void)
+{
+	int i, size = 0;
+
+	for (i = 0; i < ARRAY_SIZE(dist_fields); i++)
+		size += dist_fields[i].size;
+
+	return size;
+}
+
+/* Offset of header field in Rx classification key */
+int dpaa2_eth_cls_fld_off(int prot, int field)
+{
+	int i, off = 0;
+
+	for (i = 0; i < ARRAY_SIZE(dist_fields); i++) {
+		if (dist_fields[i].cls_prot == prot &&
+		    dist_fields[i].cls_field == field)
+			return off;
+		off += dist_fields[i].size;
+	}
+
+	WARN_ONCE(1, "Unsupported header field used for Rx flow cls\n");
 	return 0;
 }
 
-int dpaa2_eth_set_dist_key(struct dpaa2_eth_priv *priv,
-			   enum dpaa2_eth_rx_dist type, u32 key_fields)
+/* Set Rx distribution (hash or flow classification) key
+ * flags is a combination of RXH_ bits
+ */
+static int dpaa2_eth_set_dist_key(struct net_device *net_dev,
+				  enum dpaa2_eth_rx_dist type, u64 flags)
 {
-	struct device *dev = priv->net_dev->dev.parent;
+	struct device *dev = net_dev->dev.parent;
+	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
 	struct dpkg_profile_cfg cls_cfg;
-	struct dpkg_extract *key;
-	u32 hash_fields = 0;
+	u32 rx_hash_fields = 0;
 	dma_addr_t key_iova;
-	u8 *key_mem;
-	int i, err;
+	u8 *dma_mem;
+	int i;
+	int err = 0;
 
 	memset(&cls_cfg, 0, sizeof(cls_cfg));
 
-	for (i = 0; i < priv->num_dist_fields; i++) {
-		if (!(key_fields & priv->dist_fields[i].id))
-			continue;
+	for (i = 0; i < ARRAY_SIZE(dist_fields); i++) {
+		struct dpkg_extract *key =
+			&cls_cfg.extracts[cls_cfg.num_extracts];
+
+		/* For Rx hashing key we set only the selected fields.
+		 * For Rx flow classification key we set all supported fields
+		 */
+		if (type == DPAA2_ETH_RX_DIST_HASH) {
+			if (!(flags & dist_fields[i].rxnfc_field))
+				continue;
+			rx_hash_fields |= dist_fields[i].rxnfc_field;
+		}
+
+		if (cls_cfg.num_extracts >= DPKG_MAX_NUM_OF_EXTRACTS) {
+			dev_err(dev, "error adding key extraction rule, too many rules?\n");
+			return -E2BIG;
+		}
 
-		key = &cls_cfg.extracts[cls_cfg.num_extracts];
 		key->type = DPKG_EXTRACT_FROM_HDR;
-		key->extract.from_hdr.prot = priv->dist_fields[i].cls_prot;
+		key->extract.from_hdr.prot = dist_fields[i].cls_prot;
 		key->extract.from_hdr.type = DPKG_FULL_FIELD;
-		key->extract.from_hdr.field = priv->dist_fields[i].cls_field;
+		key->extract.from_hdr.field = dist_fields[i].cls_field;
 		cls_cfg.num_extracts++;
-
-		hash_fields |= priv->dist_fields[i].rxnfc_field;
 	}
 
-	key_mem = kzalloc(DPAA2_CLASSIFIER_DMA_SIZE, GFP_KERNEL);
-	if (!key_mem)
+	dma_mem = kzalloc(DPAA2_CLASSIFIER_DMA_SIZE, GFP_KERNEL);
+	if (!dma_mem)
 		return -ENOMEM;
 
-	err = dpni_prepare_key_cfg(&cls_cfg, key_mem);
+	err = dpni_prepare_key_cfg(&cls_cfg, dma_mem);
 	if (err) {
 		dev_err(dev, "dpni_prepare_key_cfg error %d\n", err);
 		goto free_key;
 	}
 
-	key_iova = dma_map_single(dev, key_mem, DPAA2_CLASSIFIER_DMA_SIZE,
+	/* Prepare for setting the rx dist */
+	key_iova = dma_map_single(dev, dma_mem, DPAA2_CLASSIFIER_DMA_SIZE,
 				  DMA_TO_DEVICE);
 	if (dma_mapping_error(dev, key_iova)) {
 		dev_err(dev, "DMA mapping failed\n");
@@ -2897,37 +2903,61 @@ int dpaa2_eth_set_dist_key(struct dpaa2_eth_priv *priv,
 		goto free_key;
 	}
 
-	switch (type) {
-	case DPAA2_ETH_RX_DIST_LEGACY:
-		err = legacy_config_dist_key(priv, key_iova);
-		break;
-	case DPAA2_ETH_RX_DIST_HASH:
-		err = config_hash_key(priv, key_iova);
-		break;
-	case DPAA2_ETH_RX_DIST_FS:
-		err = config_fs_key(priv, key_iova);
-		break;
-	default:
-		err = -EINVAL;
-		break;
+	if (type == DPAA2_ETH_RX_DIST_HASH) {
+		if (dpaa2_eth_has_legacy_dist(priv))
+			err = config_legacy_hash_key(priv, key_iova);
+		else
+			err = config_hash_key(priv, key_iova);
+	} else {
+		err = config_cls_key(priv, key_iova);
 	}
 
 	dma_unmap_single(dev, key_iova, DPAA2_CLASSIFIER_DMA_SIZE,
 			 DMA_TO_DEVICE);
-	if (err) {
-		if (err != -EOPNOTSUPP)
-			dev_err(dev, "Distribution key config failed\n");
-		goto free_key;
-	}
-
-	if (type != DPAA2_ETH_RX_DIST_FS)
-		priv->rx_hash_fields = hash_fields;
+	if (!err && type == DPAA2_ETH_RX_DIST_HASH)
+		priv->rx_hash_fields = rx_hash_fields;
 
 free_key:
-	kfree(key_mem);
+	kfree(dma_mem);
 	return err;
 }
 
+int dpaa2_eth_set_hash(struct net_device *net_dev, u64 flags)
+{
+	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
+
+	if (!dpaa2_eth_hash_enabled(priv))
+		return -EOPNOTSUPP;
+
+	return dpaa2_eth_set_dist_key(net_dev, DPAA2_ETH_RX_DIST_HASH, flags);
+}
+
+static int dpaa2_eth_set_cls(struct dpaa2_eth_priv *priv)
+{
+	struct device *dev = priv->net_dev->dev.parent;
+
+	/* Check if we actually support Rx flow classification */
+	if (dpaa2_eth_has_legacy_dist(priv)) {
+		dev_dbg(dev, "Rx cls not supported by current MC version\n");
+		return -EOPNOTSUPP;
+	}
+
+	if (priv->dpni_attrs.options & DPNI_OPT_NO_FS ||
+	    !(priv->dpni_attrs.options & DPNI_OPT_HAS_KEY_MASKING)) {
+		dev_dbg(dev, "Rx cls disabled in DPNI options\n");
+		return -EOPNOTSUPP;
+	}
+
+	if (!dpaa2_eth_hash_enabled(priv)) {
+		dev_dbg(dev, "Rx cls disabled for single queue DPNIs\n");
+		return -EOPNOTSUPP;
+	}
+
+	priv->rx_cls_enabled = 1;
+
+	return dpaa2_eth_set_dist_key(priv->net_dev, DPAA2_ETH_RX_DIST_CLS, 0);
+}
+
 /* Bind the DPNI to its needed objects and resources: buffer pool, DPIOs,
  * frame queues and channels
  */
@@ -2951,27 +2981,19 @@ static int bind_dpni(struct dpaa2_eth_priv *priv)
 		return err;
 	}
 
-	/* Verify classification options and disable hashing and/or
-	 * flow steering support in case of invalid configuration values
+	/* have the interface implicitly distribute traffic based on
+	 * the default hash key
 	 */
-	priv->dist_fields = default_dist_fields;
-	priv->num_dist_fields = ARRAY_SIZE(default_dist_fields);
-	check_cls_support(priv);
+	err = dpaa2_eth_set_hash(net_dev, DPAA2_RXH_DEFAULT);
+	if (err && err != -EOPNOTSUPP)
+		dev_err(dev, "Failed to configure hashing\n");
 
-	/* have the interface implicitly distribute traffic based on
-	 * a static hash key. Also configure flow steering key, if supported.
-	 * Errors here are not blocking, so just let the called function
-	 * print its error message and move along.
+	/* Configure the flow classification key; it includes all
+	 * supported header fields and cannot be modified at runtime
 	 */
-	if (dpaa2_eth_has_legacy_dist(priv)) {
-		dpaa2_eth_set_dist_key(priv, DPAA2_ETH_RX_DIST_LEGACY,
-				       DPAA2_ETH_DIST_ALL);
-	} else {
-		dpaa2_eth_set_dist_key(priv, DPAA2_ETH_RX_DIST_HASH,
-				       DPAA2_ETH_DIST_DEFAULT_HASH);
-		dpaa2_eth_set_dist_key(priv, DPAA2_ETH_RX_DIST_FS,
-				       DPAA2_ETH_DIST_ALL);
-	}
+	err = dpaa2_eth_set_cls(priv);
+	if (err && err != -EOPNOTSUPP)
+		dev_err(dev, "Failed to configure Rx classification key\n");
 
 	/* Configure handling of error frames */
 	err_cfg.errors = DPAA2_FAS_RX_ERR_MASK;
@@ -3146,10 +3168,10 @@ static int netdev_init(struct net_device *net_dev)
 		return err;
 	}
 
-	/* Set MTU upper limit; lower limit is default (68B) */
+	/* Set MTU upper limit; lower limit is 68B (default value) */
 	net_dev->max_mtu = DPAA2_ETH_MAX_MTU;
 	err = dpni_set_max_frame_length(priv->mc_io, 0, priv->mc_token,
-					(u16)DPAA2_ETH_MFL);
+					DPAA2_ETH_MFL);
 	if (err) {
 		dev_err(dev, "dpni_set_max_frame_length() failed\n");
 		return err;
@@ -3467,7 +3489,7 @@ static int set_vlan_qos(struct dpaa2_eth_priv *priv)
 
 	key_params.key_size = key_size;
 
-	if (dpaa2_eth_fs_mask_enabled(priv)) {
+	if (priv->dpni_attrs.options & DPNI_OPT_HAS_KEY_MASKING) {
 		mask = kzalloc(key_size, GFP_KERNEL);
 		if (!mask)
 			goto out_free;
@@ -3868,7 +3890,6 @@ static int dpaa2_eth_remove(struct fsl_mc_device *ls_dev)
 
 	fsl_mc_portal_free(priv->mc_io);
 
-	dev_set_drvdata(dev, NULL);
 	free_netdev(net_dev);
 
 	dev_dbg(net_dev->dev.parent, "Removed interface %s\n", net_dev->name);
diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h
index f47ef220ad4a..70f677a026a2 100755
--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h
+++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h
@@ -10,6 +10,7 @@
 #include <linux/netdevice.h>
 #include <linux/if_vlan.h>
 #include <linux/fsl/mc.h>
+#include <linux/filter.h>
 
 #include "../../fsl-mc/include/dpaa2-io.h"
 #include "../../fsl-mc/include/dpaa2-fd.h"
@@ -23,11 +24,6 @@
 
 #define DPAA2_ETH_STORE_SIZE		16
 
-/* We set a max threshold for how many Tx confirmations we should process
- * on a NAPI poll call, they take less processing time.
- */
-#define TX_CONF_PER_NAPI_POLL		256
-
 /* Maximum number of scatter-gather entries in an ingress frame,
  * considering the maximum receive frame size is 64K
  */
@@ -55,6 +51,11 @@
  */
 #define DPAA2_ETH_TAILDROP_THRESH	(64 * 1024)
 
+/* Maximum number of Tx confirmation frames to be processed
+ * in a single NAPI call
+ */
+#define DPAA2_ETH_TXCONF_PER_NAPI	256
+
 /* Buffer quota per queue. Must be large enough such that for minimum sized
  * frames taildrop kicks in before the bpool gets depleted, so we compute
  * how many 64B frames fit inside the taildrop threshold and add a margin
@@ -75,15 +76,6 @@
 #define DPAA2_ETH_SKB_SIZE \
 	(DPAA2_ETH_RX_BUF_SIZE + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
 
-/* PTP nominal frequency 1GHz */
-#define DPAA2_PTP_NOMINAL_FREQ_PERIOD_NS 1
-
-/* Due to a limitation in WRIOP 1.0.0, the RX buffer data must be aligned
- * to 256B. For newer revisions, the requirement is only for 64B alignment
- */
-#define DPAA2_ETH_RX_BUF_ALIGN_REV1	256
-#define DPAA2_ETH_RX_BUF_ALIGN		64
-
 /* We are accommodating a skb backpointer and some S/G info
  * in the frame's software annotation. The hardware
  * options are either 0 or 64, so we choose the latter.
@@ -94,6 +86,15 @@
 #define DPAA2_ETH_RX_HWA_SIZE		64
 #define DPAA2_ETH_TX_HWA_SIZE		128
 
+/* PTP nominal frequency 1GHz */
+#define DPAA2_PTP_NOMINAL_FREQ_PERIOD_NS 1
+
+/* Due to a limitation in WRIOP 1.0.0, the RX buffer data must be aligned
+ * to 256B. For newer revisions, the requirement is only for 64B alignment
+ */
+#define DPAA2_ETH_RX_BUF_ALIGN_REV1	256
+#define DPAA2_ETH_RX_BUF_ALIGN		64
+
 /* We store different information in the software annotation area of a Tx frame
  * based on what type of frame it is
  */
@@ -130,6 +131,7 @@ struct dpaa2_eth_swa {
 #define DPAA2_FD_FRC_FASWOV		0x0800
 #define DPAA2_FD_FRC_FAICFDV		0x0400
 
+/* Error bits in FD CTRL */
 #define DPAA2_FD_RX_ERR_MASK		(FD_CTRL_SBE | FD_CTRL_FAERR)
 #define DPAA2_FD_TX_ERR_MASK		(FD_CTRL_UFD	| \
 					 FD_CTRL_SBE	| \
@@ -182,7 +184,7 @@ static inline struct dpaa2_fas *dpaa2_get_fas(void *buf_addr, bool swa)
 	return dpaa2_get_hwa(buf_addr, swa) + DPAA2_FAS_OFFSET;
 }
 
-static inline u64 *dpaa2_get_ts(void *buf_addr, bool swa)
+static inline __le64 *dpaa2_get_ts(void *buf_addr, bool swa)
 {
 	return dpaa2_get_hwa(buf_addr, swa) + DPAA2_TS_OFFSET;
 }
@@ -320,10 +322,10 @@ struct dpaa2_eth_fq {
 	struct dpaa2_eth_channel *channel;
 	enum dpaa2_eth_fq_type type;
 
-	void (*consume)(struct dpaa2_eth_priv *,
-			struct dpaa2_eth_channel *,
-			const struct dpaa2_fd *,
-			struct napi_struct *,
+	void (*consume)(struct dpaa2_eth_priv *priv,
+			struct dpaa2_eth_channel *ch,
+			const struct dpaa2_fd *fd,
+			struct napi_struct *napi,
 			u16 queue_id);
 	struct dpaa2_eth_fq_stats stats;
 };
@@ -345,37 +347,21 @@ struct dpaa2_eth_channel {
 	bool flush;
 };
 
-struct dpaa2_eth_cls_rule {
-	struct ethtool_rx_flow_spec fs;
-	bool in_use;
-};
-
 struct dpaa2_eth_dist_fields {
 	u64 rxnfc_field;
 	enum net_prot cls_prot;
 	int cls_field;
-	int offset;
 	int size;
-	u32 id;
+};
+
+struct dpaa2_eth_cls_rule {
+	struct ethtool_rx_flow_spec fs;
+	u8 in_use;
 };
 
 /* Driver private data */
 struct dpaa2_eth_priv {
 	struct net_device *net_dev;
-	/* Standard statistics */
-	struct rtnl_link_stats64 __percpu *percpu_stats;
-	/* Extra stats, in addition to the ones known by the kernel */
-	struct dpaa2_eth_drv_stats __percpu *percpu_extras;
-	bool ts_tx_en; /* Tx timestamping enabled */
-	bool ts_rx_en; /* Rx timestamping enabled */
-	u16 tx_data_offset;
-	u16 bpid;
-	u16 tx_qdid;
-	u16 rx_buf_align;
-	struct iommu_domain *iommu_domain;
-	int max_bufs_per_ch;
-	int refill_thresh;
-	bool has_xdp_prog;
 
 	void *cscn_mem;	/* Tx congestion notifications are written here */
 	void *cscn_unaligned;
@@ -386,37 +372,49 @@ struct dpaa2_eth_priv {
 
 	u8 num_channels;
 	struct dpaa2_eth_channel *channel[DPAA2_ETH_MAX_DPCONS];
+	int max_bufs_per_ch;
+	int refill_thresh;
 
-	struct dpni_attr dpni_attrs;
-	u16 dpni_ver_major;
-	u16 dpni_ver_minor;
+	bool has_xdp_prog;
 
-	struct fsl_mc_device *dpbp_dev;
+       struct dpni_attr dpni_attrs;
+       u16 dpni_ver_major;
+       u16 dpni_ver_minor;
+	u16 tx_data_offset;
 
-	struct fsl_mc_io *mc_io;
-	/* Cores which have an affine DPIO/DPCON.
-	 * This is the cpu set on which Rx and Tx conf frames are processed
-	 */
-	struct cpumask dpio_cpumask;
+       struct fsl_mc_device *dpbp_dev;
+	u16 bpid;
+	struct iommu_domain *iommu_domain;
 
-	u16 mc_token;
+	bool ts_tx_en; /* Tx timestamping enabled */
+	bool ts_rx_en; /* Rx timestamping enabled */
 
-	struct dpni_link_state link_state;
-	bool do_link_poll;
-	struct task_struct *poll_thread;
+	u16 tx_qdid;
+	u16 rx_buf_align;
+       struct fsl_mc_io *mc_io;
+       /* Cores which have an affine DPIO/DPCON.
+        * This is the cpu set on which Rx and Tx conf frames are processed
+        */
+       struct cpumask dpio_cpumask;
 
-	/* Rx distribution (hash and flow steering) header fields
-	 * supported by the driver
-	 */
-	struct dpaa2_eth_dist_fields *dist_fields;
-	u8 num_dist_fields;
-	/* enabled ethtool hashing bits */
-	u64 rx_hash_fields;
+	/* Standard statistics */
+	struct rtnl_link_stats64 __percpu *percpu_stats;
+	/* Extra stats, in addition to the ones known by the kernel */
+	struct dpaa2_eth_drv_stats __percpu *percpu_extras;
+
+       u16 mc_token;
+
+       struct dpni_link_state link_state;
+       bool do_link_poll;
+       struct task_struct *poll_thread;
+
+       /* enabled ethtool hashing bits */
+       u64 rx_hash_fields;
+	struct dpaa2_eth_cls_rule *cls_rule;
+	u8 rx_cls_enabled;
 #ifdef CONFIG_FSL_DPAA2_ETH_DEBUGFS
 	struct dpaa2_debugfs dbg;
 #endif
-	/* array of classification rules */
-	struct dpaa2_eth_cls_rule *cls_rule;
 	struct dpni_tx_shaping_cfg shaping_cfg;
 
 	u8 dcbx_mode;
@@ -427,42 +425,17 @@ struct dpaa2_eth_priv {
 	bool ceetm_en;
 };
 
-enum dpaa2_eth_rx_dist {
-	DPAA2_ETH_RX_DIST_HASH,
-	DPAA2_ETH_RX_DIST_FS,
-	DPAA2_ETH_RX_DIST_LEGACY
-};
+#define DPAA2_RXH_SUPPORTED	(RXH_L2DA | RXH_VLAN | RXH_L3_PROTO \
+				| RXH_IP_SRC | RXH_IP_DST | RXH_L4_B_0_1 \
+				| RXH_L4_B_2_3)
 
-/* Supported Rx distribution field ids */
-#define DPAA2_ETH_DIST_ETHSRC		BIT(0)
-#define DPAA2_ETH_DIST_ETHDST		BIT(1)
-#define DPAA2_ETH_DIST_ETHTYPE		BIT(2)
-#define DPAA2_ETH_DIST_VLAN		BIT(3)
-#define DPAA2_ETH_DIST_IPSRC		BIT(4)
-#define DPAA2_ETH_DIST_IPDST		BIT(5)
-#define DPAA2_ETH_DIST_IPPROTO		BIT(6)
-#define DPAA2_ETH_DIST_L4SRC		BIT(7)
-#define DPAA2_ETH_DIST_L4DST		BIT(8)
-#define DPAA2_ETH_DIST_ALL		(~0U)
-
-/* Default Rx hash key */
-#define DPAA2_ETH_DIST_DEFAULT_HASH \
-	(DPAA2_ETH_DIST_IPPROTO | \
-	 DPAA2_ETH_DIST_IPSRC | DPAA2_ETH_DIST_IPDST | \
-	 DPAA2_ETH_DIST_L4SRC | DPAA2_ETH_DIST_L4DST)
+/* default Rx hash options, set during probing */
+#define DPAA2_RXH_DEFAULT	(RXH_L3_PROTO | RXH_IP_SRC | RXH_IP_DST | \
+				 RXH_L4_B_0_1 | RXH_L4_B_2_3)
 
 #define dpaa2_eth_hash_enabled(priv)	\
 	((priv)->dpni_attrs.num_queues > 1)
 
-#define dpaa2_eth_fs_enabled(priv)	\
-	(!((priv)->dpni_attrs.options & DPNI_OPT_NO_FS))
-
-#define dpaa2_eth_fs_mask_enabled(priv)	\
-	((priv)->dpni_attrs.options & DPNI_OPT_HAS_KEY_MASKING)
-
-#define dpaa2_eth_fs_count(priv)	\
-	((priv)->dpni_attrs.fs_entries)
-
 /* Required by struct dpni_rx_tc_dist_cfg::key_cfg_iova */
 #define DPAA2_CLASSIFIER_DMA_SIZE 256
 
@@ -478,14 +451,29 @@ static inline int dpaa2_eth_cmp_dpni_ver(struct dpaa2_eth_priv *priv,
 	return priv->dpni_ver_major - ver_major;
 }
 
-#define DPNI_DIST_KEY_VER_MAJOR			7
-#define DPNI_DIST_KEY_VER_MINOR			5
+/* Minimum firmware version that supports a more flexible API
+ * for configuring the Rx flow hash key
+ */
+#define DPNI_RX_DIST_KEY_VER_MAJOR	7
+#define DPNI_RX_DIST_KEY_VER_MINOR	5
 
-static inline bool dpaa2_eth_has_legacy_dist(struct dpaa2_eth_priv *priv)
-{
-	return (dpaa2_eth_cmp_dpni_ver(priv, DPNI_DIST_KEY_VER_MAJOR,
-				       DPNI_DIST_KEY_VER_MINOR) < 0);
-}
+#define dpaa2_eth_has_legacy_dist(priv)					\
+	(dpaa2_eth_cmp_dpni_ver((priv), DPNI_RX_DIST_KEY_VER_MAJOR,	\
+				DPNI_RX_DIST_KEY_VER_MINOR) < 0)
+
+#define dpaa2_eth_fs_count(priv)	\
+	((priv)->dpni_attrs.fs_entries)
+
+#define dpaa2_eth_queue_count(priv)	\
+	((priv)->dpni_attrs.num_queues)
+
+#define dpaa2_eth_tc_count(priv)	\
+	((priv)->dpni_attrs.num_tcs)
+
+enum dpaa2_eth_rx_dist {
+	DPAA2_ETH_RX_DIST_HASH,
+	DPAA2_ETH_RX_DIST_CLS
+};
 
 /* Hardware only sees DPAA2_ETH_RX_BUF_SIZE, but the skb built around
  * the buffer also needs space for its shared info struct, and we need
@@ -496,9 +484,9 @@ static inline unsigned int dpaa2_eth_buf_raw_size(struct dpaa2_eth_priv *priv)
 	return DPAA2_ETH_SKB_SIZE + priv->rx_buf_align;
 }
 
-/* Total headroom needed by the hardware in Tx frame buffers */
-static inline unsigned int
-dpaa2_eth_needed_headroom(struct dpaa2_eth_priv *priv, struct sk_buff *skb)
+static inline
+unsigned int dpaa2_eth_needed_headroom(struct dpaa2_eth_priv *priv,
+				       struct sk_buff *skb)
 {
 	unsigned int headroom = DPAA2_ETH_SWA_SIZE;
 
@@ -530,16 +518,6 @@ static inline unsigned int dpaa2_eth_rx_headroom(struct dpaa2_eth_priv *priv)
 	       DPAA2_ETH_RX_HWA_SIZE;
 }
 
-static inline int dpaa2_eth_queue_count(struct dpaa2_eth_priv *priv)
-{
-	return priv->dpni_attrs.num_queues;
-}
-
-static inline int dpaa2_eth_tc_count(struct dpaa2_eth_priv *priv)
-{
-	return priv->dpni_attrs.num_tcs;
-}
-
 static inline bool dpaa2_eth_is_pfc_enabled(struct dpaa2_eth_priv *priv,
 					    int traffic_class)
 {
@@ -570,11 +548,10 @@ static inline int dpaa2_eth_ch_count(struct dpaa2_eth_priv *priv)
 	return 1;
 }
 
-void check_cls_support(struct dpaa2_eth_priv *priv);
+int dpaa2_eth_set_hash(struct net_device *net_dev, u64 flags);
+int dpaa2_eth_cls_key_size(void);
+int dpaa2_eth_cls_fld_off(int prot, int field);
 
 int set_rx_taildrop(struct dpaa2_eth_priv *priv);
 
-int dpaa2_eth_set_dist_key(struct dpaa2_eth_priv *priv,
-			   enum dpaa2_eth_rx_dist type, u32 key_fields);
-
 #endif	/* __DPAA2_H */
diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-ethtool.c b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-ethtool.c
index 7171d8a0542d..ab6e5847420c 100644
--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-ethtool.c
+++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-ethtool.c
@@ -46,14 +46,12 @@ static char dpaa2_ethtool_extras[][ETH_GSTRING_LEN] = {
 	"[drv] channel pull errors",
 	"[drv] cdan",
 	"[drv] tx congestion state",
-#ifdef CONFIG_FSL_QBMAN_DEBUG
 	/* FQ stats */
 	"rx pending frames",
 	"rx pending bytes",
 	"tx conf pending frames",
 	"tx conf pending bytes",
 	"buffer count"
-#endif
 };
 
 #define DPAA2_ETH_NUM_EXTRA_STATS	ARRAY_SIZE(dpaa2_ethtool_extras)
@@ -64,8 +62,6 @@ static void dpaa2_eth_get_drvinfo(struct net_device *net_dev,
 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
 
 	strlcpy(drvinfo->driver, KBUILD_MODNAME, sizeof(drvinfo->driver));
-	strlcpy(drvinfo->version, dpaa2_eth_drv_version,
-		sizeof(drvinfo->version));
 
 	snprintf(drvinfo->fw_version, sizeof(drvinfo->fw_version),
 		 "%u.%u", priv->dpni_ver_major, priv->dpni_ver_minor);
@@ -257,13 +253,10 @@ static void dpaa2_eth_get_ethtool_stats(struct net_device *net_dev,
 	int j, k, err;
 	int num_cnt;
 	union dpni_statistics dpni_stats;
-
-#ifdef CONFIG_FSL_QBMAN_DEBUG
 	u32 fcnt, bcnt;
 	u32 fcnt_rx_total = 0, fcnt_tx_total = 0;
 	u32 bcnt_rx_total = 0, bcnt_tx_total = 0;
 	u32 buf_cnt;
-#endif
 	u64 cdan = 0;
 	u64 portal_busy = 0, pull_err = 0;
 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
@@ -315,7 +308,6 @@ static void dpaa2_eth_get_ethtool_stats(struct net_device *net_dev,
 
 	*(data + i++) = dpaa2_cscn_state_congested(priv->cscn_mem);
 
-#ifdef CONFIG_FSL_QBMAN_DEBUG
 	for (j = 0; j < priv->num_fqs; j++) {
 		/* Print FQ instantaneous counts */
 		err = dpaa2_io_query_fq_count(NULL, priv->fq[j].fqid,
@@ -345,279 +337,202 @@ static void dpaa2_eth_get_ethtool_stats(struct net_device *net_dev,
 		return;
 	}
 	*(data + i++) = buf_cnt;
-#endif
 }
 
-static int cls_key_off(struct dpaa2_eth_priv *priv, int prot, int field)
+static int prep_eth_rule(struct ethhdr *eth_value, struct ethhdr *eth_mask,
+			 void *key, void *mask)
 {
-	int i, off = 0;
+	int off;
 
-	for (i = 0; i < priv->num_dist_fields; i++) {
-		if (priv->dist_fields[i].cls_prot == prot &&
-		    priv->dist_fields[i].cls_field == field)
-			return off;
-		off += priv->dist_fields[i].size;
+	if (eth_mask->h_proto) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_ETH, NH_FLD_ETH_TYPE);
+		*(__be16 *)(key + off) = eth_value->h_proto;
+		*(__be16 *)(mask + off) = eth_mask->h_proto;
 	}
 
-	return -1;
-}
-
-static u8 cls_key_size(struct dpaa2_eth_priv *priv)
-{
-	u8 i, size = 0;
-
-	for (i = 0; i < priv->num_dist_fields; i++)
-		size += priv->dist_fields[i].size;
-
-	return size;
-}
-
-void check_cls_support(struct dpaa2_eth_priv *priv)
-{
-	u8 key_size = cls_key_size(priv);
-	struct device *dev = priv->net_dev->dev.parent;
-
-	if (dpaa2_eth_hash_enabled(priv)) {
-		if (priv->dpni_attrs.fs_key_size < key_size) {
-			dev_info(dev, "max_dist_key_size = %d, expected %d. Hashing and steering are disabled\n",
-				 priv->dpni_attrs.fs_key_size,
-				 key_size);
-			goto disable_fs;
-		}
-		if (priv->num_dist_fields > DPKG_MAX_NUM_OF_EXTRACTS) {
-			dev_info(dev, "Too many key fields (max = %d). Hashing and steering are disabled\n",
-				 DPKG_MAX_NUM_OF_EXTRACTS);
-			goto disable_fs;
-		}
+	if (!is_zero_ether_addr(eth_mask->h_source)) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_ETH, NH_FLD_ETH_SA);
+		ether_addr_copy(key + off, eth_value->h_source);
+		ether_addr_copy(mask + off, eth_mask->h_source);
 	}
 
-	if (dpaa2_eth_fs_enabled(priv)) {
-		if (!dpaa2_eth_hash_enabled(priv)) {
-			dev_info(dev, "Insufficient queues. Steering is disabled\n");
-			goto disable_fs;
-		}
-
-		if (!dpaa2_eth_fs_mask_enabled(priv)) {
-			dev_info(dev, "Key masks not supported. Steering is disabled\n");
-			goto disable_fs;
-		}
+	if (!is_zero_ether_addr(eth_mask->h_dest)) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_ETH, NH_FLD_ETH_DA);
+		ether_addr_copy(key + off, eth_value->h_dest);
+		ether_addr_copy(mask + off, eth_mask->h_dest);
 	}
 
-	return;
-
-disable_fs:
-	priv->dpni_attrs.options |= DPNI_OPT_NO_FS;
-	priv->dpni_attrs.options &= ~DPNI_OPT_HAS_KEY_MASKING;
+	return 0;
 }
 
-static int prep_l4_rule(struct dpaa2_eth_priv *priv,
-			struct ethtool_tcpip4_spec *l4_value,
-			struct ethtool_tcpip4_spec *l4_mask,
-			void *key, void *mask, u8 l4_proto)
+static int prep_user_ip_rule(struct ethtool_usrip4_spec *uip_value,
+			     struct ethtool_usrip4_spec *uip_mask,
+			     void *key, void *mask)
 {
-	int offset;
+	int off;
+	u32 tmp_value, tmp_mask;
 
-	if (l4_mask->tos) {
-		netdev_err(priv->net_dev, "ToS is not supported for IPv4 L4\n");
+	if (uip_mask->tos || uip_mask->ip_ver)
 		return -EOPNOTSUPP;
-	}
-
-	if (l4_mask->ip4src) {
-		offset = cls_key_off(priv, NET_PROT_IP, NH_FLD_IP_SRC);
-		*(u32 *)(key + offset) = l4_value->ip4src;
-		*(u32 *)(mask + offset) = l4_mask->ip4src;
-	}
 
-	if (l4_mask->ip4dst) {
-		offset = cls_key_off(priv, NET_PROT_IP, NH_FLD_IP_DST);
-		*(u32 *)(key + offset) = l4_value->ip4dst;
-		*(u32 *)(mask + offset) = l4_mask->ip4dst;
+	if (uip_mask->ip4src) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_IP, NH_FLD_IP_SRC);
+		*(__be32 *)(key + off) = uip_value->ip4src;
+		*(__be32 *)(mask + off) = uip_mask->ip4src;
 	}
 
-	if (l4_mask->psrc) {
-		offset = cls_key_off(priv, NET_PROT_UDP, NH_FLD_UDP_PORT_SRC);
-		*(u32 *)(key + offset) = l4_value->psrc;
-		*(u32 *)(mask + offset) = l4_mask->psrc;
+	if (uip_mask->ip4dst) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_IP, NH_FLD_IP_DST);
+		*(__be32 *)(key + off) = uip_value->ip4dst;
+		*(__be32 *)(mask + off) = uip_mask->ip4dst;
 	}
 
-	if (l4_mask->pdst) {
-		offset = cls_key_off(priv, NET_PROT_UDP, NH_FLD_UDP_PORT_DST);
-		*(u32 *)(key + offset) = l4_value->pdst;
-		*(u32 *)(mask + offset) = l4_mask->pdst;
+	if (uip_mask->proto) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_IP, NH_FLD_IP_PROTO);
+		*(u8 *)(key + off) = uip_value->proto;
+		*(u8 *)(mask + off) = uip_mask->proto;
 	}
 
-	/* Only apply the rule for the user-specified L4 protocol
-	 * and if ethertype matches IPv4
-	 */
-	offset = cls_key_off(priv, NET_PROT_ETH, NH_FLD_ETH_TYPE);
-	*(u16 *)(key + offset) = htons(ETH_P_IP);
-	*(u16 *)(mask + offset) = 0xFFFF;
-
-	offset = cls_key_off(priv, NET_PROT_IP, NH_FLD_IP_PROTO);
-	*(u8 *)(key + offset) = l4_proto;
-	*(u8 *)(mask + offset) = 0xFF;
-
-	/* TODO: check IP version */
-
-	return 0;
-}
-
-static int prep_eth_rule(struct dpaa2_eth_priv *priv,
-			 struct ethhdr *eth_value, struct ethhdr *eth_mask,
-			 void *key, void *mask)
-{
-	int offset;
+	if (uip_mask->l4_4_bytes) {
+		tmp_value = be32_to_cpu(uip_value->l4_4_bytes);
+		tmp_mask = be32_to_cpu(uip_mask->l4_4_bytes);
 
-	if (eth_mask->h_proto) {
-		netdev_err(priv->net_dev, "Ethertype is not supported!\n");
-		return -EOPNOTSUPP;
-	}
+		off = dpaa2_eth_cls_fld_off(NET_PROT_UDP, NH_FLD_UDP_PORT_SRC);
+		*(__be16 *)(key + off) = htons(tmp_value >> 16);
+		*(__be16 *)(mask + off) = htons(tmp_mask >> 16);
 
-	if (!is_zero_ether_addr(eth_mask->h_source)) {
-		offset = cls_key_off(priv, NET_PROT_ETH, NH_FLD_ETH_SA);
-		ether_addr_copy(key + offset, eth_value->h_source);
-		ether_addr_copy(mask + offset, eth_mask->h_source);
+		off = dpaa2_eth_cls_fld_off(NET_PROT_UDP, NH_FLD_UDP_PORT_DST);
+		*(__be16 *)(key + off) = htons(tmp_value & 0xFFFF);
+		*(__be16 *)(mask + off) = htons(tmp_mask & 0xFFFF);
 	}
 
-	if (!is_zero_ether_addr(eth_mask->h_dest)) {
-		offset = cls_key_off(priv, NET_PROT_ETH, NH_FLD_ETH_DA);
-		ether_addr_copy(key + offset, eth_value->h_dest);
-		ether_addr_copy(mask + offset, eth_mask->h_dest);
-	}
+	/* Only apply the rule for IPv4 frames */
+	off = dpaa2_eth_cls_fld_off(NET_PROT_ETH, NH_FLD_ETH_TYPE);
+	*(__be16 *)(key + off) = htons(ETH_P_IP);
+	*(__be16 *)(mask + off) = htons(0xFFFF);
 
 	return 0;
 }
 
-static int prep_user_ip_rule(struct dpaa2_eth_priv *priv,
-			     struct ethtool_usrip4_spec *uip_value,
-			     struct ethtool_usrip4_spec *uip_mask,
-			     void *key, void *mask)
+static int prep_l4_rule(struct ethtool_tcpip4_spec *l4_value,
+			struct ethtool_tcpip4_spec *l4_mask,
+			void *key, void *mask, u8 l4_proto)
 {
-	int offset;
+	int off;
 
-	if (uip_mask->tos)
+	if (l4_mask->tos)
 		return -EOPNOTSUPP;
-
-	if (uip_mask->ip4src) {
-		offset = cls_key_off(priv, NET_PROT_IP, NH_FLD_IP_SRC);
-		*(u32 *)(key + offset) = uip_value->ip4src;
-		*(u32 *)(mask + offset) = uip_mask->ip4src;
+	if (l4_mask->ip4src) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_IP, NH_FLD_IP_SRC);
+		*(__be32 *)(key + off) = l4_value->ip4src;
+		*(__be32 *)(mask + off) = l4_mask->ip4src;
 	}
 
-	if (uip_mask->ip4dst) {
-		offset = cls_key_off(priv, NET_PROT_IP, NH_FLD_IP_DST);
-		*(u32 *)(key + offset) = uip_value->ip4dst;
-		*(u32 *)(mask + offset) = uip_mask->ip4dst;
+	if (l4_mask->ip4dst) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_IP, NH_FLD_IP_DST);
+		*(__be32 *)(key + off) = l4_value->ip4dst;
+		*(__be32 *)(mask + off) = l4_mask->ip4dst;
 	}
 
-	if (uip_mask->proto) {
-		offset = cls_key_off(priv, NET_PROT_IP, NH_FLD_IP_PROTO);
-		*(u32 *)(key + offset) = uip_value->proto;
-		*(u32 *)(mask + offset) = uip_mask->proto;
+	if (l4_mask->psrc) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_UDP, NH_FLD_UDP_PORT_SRC);
+		*(__be16 *)(key + off) = l4_value->psrc;
+		*(__be16 *)(mask + off) = l4_mask->psrc;
 	}
-	if (uip_mask->l4_4_bytes) {
-		offset = cls_key_off(priv, NET_PROT_UDP, NH_FLD_UDP_PORT_SRC);
-		*(u16 *)(key + offset) = uip_value->l4_4_bytes << 16;
-		*(u16 *)(mask + offset) = uip_mask->l4_4_bytes << 16;
 
-		offset = cls_key_off(priv, NET_PROT_UDP, NH_FLD_UDP_PORT_DST);
-		*(u16 *)(key + offset) = uip_value->l4_4_bytes & 0xFFFF;
-		*(u16 *)(mask + offset) = uip_mask->l4_4_bytes & 0xFFFF;
+	if (l4_mask->pdst) {
+		off = dpaa2_eth_cls_fld_off(NET_PROT_UDP, NH_FLD_UDP_PORT_DST);
+		*(__be16 *)(key + off) = l4_value->pdst;
+		*(__be16 *)(mask + off) = l4_mask->pdst;
 	}
 
-	/* Ethertype must be IP */
-	offset = cls_key_off(priv, NET_PROT_ETH, NH_FLD_ETH_TYPE);
-	*(u16 *)(key + offset) = htons(ETH_P_IP);
-	*(u16 *)(mask + offset) = 0xFFFF;
+	/* Only apply the rule for the user-specified L4 protocol
+	 * and if ethertype matches IPv4
+	 */
+	off = dpaa2_eth_cls_fld_off(NET_PROT_ETH, NH_FLD_ETH_TYPE);
+	*(__be16 *)(key + off) = htons(ETH_P_IP);
+	*(__be16 *)(mask + off) = htons(0xFFFF);
+
+	off = dpaa2_eth_cls_fld_off(NET_PROT_IP, NH_FLD_IP_PROTO);
+	*(u8 *)(key + off) = l4_proto;
+	*(u8 *)(mask + off) = 0xFF;
 
 	return 0;
 }
 
-static int prep_ext_rule(struct dpaa2_eth_priv *priv,
-			 struct ethtool_flow_ext *ext_value,
+static int prep_ext_rule(struct ethtool_flow_ext *ext_value,
 			 struct ethtool_flow_ext *ext_mask,
 			 void *key, void *mask)
 {
-	int offset;
+	int off;
 
 	if (ext_mask->vlan_etype)
 		return -EOPNOTSUPP;
 
 	if (ext_mask->vlan_tci) {
-		offset = cls_key_off(priv, NET_PROT_VLAN, NH_FLD_VLAN_TCI);
-		*(u16 *)(key + offset) = ext_value->vlan_tci;
-		*(u16 *)(mask + offset) = ext_mask->vlan_tci;
+		off = dpaa2_eth_cls_fld_off(NET_PROT_VLAN, NH_FLD_VLAN_TCI);
+		*(__be16 *)(key + off) = ext_value->vlan_tci;
+		*(__be16 *)(mask + off) = ext_mask->vlan_tci;
 	}
 
 	return 0;
 }
 
-static int prep_mac_ext_rule(struct dpaa2_eth_priv *priv,
-			     struct ethtool_flow_ext *ext_value,
+static int prep_mac_ext_rule(struct ethtool_flow_ext *ext_value,
 			     struct ethtool_flow_ext *ext_mask,
 			     void *key, void *mask)
 {
-	int offset;
+	int off;
 
 	if (!is_zero_ether_addr(ext_mask->h_dest)) {
-		offset = cls_key_off(priv, NET_PROT_ETH, NH_FLD_ETH_DA);
-		ether_addr_copy(key + offset, ext_value->h_dest);
-		ether_addr_copy(mask + offset, ext_mask->h_dest);
+		off = dpaa2_eth_cls_fld_off(NET_PROT_ETH, NH_FLD_ETH_DA);
+		ether_addr_copy(key + off, ext_value->h_dest);
+		ether_addr_copy(mask + off, ext_mask->h_dest);
 	}
 
 	return 0;
 }
 
-static int prep_cls_rule(struct net_device *net_dev,
-			 struct ethtool_rx_flow_spec *fs,
-			 void *key)
+static int prep_cls_rule(struct ethtool_rx_flow_spec *fs, void *key, void *mask)
 {
-	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
-	const u8 key_size = cls_key_size(priv);
-	void *msk = key + key_size;
 	int err;
 
-	memset(key, 0, key_size * 2);
-
-	switch (fs->flow_type & 0xff) {
+	switch (fs->flow_type & 0xFF) {
+	case ETHER_FLOW:
+		err = prep_eth_rule(&fs->h_u.ether_spec, &fs->m_u.ether_spec,
+				    key, mask);
+		break;
+	case IP_USER_FLOW:
+		err = prep_user_ip_rule(&fs->h_u.usr_ip4_spec,
+				    &fs->m_u.usr_ip4_spec, key, mask);
+		break;
 	case TCP_V4_FLOW:
-		err = prep_l4_rule(priv, &fs->h_u.tcp_ip4_spec,
-				   &fs->m_u.tcp_ip4_spec, key, msk,
-				   IPPROTO_TCP);
+		err = prep_l4_rule(&fs->h_u.tcp_ip4_spec, &fs->m_u.tcp_ip4_spec,
+				   key, mask, IPPROTO_TCP);
 		break;
 	case UDP_V4_FLOW:
-		err = prep_l4_rule(priv, &fs->h_u.udp_ip4_spec,
-				   &fs->m_u.udp_ip4_spec, key, msk,
-				   IPPROTO_UDP);
+		err = prep_l4_rule(&fs->h_u.udp_ip4_spec, &fs->m_u.udp_ip4_spec,
+				   key, mask, IPPROTO_UDP);
 		break;
 	case SCTP_V4_FLOW:
-		err = prep_l4_rule(priv, &fs->h_u.sctp_ip4_spec,
-				   &fs->m_u.sctp_ip4_spec, key, msk,
-				   IPPROTO_SCTP);
-		break;
-	case ETHER_FLOW:
-		err = prep_eth_rule(priv, &fs->h_u.ether_spec,
-				    &fs->m_u.ether_spec, key, msk);
-		break;
-	case IP_USER_FLOW:
-		err = prep_user_ip_rule(priv, &fs->h_u.usr_ip4_spec,
-					&fs->m_u.usr_ip4_spec, key, msk);
+		err = prep_l4_rule(&fs->h_u.sctp_ip4_spec, &fs->m_u.sctp_ip4_spec,
+				   key, mask, IPPROTO_SCTP);
 		break;
 	default:
-		/* TODO: AH, ESP */
 		return -EOPNOTSUPP;
 	}
+
 	if (err)
 		return err;
 
 	if (fs->flow_type & FLOW_EXT) {
-		err = prep_ext_rule(priv, &fs->h_ext, &fs->m_ext, key, msk);
+		err = prep_ext_rule(&fs->h_ext, &fs->m_ext, key, mask);
 		if (err)
 			return err;
 	}
 
 	if (fs->flow_type & FLOW_MAC_EXT) {
-		err = prep_mac_ext_rule(priv, &fs->h_ext, &fs->m_ext, key, msk);
+		err = prep_mac_ext_rule(&fs->h_ext, &fs->m_ext, key, mask);
 		if (err)
 			return err;
 	}
@@ -625,161 +540,116 @@ static int prep_cls_rule(struct net_device *net_dev,
 	return 0;
 }
 
-static int del_cls(struct net_device *net_dev, int location);
-
-static int do_cls(struct net_device *net_dev,
-		  struct ethtool_rx_flow_spec *fs,
-		  bool add)
+static int do_cls_rule(struct net_device *net_dev,
+		       struct ethtool_rx_flow_spec *fs,
+		       bool add)
 {
 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
 	struct device *dev = net_dev->dev.parent;
-	const int rule_cnt = dpaa2_eth_fs_count(priv);
-	struct dpni_rule_cfg rule_cfg;
+	struct dpni_rule_cfg rule_cfg = { 0 };
 	struct dpni_fs_action_cfg fs_act = { 0 };
-	void *dma_mem;
-	int err = 0, tc;
+	dma_addr_t key_iova;
+	void *key_buf;
+	int i, err = 0;
 
-	if (!dpaa2_eth_fs_enabled(priv)) {
-		netdev_err(net_dev, "dev does not support steering!\n");
-		/* dev doesn't support steering */
-		return -EOPNOTSUPP;
-	}
-
-	if ((fs->ring_cookie != RX_CLS_FLOW_DISC &&
-	     fs->ring_cookie >= dpaa2_eth_queue_count(priv)) ||
-	     fs->location >= rule_cnt)
+	if (fs->ring_cookie != RX_CLS_FLOW_DISC &&
+	    fs->ring_cookie >= dpaa2_eth_queue_count(priv))
 		return -EINVAL;
 
-	/* When adding a new rule, check if location if available
-	 * and if not, free the existing table entry before inserting
-	 * the new one
-	 */
-	if (add && (priv->cls_rule[fs->location].in_use == true))
-		del_cls(net_dev, fs->location);
-
-	memset(&rule_cfg, 0, sizeof(rule_cfg));
-	rule_cfg.key_size = cls_key_size(priv);
+	rule_cfg.key_size = dpaa2_eth_cls_key_size();
 
 	/* allocate twice the key size, for the actual key and for mask */
-	dma_mem = kzalloc(rule_cfg.key_size * 2, GFP_DMA | GFP_KERNEL);
-	if (!dma_mem)
+	key_buf = kzalloc(rule_cfg.key_size * 2, GFP_KERNEL);
+	if (!key_buf)
 		return -ENOMEM;
 
-	err = prep_cls_rule(net_dev, fs, dma_mem);
+	/* Fill the key and mask memory areas */
+	err = prep_cls_rule(fs, key_buf, key_buf + rule_cfg.key_size);
 	if (err)
-		goto err_free_mem;
+		goto free_mem;
 
-	rule_cfg.key_iova = dma_map_single(dev, dma_mem,
-					   rule_cfg.key_size * 2,
-					   DMA_TO_DEVICE);
+	key_iova = dma_map_single(dev, key_buf, rule_cfg.key_size * 2,
+				  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, key_iova)) {
+		err = -ENOMEM;
+		goto free_mem;
+	}
 
-	rule_cfg.mask_iova = rule_cfg.key_iova + rule_cfg.key_size;
+	rule_cfg.key_iova = key_iova;
+	rule_cfg.mask_iova = key_iova + rule_cfg.key_size;
 
-	if (fs->ring_cookie == RX_CLS_FLOW_DISC)
-		fs_act.options |= DPNI_FS_OPT_DISCARD;
-	else
-		fs_act.flow_id = fs->ring_cookie;
-
-	for (tc = 0; tc < dpaa2_eth_tc_count(priv); tc++) {
+	if (add) {
+		if (fs->ring_cookie == RX_CLS_FLOW_DISC)
+			fs_act.options |= DPNI_FS_OPT_DISCARD;
+		else
+			fs_act.flow_id = fs->ring_cookie;
+	}
+	for (i = 0; i < dpaa2_eth_tc_count(priv); i++) {
 		if (add)
 			err = dpni_add_fs_entry(priv->mc_io, 0, priv->mc_token,
-						tc, fs->location, &rule_cfg,
+						i, fs->location, &rule_cfg,
 						&fs_act);
 		else
 			err = dpni_remove_fs_entry(priv->mc_io, 0,
-						   priv->mc_token, tc,
+						   priv->mc_token, i,
 						   &rule_cfg);
-
 		if (err)
 			break;
 	}
 
-	dma_unmap_single(dev, rule_cfg.key_iova,
-			 rule_cfg.key_size * 2, DMA_TO_DEVICE);
+	dma_unmap_single(dev, key_iova, rule_cfg.key_size * 2, DMA_TO_DEVICE);
 
-	if (err)
-		netdev_err(net_dev, "dpaa2_add/remove_cls() error %d\n", err);
-
-err_free_mem:
-	kfree(dma_mem);
+free_mem:
+	kfree(key_buf);
 
 	return err;
 }
 
-static int add_cls(struct net_device *net_dev,
-		   struct ethtool_rx_flow_spec *fs)
+static int update_cls_rule(struct net_device *net_dev,
+			   struct ethtool_rx_flow_spec *new_fs,
+			   int location)
 {
 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
-	int err;
+	struct dpaa2_eth_cls_rule *rule;
+	int err = -EINVAL;
 
-	err = do_cls(net_dev, fs, true);
-	if (err)
-		return err;
+	if (!priv->rx_cls_enabled)
+		return -EOPNOTSUPP;
 
-	priv->cls_rule[fs->location].in_use = true;
-	priv->cls_rule[fs->location].fs = *fs;
+	if (location >= dpaa2_eth_fs_count(priv))
+		return -EINVAL;
 
-	return 0;
-}
+	rule = &priv->cls_rule[location];
 
-static int del_cls(struct net_device *net_dev, int location)
-{
-	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
-	int err;
+	/* If a rule is present at the specified location, delete it. */
+	if (rule->in_use) {
+		err = do_cls_rule(net_dev, &rule->fs, false);
+		if (err)
+			return err;
+
+		rule->in_use = 0;
+	}
 
-	err = do_cls(net_dev, &priv->cls_rule[location].fs, false);
+	/* If no new entry to add, return here */
+	if (!new_fs)
+		return err;
+
+	err = do_cls_rule(net_dev, new_fs, true);
 	if (err)
 		return err;
 
-	priv->cls_rule[location].in_use = false;
+	rule->in_use = 1;
+	rule->fs = *new_fs;
 
 	return 0;
 }
 
-static int set_hash(struct net_device *net_dev, u64 data)
-{
-	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
-	u32 key = 0;
-	int i;
-
-	if (data & RXH_DISCARD)
-		return -EOPNOTSUPP;
-
-	for (i = 0; i < priv->num_dist_fields; i++)
-		if (priv->dist_fields[i].rxnfc_field & data)
-			key |= priv->dist_fields[i].id;
-
-	return dpaa2_eth_set_dist_key(priv, DPAA2_ETH_RX_DIST_HASH, key);
-}
-
-static int dpaa2_eth_set_rxnfc(struct net_device *net_dev,
-			       struct ethtool_rxnfc *rxnfc)
-{
-	int err = 0;
-
-	switch (rxnfc->cmd) {
-	case ETHTOOL_SRXCLSRLINS:
-		err = add_cls(net_dev, &rxnfc->fs);
-		break;
-	case ETHTOOL_SRXCLSRLDEL:
-		err = del_cls(net_dev, rxnfc->fs.location);
-		break;
-	case ETHTOOL_SRXFH:
-		err = set_hash(net_dev, rxnfc->data);
-		break;
-	default:
-		err = -EOPNOTSUPP;
-	}
-
-	return err;
-}
-
 static int dpaa2_eth_get_rxnfc(struct net_device *net_dev,
 			       struct ethtool_rxnfc *rxnfc, u32 *rule_locs)
 {
 	struct dpaa2_eth_priv *priv = netdev_priv(net_dev);
-	const int rule_cnt = dpaa2_eth_fs_count(priv);
-	int i, j;
+	int rule_cnt = dpaa2_eth_fs_count(priv);
+	int i, j = 0;
 
 	switch (rxnfc->cmd) {
 	case ETHTOOL_GRXFH:
@@ -792,23 +662,22 @@ static int dpaa2_eth_get_rxnfc(struct net_device *net_dev,
 	case ETHTOOL_GRXRINGS:
 		rxnfc->data = dpaa2_eth_queue_count(priv);
 		break;
-
 	case ETHTOOL_GRXCLSRLCNT:
-		for (i = 0, rxnfc->rule_cnt = 0; i < rule_cnt; i++)
+		rxnfc->rule_cnt = 0;
+		for (i = 0; i < rule_cnt; i++)
 			if (priv->cls_rule[i].in_use)
 				rxnfc->rule_cnt++;
 		rxnfc->data = rule_cnt;
 		break;
-
 	case ETHTOOL_GRXCLSRULE:
+		if (rxnfc->fs.location >= rule_cnt)
+			return -EINVAL;
 		if (!priv->cls_rule[rxnfc->fs.location].in_use)
 			return -EINVAL;
-
 		rxnfc->fs = priv->cls_rule[rxnfc->fs.location].fs;
 		break;
-
 	case ETHTOOL_GRXCLSRLALL:
-		for (i = 0, j = 0; i < rule_cnt; i++) {
+		for (i = 0; i < rule_cnt; i++) {
 			if (!priv->cls_rule[i].in_use)
 				continue;
 			if (j == rxnfc->rule_cnt)
@@ -818,7 +687,6 @@ static int dpaa2_eth_get_rxnfc(struct net_device *net_dev,
 		rxnfc->rule_cnt = j;
 		rxnfc->data = rule_cnt;
 		break;
-
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -846,6 +714,30 @@ static int dpaa2_eth_get_ts_info(struct net_device *dev,
 	return 0;
 }
 
+static int dpaa2_eth_set_rxnfc(struct net_device *net_dev,
+			       struct ethtool_rxnfc *rxnfc)
+{
+	int err = 0;
+
+	switch (rxnfc->cmd) {
+	case ETHTOOL_SRXFH:
+		if ((rxnfc->data & DPAA2_RXH_SUPPORTED) != rxnfc->data)
+			return -EOPNOTSUPP;
+		err = dpaa2_eth_set_hash(net_dev, rxnfc->data);
+		break;
+	case ETHTOOL_SRXCLSRLINS:
+		err = update_cls_rule(net_dev, &rxnfc->fs, rxnfc->fs.location);
+		break;
+	case ETHTOOL_SRXCLSRLDEL:
+		err = update_cls_rule(net_dev, NULL, rxnfc->fs.location);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return err;
+}
+
 const struct ethtool_ops dpaa2_ethtool_ops = {
 	.get_drvinfo = dpaa2_eth_get_drvinfo,
 	.get_link = ethtool_op_get_link,
diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpni.c b/drivers/staging/fsl-dpaa2/ethernet/dpni.c
index c9394e1c4614..aaa7c1b28813 100644
--- a/drivers/staging/fsl-dpaa2/ethernet/dpni.c
+++ b/drivers/staging/fsl-dpaa2/ethernet/dpni.c
@@ -1514,84 +1514,6 @@ int dpni_remove_qos_entry(struct fsl_mc_io *mc_io,
 	return mc_send_command(mc_io, &cmd);
 }
 
-/**
- * dpni_add_fs_entry() - Add Flow Steering entry for a specific traffic class
- *			(to select a flow ID)
- * @mc_io:	Pointer to MC portal's I/O object
- * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
- * @token:	Token of DPNI object
- * @tc_id:	Traffic class selection (0-7)
- * @index:	Location in the QoS table where to insert the entry.
- *		Only relevant if MASKING is enabled for QoS
- *		classification on this DPNI, it is ignored for exact match.
- * @cfg:	Flow steering rule to add
- * @action:	Action to be taken as result of a classification hit
- *
- * Return:	'0' on Success; Error code otherwise.
- */
-int dpni_add_fs_entry(struct fsl_mc_io *mc_io,
-		      u32 cmd_flags,
-		      u16 token,
-		      u8 tc_id,
-		      u16 index,
-		      const struct dpni_rule_cfg *cfg,
-		      const struct dpni_fs_action_cfg *action)
-{
-	struct dpni_cmd_add_fs_entry *cmd_params;
-	struct fsl_mc_command cmd = { 0 };
-
-	/* prepare command */
-	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ADD_FS_ENT,
-					  cmd_flags,
-					  token);
-	cmd_params = (struct dpni_cmd_add_fs_entry *)cmd.params;
-	cmd_params->tc_id = tc_id;
-	cmd_params->key_size = cfg->key_size;
-	cmd_params->index = cpu_to_le16(index);
-	cmd_params->key_iova = cpu_to_le64(cfg->key_iova);
-	cmd_params->mask_iova = cpu_to_le64(cfg->mask_iova);
-	cmd_params->options = cpu_to_le16(action->options);
-	cmd_params->flow_id = cpu_to_le16(action->flow_id);
-	cmd_params->flc = cpu_to_le64(action->flc);
-
-	/* send command to mc*/
-	return mc_send_command(mc_io, &cmd);
-}
-
-/**
- * dpni_remove_fs_entry() - Remove Flow Steering entry from a specific
- *			    traffic class
- * @mc_io:	Pointer to MC portal's I/O object
- * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
- * @token:	Token of DPNI object
- * @tc_id:	Traffic class selection (0-7)
- * @cfg:	Flow steering rule to remove
- *
- * Return:	'0' on Success; Error code otherwise.
- */
-int dpni_remove_fs_entry(struct fsl_mc_io *mc_io,
-			 u32 cmd_flags,
-			 u16 token,
-			 u8 tc_id,
-			 const struct dpni_rule_cfg *cfg)
-{
-	struct dpni_cmd_remove_fs_entry *cmd_params;
-	struct fsl_mc_command cmd = { 0 };
-
-	/* prepare command */
-	cmd.header = mc_encode_cmd_header(DPNI_CMDID_REMOVE_FS_ENT,
-					  cmd_flags,
-					  token);
-	cmd_params = (struct dpni_cmd_remove_fs_entry *)cmd.params;
-	cmd_params->tc_id = tc_id;
-	cmd_params->key_size = cfg->key_size;
-	cmd_params->key_iova = cpu_to_le64(cfg->key_iova);
-	cmd_params->mask_iova = cpu_to_le64(cfg->mask_iova);
-
-	/* send command to mc*/
-	return mc_send_command(mc_io, &cmd);
-}
-
 /**
  * dpni_set_congestion_notification() - Set traffic class congestion
  *					notification configuration
@@ -2040,11 +1962,11 @@ int dpni_set_rx_fs_dist(struct fsl_mc_io *mc_io,
 					  cmd_flags,
 					  token);
 	cmd_params = (struct dpni_cmd_set_rx_fs_dist *)cmd.params;
-	cmd_params->dist_size = le16_to_cpu(cfg->dist_size);
+	cmd_params->dist_size = cpu_to_le16(cfg->dist_size);
 	dpni_set_field(cmd_params->enable, RX_FS_DIST_ENABLE, cfg->enable);
 	cmd_params->tc = cfg->tc;
-	cmd_params->miss_flow_id = le16_to_cpu(cfg->fs_miss_flow_id);
-	cmd_params->key_cfg_iova = le64_to_cpu(cfg->key_cfg_iova);
+	cmd_params->miss_flow_id = cpu_to_le16(cfg->fs_miss_flow_id);
+	cmd_params->key_cfg_iova = cpu_to_le64(cfg->key_cfg_iova);
 
 	/* send command to mc*/
 	return mc_send_command(mc_io, &cmd);
@@ -2074,10 +1996,88 @@ int dpni_set_rx_hash_dist(struct fsl_mc_io *mc_io,
 					  cmd_flags,
 					  token);
 	cmd_params = (struct dpni_cmd_set_rx_hash_dist *)cmd.params;
-	cmd_params->dist_size = le16_to_cpu(cfg->dist_size);
+	cmd_params->dist_size = cpu_to_le16(cfg->dist_size);
 	dpni_set_field(cmd_params->enable, RX_FS_DIST_ENABLE, cfg->enable);
 	cmd_params->tc = cfg->tc;
-	cmd_params->key_cfg_iova = le64_to_cpu(cfg->key_cfg_iova);
+	cmd_params->key_cfg_iova = cpu_to_le64(cfg->key_cfg_iova);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+/**
+ * dpni_add_fs_entry() - Add Flow Steering entry for a specific traffic class
+ *			(to select a flow ID)
+ * @mc_io:	Pointer to MC portal's I/O object
+ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
+ * @token:	Token of DPNI object
+ * @tc_id:	Traffic class selection (0-7)
+ * @index:	Location in the QoS table where to insert the entry.
+ *		Only relevant if MASKING is enabled for QoS
+ *		classification on this DPNI, it is ignored for exact match.
+ * @cfg:	Flow steering rule to add
+ * @action:	Action to be taken as result of a classification hit
+ *
+ * Return:	'0' on Success; Error code otherwise.
+ */
+int dpni_add_fs_entry(struct fsl_mc_io *mc_io,
+		      u32 cmd_flags,
+		      u16 token,
+		      u8 tc_id,
+		      u16 index,
+		      const struct dpni_rule_cfg *cfg,
+		      const struct dpni_fs_action_cfg *action)
+{
+	struct dpni_cmd_add_fs_entry *cmd_params;
+	struct fsl_mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ADD_FS_ENT,
+					  cmd_flags,
+					  token);
+	cmd_params = (struct dpni_cmd_add_fs_entry *)cmd.params;
+	cmd_params->tc_id = tc_id;
+	cmd_params->key_size = cfg->key_size;
+	cmd_params->index = cpu_to_le16(index);
+	cmd_params->key_iova = cpu_to_le64(cfg->key_iova);
+	cmd_params->mask_iova = cpu_to_le64(cfg->mask_iova);
+	cmd_params->options = cpu_to_le16(action->options);
+	cmd_params->flow_id = cpu_to_le16(action->flow_id);
+	cmd_params->flc = cpu_to_le64(action->flc);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+/**
+ * dpni_remove_fs_entry() - Remove Flow Steering entry from a specific
+ *			    traffic class
+ * @mc_io:	Pointer to MC portal's I/O object
+ * @cmd_flags:	Command flags; one or more of 'MC_CMD_FLAG_'
+ * @token:	Token of DPNI object
+ * @tc_id:	Traffic class selection (0-7)
+ * @cfg:	Flow steering rule to remove
+ *
+ * Return:	'0' on Success; Error code otherwise.
+ */
+int dpni_remove_fs_entry(struct fsl_mc_io *mc_io,
+			 u32 cmd_flags,
+			 u16 token,
+			 u8 tc_id,
+			 const struct dpni_rule_cfg *cfg)
+{
+	struct dpni_cmd_remove_fs_entry *cmd_params;
+	struct fsl_mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_REMOVE_FS_ENT,
+					  cmd_flags,
+					  token);
+	cmd_params = (struct dpni_cmd_remove_fs_entry *)cmd.params;
+	cmd_params->tc_id = tc_id;
+	cmd_params->key_size = cfg->key_size;
+	cmd_params->key_iova = cpu_to_le64(cfg->key_iova);
+	cmd_params->mask_iova = cpu_to_le64(cfg->mask_iova);
 
 	/* send command to mc*/
 	return mc_send_command(mc_io, &cmd);
-- 
2.17.0

