From 3191148a433df16029da77778a10482b69366fb8 Mon Sep 17 00:00:00 2001
From: Andy Duan <fugang.duan@nxp.com>
Date: Tue, 21 Mar 2017 15:01:57 +0800
Subject: [PATCH 1594/5242] MLK-14498-5 tty: serial: imx: fix the DMA issue

commit  8bce358092b00ac3b8bf565e87989eb73e45bd7b from
https://source.codeaurora.org/external/imx/linux-imx.git

The commmunity driver uart DMA don't work, it better to use
4.1.y DMA process mechanism, so there have many conflict during
code merging. Decisively, to use 4.1.y commit f00cf8855eaa in the
merge point for DMA implemention.

In DMA mode, don't involve CPU interrupt, remove .imx_dma_rxint()
function.

After the patch, DMA and CPU mode both work fine with the current
SDMA driver.

Signed-off-by: Fugang Duan <fugang.duan@nxp.com>
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 drivers/tty/serial/imx.c |  427 ++++++++++++++++++++++++++--------------------
 1 file changed, 239 insertions(+), 188 deletions(-)

diff --git a/drivers/tty/serial/imx.c b/drivers/tty/serial/imx.c
index 040aa74..24ca85e 100644
--- a/drivers/tty/serial/imx.c
+++ b/drivers/tty/serial/imx.c
@@ -176,6 +176,7 @@
 #define DRIVER_NAME "IMX-uart"
 
 #define UART_NR 8
+#define IMX_RXBD_NUM 20
 #define IMX_MODULE_MAX_CLK_RATE	80000000
 
 /* i.MX21 type uart runs on all i.mx except i.MX1 and i.MX6q */
@@ -192,6 +193,24 @@ struct imx_uart_data {
 	enum imx_uart_type devtype;
 };
 
+struct imx_dma_bufinfo {
+	bool filled;
+	unsigned int rx_bytes;
+};
+
+struct imx_dma_rxbuf {
+	unsigned int		periods;
+	unsigned int		period_len;
+	unsigned int		buf_len;
+
+	void			*buf;
+	dma_addr_t		dmaaddr;
+	unsigned int		cur_idx;
+	unsigned int		last_completed_idx;
+	dma_cookie_t		cookie;
+	struct imx_dma_bufinfo	buf_info[IMX_RXBD_NUM];
+};
+
 struct imx_port {
 	struct uart_port	port;
 	struct timer_list	timer;
@@ -217,11 +236,8 @@ struct imx_port {
 	unsigned int		dma_is_rxing:1;
 	unsigned int		dma_is_txing:1;
 	struct dma_chan		*dma_chan_rx, *dma_chan_tx;
-	struct scatterlist	rx_sgl, tx_sgl[2];
-	void			*rx_buf;
-	struct circ_buf		rx_ring;
-	unsigned int		rx_periods;
-	dma_cookie_t		rx_cookie;
+	struct scatterlist	tx_sgl[2];
+	struct imx_dma_rxbuf	rx_buf;
 	unsigned int		tx_bytes;
 	unsigned int		dma_tx_nents;
 	wait_queue_head_t	dma_wait;
@@ -619,15 +635,15 @@ static void imx_uart_dma_tx(struct imx_port *sport)
 
 	sport->tx_bytes = uart_circ_chars_pending(xmit);
 
-	if (xmit->tail < xmit->head) {
-		sport->dma_tx_nents = 1;
-		sg_init_one(sgl, xmit->buf + xmit->tail, sport->tx_bytes);
-	} else {
+	if (xmit->tail > xmit->head && xmit->head > 0) {
 		sport->dma_tx_nents = 2;
 		sg_init_table(sgl, 2);
 		sg_set_buf(sgl, xmit->buf + xmit->tail,
 				UART_XMIT_SIZE - xmit->tail);
 		sg_set_buf(sgl + 1, xmit->buf, xmit->head);
+	} else {
+		sport->dma_tx_nents = 1;
+		sg_init_one(sgl, xmit->buf + xmit->tail, sport->tx_bytes);
 	}
 
 	ret = dma_map_sg(dev, sgl, sport->dma_tx_nents, DMA_TO_DEVICE);
@@ -823,13 +839,17 @@ static irqreturn_t imx_uart_rxint(int irq, void *dev_id)
  */
 static unsigned int imx_uart_get_hwmctrl(struct imx_port *sport)
 {
-	unsigned int tmp = TIOCM_DSR;
-	unsigned usr1 = imx_uart_readl(sport, USR1);
-	unsigned usr2 = imx_uart_readl(sport, USR2);
+	unsigned int tmp = TIOCM_DSR | TIOCM_CAR;
+	unsigned int usr1 = imx_uart_readl(sport, USR1);
+	unsigned int usr2 = imx_uart_readl(sport, USR2);
+	unsigned int ucr2 = imx_uart_readl(sport, UCR2);
 
 	if (usr1 & USR1_RTSS)
 		tmp |= TIOCM_CTS;
 
+	if (ucr2 & UCR2_CTS)
+		tmp |= TIOCM_RTS;
+
 	/* in DCE mode DCDIN is always 0 */
 	if (!(usr2 & USR2_DCDIN))
 		tmp |= TIOCM_CAR;
@@ -838,6 +858,9 @@ static unsigned int imx_uart_get_hwmctrl(struct imx_port *sport)
 		if (!(imx_uart_readl(sport, USR2) & USR2_RIIN))
 			tmp |= TIOCM_RI;
 
+	if (imx_uart_readl(sport, uts_reg(sport)) & UTS_LOOP)
+		tmp |= TIOCM_LOOP;
+
 	return tmp;
 }
 
@@ -1025,146 +1048,70 @@ static void imx_uart_break_ctl(struct uart_port *port, int break_state)
 	spin_unlock_irqrestore(&sport->port.lock, flags);
 }
 
-/*
- * This is our per-port timeout handler, for checking the
- * modem status signals.
- */
-static void imx_uart_timeout(struct timer_list *t)
+#define TXTL 2 /* reset default */
+#define RXTL 1 /* For console port */
+#define RXTL_UART 16 /* For uart */
+
+static int imx_uart_setup_ufcr(struct imx_port *sport, unsigned int mode)
 {
-	struct imx_port *sport = from_timer(sport, t, timer);
-	unsigned long flags;
+	unsigned int val;
+	unsigned int rx_fifo_trig;
 
-	if (sport->port.state) {
-		spin_lock_irqsave(&sport->port.lock, flags);
-		imx_uart_mctrl_check(sport);
-		spin_unlock_irqrestore(&sport->port.lock, flags);
+	if (uart_console(&sport->port))
+		rx_fifo_trig = RXTL;
+	else
+		rx_fifo_trig = RXTL_UART;
 
-		mod_timer(&sport->timer, jiffies + MCTRL_TIMEOUT);
-	}
+	/* set receiver / transmitter trigger level */
+	val = readl(sport->port.membase + UFCR) & (UFCR_RFDIV | UFCR_DCEDTE);
+	val |= TXTL << UFCR_TXTL_SHF | rx_fifo_trig;
+	writel(val, sport->port.membase + UFCR);
+	return 0;
 }
 
 #define RX_BUF_SIZE	(PAGE_SIZE)
-
-/*
- * There are two kinds of RX DMA interrupts(such as in the MX6Q):
- *   [1] the RX DMA buffer is full.
- *   [2] the aging timer expires
- *
- * Condition [2] is triggered when a character has been sitting in the FIFO
- * for at least 8 byte durations.
- */
-static void imx_uart_dma_rx_callback(void *data)
+static void dma_rx_push_data(struct imx_port *sport, struct tty_struct *tty,
+				unsigned int start, unsigned int end)
 {
-	struct imx_port *sport = data;
-	struct dma_chan	*chan = sport->dma_chan_rx;
-	struct scatterlist *sgl = &sport->rx_sgl;
+	unsigned int i;
 	struct tty_port *port = &sport->port.state->port;
-	struct dma_tx_state state;
-	struct circ_buf *rx_ring = &sport->rx_ring;
-	enum dma_status status;
-	unsigned int w_bytes = 0;
-	unsigned int r_bytes;
-	unsigned int bd_size;
-
-	status = dmaengine_tx_status(chan, (dma_cookie_t)0, &state);
-
-	if (status == DMA_ERROR) {
-		imx_uart_clear_rx_errors(sport);
-		return;
-	}
-
-	if (!(sport->port.ignore_status_mask & URXD_DUMMY_READ)) {
-
-		/*
-		 * The state-residue variable represents the empty space
-		 * relative to the entire buffer. Taking this in consideration
-		 * the head is always calculated base on the buffer total
-		 * length - DMA transaction residue. The UART script from the
-		 * SDMA firmware will jump to the next buffer descriptor,
-		 * once a DMA transaction if finalized (IMX53 RM - A.4.1.2.4).
-		 * Taking this in consideration the tail is always at the
-		 * beginning of the buffer descriptor that contains the head.
-		 */
-
-		/* Calculate the head */
-		rx_ring->head = sg_dma_len(sgl) - state.residue;
-
-		/* Calculate the tail. */
-		bd_size = sg_dma_len(sgl) / sport->rx_periods;
-		rx_ring->tail = ((rx_ring->head-1) / bd_size) * bd_size;
-
-		if (rx_ring->head <= sg_dma_len(sgl) &&
-		    rx_ring->head > rx_ring->tail) {
-
-			/* Move data from tail to head */
-			r_bytes = rx_ring->head - rx_ring->tail;
-
-			/* CPU claims ownership of RX DMA buffer */
-			dma_sync_sg_for_cpu(sport->port.dev, sgl, 1,
-				DMA_FROM_DEVICE);
-
-			w_bytes = tty_insert_flip_string(port,
-				sport->rx_buf + rx_ring->tail, r_bytes);
-
-			/* UART retrieves ownership of RX DMA buffer */
-			dma_sync_sg_for_device(sport->port.dev, sgl, 1,
-				DMA_FROM_DEVICE);
 
-			if (w_bytes != r_bytes)
-				sport->port.icount.buf_overrun++;
-
-			sport->port.icount.rx += w_bytes;
-		} else	{
-			WARN_ON(rx_ring->head > sg_dma_len(sgl));
-			WARN_ON(rx_ring->head <= rx_ring->tail);
+	for (i = start; i < end; i++) {
+		if (sport->rx_buf.buf_info[i].filled) {
+			tty_insert_flip_string(port, sport->rx_buf.buf + (i
+					* RX_BUF_SIZE), sport->rx_buf.buf_info[i].rx_bytes);
+			tty_flip_buffer_push(port);
+			sport->rx_buf.buf_info[i].filled = false;
+			sport->rx_buf.last_completed_idx++;
+			sport->rx_buf.last_completed_idx %= IMX_RXBD_NUM;
+			sport->port.icount.rx += sport->rx_buf.buf_info[i].rx_bytes;
 		}
 	}
-
-	if (w_bytes) {
-		tty_flip_buffer_push(port);
-		dev_dbg(sport->port.dev, "We get %d bytes.\n", w_bytes);
-	}
 }
 
-/* RX DMA buffer periods */
-#define RX_DMA_PERIODS 4
-
-static int imx_uart_start_rx_dma(struct imx_port *sport)
+static void dma_rx_work(struct imx_port *sport)
 {
-	struct scatterlist *sgl = &sport->rx_sgl;
-	struct dma_chan	*chan = sport->dma_chan_rx;
-	struct device *dev = sport->port.dev;
-	struct dma_async_tx_descriptor *desc;
-	int ret;
-
-	sport->rx_ring.head = 0;
-	sport->rx_ring.tail = 0;
-	sport->rx_periods = RX_DMA_PERIODS;
+	struct tty_struct *tty = sport->port.state->port.tty;
+	unsigned int cur_idx = sport->rx_buf.cur_idx;
 
-	sg_init_one(sgl, sport->rx_buf, RX_BUF_SIZE);
-	ret = dma_map_sg(dev, sgl, 1, DMA_FROM_DEVICE);
-	if (ret == 0) {
-		dev_err(dev, "DMA mapping error for RX.\n");
-		return -EINVAL;
+	if (sport->rx_buf.last_completed_idx < cur_idx) {
+		dma_rx_push_data(sport, tty, sport->rx_buf.last_completed_idx + 1, cur_idx);
+	} else if (sport->rx_buf.last_completed_idx == (IMX_RXBD_NUM - 1)) {
+		dma_rx_push_data(sport, tty, 0, cur_idx);
+	} else {
+		dma_rx_push_data(sport, tty, sport->rx_buf.last_completed_idx + 1,
+					IMX_RXBD_NUM);
+		dma_rx_push_data(sport, tty, 0, cur_idx);
 	}
+}
 
-	desc = dmaengine_prep_dma_cyclic(chan, sg_dma_address(sgl),
-		sg_dma_len(sgl), sg_dma_len(sgl) / sport->rx_periods,
-		DMA_DEV_TO_MEM, DMA_PREP_INTERRUPT);
-
-	if (!desc) {
-		dma_unmap_sg(dev, sgl, 1, DMA_FROM_DEVICE);
-		dev_err(dev, "We cannot prepare for the RX slave dma!\n");
-		return -EINVAL;
-	}
-	desc->callback = imx_uart_dma_rx_callback;
-	desc->callback_param = sport;
+static void imx_rx_dma_done(struct imx_port *sport)
+{
+	sport->dma_is_rxing = 0;
 
-	dev_dbg(dev, "RX: prepare for the DMA.\n");
-	sport->dma_is_rxing = 1;
-	sport->rx_cookie = dmaengine_submit(desc);
-	dma_async_issue_pending(chan);
-	return 0;
+	/* Is the shutdown waiting for us? */
+	if (waitqueue_active(&sport->dma_wait))
+		wake_up(&sport->dma_wait);
 }
 
 static void imx_uart_clear_rx_errors(struct imx_port *sport)
@@ -1197,38 +1144,112 @@ static void imx_uart_clear_rx_errors(struct imx_port *sport)
 		sport->port.icount.overrun++;
 		imx_uart_writel(sport, USR2_ORE, USR2);
 	}
+}
 
+/*
+ * This is our per-port timeout handler, for checking the
+ * modem status signals.
+ */
+static void imx_uart_timeout(struct timer_list *t)
+{
+	struct imx_port *sport = from_timer(sport, t, timer);
+	unsigned long flags;
+
+	if (sport->port.state) {
+		spin_lock_irqsave(&sport->port.lock, flags);
+		imx_uart_mctrl_check(sport);
+		spin_unlock_irqrestore(&sport->port.lock, flags);
+
+		mod_timer(&sport->timer, jiffies + MCTRL_TIMEOUT);
+	}
 }
 
-#define TXTL_DEFAULT 2 /* reset default */
-#define RXTL_DEFAULT 1 /* reset default */
-#define TXTL_DMA 8 /* DMA burst setting */
-#define RXTL_DMA 9 /* DMA burst setting */
+/*
+ * There are three kinds of RX DMA interrupts(such as in the MX6Q):
+ *   [1] the RX DMA buffer is full.
+ *   [2] the Aging timer expires(wait for 8 bytes long)
+ *   [3] the Idle Condition Detect(enabled the UCR4_IDDMAEN).
+ *
+ * The [2] is trigger when a character was been sitting in the FIFO
+ * meanwhile [3] can wait for 32 bytes long when the RX line is
+ * on IDLE state and RxFIFO is empty.
+ */
+static void imx_uart_dma_rx_callback(void *data)
+{
+	struct imx_port *sport = data;
+	struct dma_chan	*chan = sport->dma_chan_rx;
+	struct tty_struct *tty = sport->port.state->port.tty;
+	struct dma_tx_state state;
+	enum dma_status status;
+	unsigned int count;
 
-static void imx_uart_setup_ufcr(struct imx_port *sport,
-				unsigned char txwl, unsigned char rxwl)
+	/* If we have finish the reading. we will not accept any more data. */
+	if (tty->closing) {
+		imx_rx_dma_done(sport);
+		return;
+	}
+
+	status = dmaengine_tx_status(chan, sport->rx_buf.cookie, &state);
+	if (status == DMA_ERROR) {
+		imx_uart_clear_rx_errors(sport);
+		return;
+	}
+
+	count = RX_BUF_SIZE - state.residue;
+	sport->rx_buf.buf_info[sport->rx_buf.cur_idx].filled = true;
+	sport->rx_buf.buf_info[sport->rx_buf.cur_idx].rx_bytes = count;
+	sport->rx_buf.cur_idx++;
+	sport->rx_buf.cur_idx %= IMX_RXBD_NUM;
+	dev_dbg(sport->port.dev, "We get %d bytes.\n", count);
+
+	if (sport->rx_buf.cur_idx == sport->rx_buf.last_completed_idx)
+		dev_err(sport->port.dev, "overwrite!\n");
+
+	if (count)
+		dma_rx_work(sport);
+}
+
+static int imx_uart_start_rx_dma(struct imx_port *sport)
 {
-	unsigned int val;
+	struct dma_chan	*chan = sport->dma_chan_rx;
+	struct dma_async_tx_descriptor *desc;
 
-	/* set receiver / transmitter trigger level */
-	val = imx_uart_readl(sport, UFCR) & (UFCR_RFDIV | UFCR_DCEDTE);
-	val |= txwl << UFCR_TXTL_SHF | rxwl;
-	imx_uart_writel(sport, val, UFCR);
+	sport->rx_buf.periods = IMX_RXBD_NUM;
+	sport->rx_buf.period_len = RX_BUF_SIZE;
+	sport->rx_buf.buf_len = IMX_RXBD_NUM * RX_BUF_SIZE;
+	sport->rx_buf.cur_idx = 0;
+	sport->rx_buf.last_completed_idx = -1;
+	desc = dmaengine_prep_dma_cyclic(chan, sport->rx_buf.dmaaddr,
+		sport->rx_buf.buf_len, sport->rx_buf.period_len,
+		DMA_DEV_TO_MEM, DMA_PREP_INTERRUPT);
+
+	if (!desc) {
+		dev_err(sport->port.dev, "Prepare for the RX slave dma failed!\n");
+		return -EINVAL;
+	}
+	desc->callback = imx_uart_dma_rx_callback;
+	desc->callback_param = sport;
+
+	dev_dbg(sport->port.dev, "RX: prepare for the DMA.\n");
+	sport->dma_is_rxing = 1;
+	sport->rx_buf.cookie = dmaengine_submit(desc);
+	dma_async_issue_pending(chan);
+	return 0;
 }
 
 static void imx_uart_dma_exit(struct imx_port *sport)
 {
 	if (sport->dma_chan_rx) {
-		dmaengine_terminate_sync(sport->dma_chan_rx);
 		dma_release_channel(sport->dma_chan_rx);
 		sport->dma_chan_rx = NULL;
-		sport->rx_cookie = -EINVAL;
-		kfree(sport->rx_buf);
-		sport->rx_buf = NULL;
+
+		dma_free_coherent(NULL, IMX_RXBD_NUM * RX_BUF_SIZE,
+					(void *)sport->rx_buf.buf,
+					sport->rx_buf.dmaaddr);
+		sport->rx_buf.buf = NULL;
 	}
 
 	if (sport->dma_chan_tx) {
-		dmaengine_terminate_sync(sport->dma_chan_tx);
 		dma_release_channel(sport->dma_chan_tx);
 		sport->dma_chan_tx = NULL;
 	}
@@ -1238,7 +1259,7 @@ static int imx_uart_dma_init(struct imx_port *sport)
 {
 	struct dma_slave_config slave_config = {};
 	struct device *dev = sport->port.dev;
-	int ret;
+	int ret, i;
 
 	/* Prepare for RX : */
 	sport->dma_chan_rx = dma_request_slave_channel(dev, "rx");
@@ -1251,20 +1272,25 @@ static int imx_uart_dma_init(struct imx_port *sport)
 	slave_config.direction = DMA_DEV_TO_MEM;
 	slave_config.src_addr = sport->port.mapbase + URXD0;
 	slave_config.src_addr_width = DMA_SLAVE_BUSWIDTH_1_BYTE;
-	/* one byte less than the watermark level to enable the aging timer */
-	slave_config.src_maxburst = RXTL_DMA - 1;
+	slave_config.src_maxburst = RXTL_UART;
 	ret = dmaengine_slave_config(sport->dma_chan_rx, &slave_config);
 	if (ret) {
 		dev_err(dev, "error in RX dma configuration.\n");
 		goto err;
 	}
 
-	sport->rx_buf = kzalloc(RX_BUF_SIZE, GFP_KERNEL);
-	if (!sport->rx_buf) {
+	sport->rx_buf.buf = dma_alloc_coherent(NULL, IMX_RXBD_NUM * RX_BUF_SIZE,
+					&sport->rx_buf.dmaaddr, GFP_KERNEL);
+	if (!sport->rx_buf.buf) {
+		dev_err(dev, "cannot alloc DMA buffer.\n");
 		ret = -ENOMEM;
 		goto err;
 	}
-	sport->rx_ring.buf = sport->rx_buf;
+
+	for (i = 0; i < IMX_RXBD_NUM; i++) {
+		sport->rx_buf.buf_info[i].rx_bytes = 0;
+		sport->rx_buf.buf_info[i].filled = false;
+	}
 
 	/* Prepare for TX : */
 	sport->dma_chan_tx = dma_request_slave_channel(dev, "tx");
@@ -1277,7 +1303,7 @@ static int imx_uart_dma_init(struct imx_port *sport)
 	slave_config.direction = DMA_MEM_TO_DEV;
 	slave_config.dst_addr = sport->port.mapbase + URTX0;
 	slave_config.dst_addr_width = DMA_SLAVE_BUSWIDTH_1_BYTE;
-	slave_config.dst_maxburst = TXTL_DMA;
+	slave_config.dst_maxburst = TXTL;
 	ret = dmaengine_slave_config(sport->dma_chan_tx, &slave_config);
 	if (ret) {
 		dev_err(dev, "error in TX dma configuration.");
@@ -1292,7 +1318,7 @@ static int imx_uart_dma_init(struct imx_port *sport)
 
 static void imx_uart_enable_dma(struct imx_port *sport)
 {
-	u32 ucr1;
+	u32 ucr1, ucr4;
 
 	imx_uart_setup_ufcr(sport, TXTL_DMA, RXTL_DMA);
 
@@ -1300,22 +1326,37 @@ static void imx_uart_enable_dma(struct imx_port *sport)
 
 	/* set UCR1 */
 	ucr1 = imx_uart_readl(sport, UCR1);
-	ucr1 |= UCR1_RXDMAEN | UCR1_TXDMAEN | UCR1_ATDMAEN;
+	ucr1 |= UCR1_RXDMAEN | UCR1_TXDMAEN | UCR1_ATDMAEN |
+		/* wait for 32 idle frames for IDDMA interrupt */
+		UCR1_ICD_REG(3);
 	imx_uart_writel(sport, ucr1, UCR1);
 
+	/* set UCR4 */
+	ucr4 = imx_uart_readl(sport, UCR4);
+	ucr4 |= UCR4_IDDMAEN;
+	imx_uart_writel(sport, ucr4, UCR4);
+
 	sport->dma_is_enabled = 1;
 }
 
 static void imx_uart_disable_dma(struct imx_port *sport)
 {
-	u32 ucr1;
+	u32 ucr1, ucr2, ucr4;
 
 	/* clear UCR1 */
 	ucr1 = imx_uart_readl(sport, UCR1);
 	ucr1 &= ~(UCR1_RXDMAEN | UCR1_TXDMAEN | UCR1_ATDMAEN);
 	imx_uart_writel(sport, ucr1, UCR1);
 
-	imx_uart_setup_ufcr(sport, TXTL_DEFAULT, RXTL_DEFAULT);
+	/* clear UCR2 */
+	ucr2 = imx_uart_readl(sport, UCR2);
+	ucr2 &= ~(UCR2_CTSC | UCR2_CTS);
+	imx_uart_writel(sport, ucr2, UCR2);
+
+	/* clear UCR4 */
+	ucr4 = imx_uart_readl(sport, UCR4);
+	ucr4 &= ~UCR4_IDDMAEN;
+	imx_uart_writel(sport, ucr4, UCR4);
 
 	sport->dma_is_enabled = 0;
 }
@@ -1340,7 +1381,7 @@ static int imx_uart_startup(struct uart_port *port)
 		return retval;
 	}
 
-	imx_uart_setup_ufcr(sport, TXTL_DEFAULT, RXTL_DEFAULT);
+	imx_uart_setup_ufcr(sport, 0);
 
 	/* disable the DREN bit (Data Ready interrupt enable) before
 	 * requesting IRQs
@@ -1353,11 +1394,6 @@ static int imx_uart_startup(struct uart_port *port)
 
 	imx_uart_writel(sport, ucr4 & ~UCR4_DREN, UCR4);
 
-	/* Can we enable the DMA support? */
-	if (!uart_console(port) && imx_uart_dma_init(sport) == 0)
-		dma_is_inited = 1;
-
-	spin_lock_irqsave(&sport->port.lock, flags);
 	/* Reset fifo's and state machines */
 	i = 100;
 
@@ -1368,17 +1404,18 @@ static int imx_uart_startup(struct uart_port *port)
 	while (!(imx_uart_readl(sport, UCR2) & UCR2_SRST) && (--i > 0))
 		udelay(1);
 
+	spin_lock_irqsave(&sport->port.lock, flags);
+
 	/*
 	 * Finally, clear and enable interrupts
 	 */
-	imx_uart_writel(sport, USR1_RTSD | USR1_DTRD, USR1);
+	imx_uart_writel(sport, USR1_RTSD, USR1);
 	imx_uart_writel(sport, USR2_ORE, USR2);
 
-	ucr1 = imx_uart_readl(sport, UCR1) & ~UCR1_RRDYEN;
-	ucr1 |= UCR1_UARTEN;
-	if (sport->have_rtscts)
-		ucr1 |= UCR1_RTSDEN;
-
+	ucr1 = imx_uart_readl(sport, UCR1);
+	if (!sport->dma_is_inited)
+		ucr1 |= UCR1_RRDYEN;
+	ucr1 |= UCR1_RTSDEN | UCR1_UARTEN;
 	imx_uart_writel(sport, ucr1, UCR1);
 
 	ucr4 = imx_uart_readl(sport, UCR4) & ~UCR4_OREN;
@@ -1442,17 +1479,17 @@ static void imx_uart_shutdown(struct uart_port *port)
 	u32 ucr1, ucr2, ucr4;
 
 	if (sport->dma_is_enabled) {
-		dmaengine_terminate_sync(sport->dma_chan_tx);
-		if (sport->dma_is_txing) {
-			dma_unmap_sg(sport->port.dev, &sport->tx_sgl[0],
-				     sport->dma_tx_nents, DMA_TO_DEVICE);
-			sport->dma_is_txing = 0;
-		}
-		dmaengine_terminate_sync(sport->dma_chan_rx);
-		if (sport->dma_is_rxing) {
-			dma_unmap_sg(sport->port.dev, &sport->rx_sgl,
-				     1, DMA_FROM_DEVICE);
+		int ret;
+
+		/* We have to wait for the DMA to finish. */
+		ret = wait_event_interruptible_timeout(sport->dma_wait,
+			!sport->dma_is_rxing && !sport->dma_is_txing,
+			msecs_to_jiffies(1));
+		if (ret <= 0) {
 			sport->dma_is_rxing = 0;
+			sport->dma_is_txing = 0;
+			dmaengine_terminate_all(sport->dma_chan_tx);
+			dmaengine_terminate_all(sport->dma_chan_rx);
 		}
 
 		spin_lock_irqsave(&sport->port.lock, flags);
@@ -1553,8 +1590,8 @@ static void imx_uart_flush_buffer(struct uart_port *port)
 {
 	struct imx_port *sport = (struct imx_port *)port;
 	unsigned long flags;
-	u32 ucr2, old_ucr1, old_ucr2, ufcr;
-	unsigned int baud, quot;
+	u32 ufcr;
+	unsigned long ucr2, old_ucr1, old_ucr2, baud, quot;
 	unsigned int old_csize = old ? old->c_cflag & CSIZE : CS8;
 	unsigned long div;
 	unsigned long num, denom;
@@ -1593,6 +1630,11 @@ static void imx_uart_flush_buffer(struct uart_port *port)
 			} else {
 				imx_uart_rts_auto(sport, &ucr2);
 			}
+
+			/* Can we enable the DMA support? */
+			if (imx_uart_is_imx6q(sport) && !uart_console(port)
+				&& !sport->dma_is_inited)
+				imx_uart_dma_init(sport);
 		} else {
 			termios->c_cflag &= ~CRTSCTS;
 		}
@@ -1604,7 +1646,6 @@ static void imx_uart_flush_buffer(struct uart_port *port)
 			imx_uart_rts_inactive(sport, &ucr2);
 	}
 
-
 	if (termios->c_cflag & CSTOPB)
 		ucr2 |= UCR2_STPB;
 	if (termios->c_cflag & PARENB) {
@@ -1712,6 +1753,16 @@ static void imx_uart_flush_buffer(struct uart_port *port)
 	if (UART_ENABLE_MS(&sport->port, termios->c_cflag))
 		imx_uart_enable_ms(&sport->port);
 
+	if (sport->dma_is_inited && !sport->dma_is_enabled) {
+		imx_uart_enable_dma(sport);
+		imx_uart_start_rx_dma(sport);
+	}
+
+	if (!sport->dma_is_enabled) {
+		ucr2 = imx_uart_readl(sport, UCR2);
+		imx_uart_writel(sport, ucr2 | UCR2_ATEN, UCR2);
+	}
+
 	spin_unlock_irqrestore(&sport->port.lock, flags);
 }
 
@@ -1777,7 +1828,7 @@ static int imx_uart_poll_init(struct uart_port *port)
 	if (retval)
 		clk_disable_unprepare(sport->clk_ipg);
 
-	imx_uart_setup_ufcr(sport, TXTL_DEFAULT, RXTL_DEFAULT);
+	imx_uart_setup_ufcr(sport, 0);
 
 	spin_lock_irqsave(&sport->port.lock, flags);
 
@@ -2072,7 +2123,7 @@ static void imx_uart_console_putchar(struct uart_port *port, int ch)
 	else
 		imx_uart_console_get_options(sport, &baud, &parity, &bits);
 
-	imx_uart_setup_ufcr(sport, TXTL_DEFAULT, RXTL_DEFAULT);
+	imx_uart_setup_ufcr(sport, 0);
 
 	retval = uart_set_options(&sport->port, co, baud, parity, bits, flow);
 
-- 
1.7.9.5

