From ad30a53839716ebece91924ffb5d9b7a1e971cfd Mon Sep 17 00:00:00 2001
From: Stefan Chulski <stefanc@marvell.com>
Date: Sun, 21 May 2017 13:25:14 +0300
Subject: [PATCH 1031/1345] net: mvpp2x: Add skb recycling feature

commit  e303f678084b825ef583bb73504997bb2f1d87da from
https://github.com/MarvellEmbeddedProcessors/linux-marvell.git

This patch implement skb recycling feature for forwarded on same
CP packets.

Feature brief description:
- Feature use buffer recycling infrastructure to recycle skb.
- In RX routine skb pulled from per_cpu per CP skb pool.
  If skb_pool_get callback return skb -> local skb build callback
  without memory allocation used.
  Else RX routine will use regular skb build callback.
- In TX done routine skb is pushed into pool only if skb shadow
  indicated.

Implementation of skb recycling feature provide ~10% performance
improvement in NW FWR IXIA benchmarks on same CP.

Change-Id: Ib6d649e28df71a9207ce331d1570490fa1ca1b2e
Signed-off-by: Stefan Chulski <stefanc@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/39718
Tested-by: iSoC Platform CI <ykjenk@marvell.com>
Reviewed-by: Omri Itach <omrii@marvell.com>
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h      |   22 ++++
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c |  120 +++++++++++++++++++-
 2 files changed, 140 insertions(+), 2 deletions(-)

diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h
index 2a495a2..f10b75b 100644
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x.h
@@ -195,6 +195,7 @@
 
 #define MVPP2_EXTRA_BUF_SIZE	120
 #define MVPP2_EXTRA_BUF_NUM	(MVPP2_MAX_TXD * MVPP2_MAX_TXQ)
+#define MVPP2_SKB_NUM		(MVPP2_MAX_RXD * MVPP2_MAX_RXQ * MVPP2_MAX_PORTS)
 
 enum mvppv2_version {
 	PPV21 = 21,
@@ -553,6 +554,9 @@ struct mv_pp2x {
 	u16 num_pools;
 	struct mv_pp2x_bm_pool *bm_pools;
 
+	/* Per-CPU CP control */
+	struct mv_pp2x_cp_pcpu __percpu *pcpu;
+
 	/* RX flow hash indir'n table, in pp22, the table contains the
 	* CPU idx according to weight
 	*/
@@ -588,6 +592,12 @@ struct mv_pp2x_port_pcpu {
 	bool tx_timer_scheduled;
 };
 
+/* Per-CPU CP control */
+struct mv_pp2x_cp_pcpu {
+	struct list_head skb_port_list;
+	struct mv_pp2x_skb_pool *skb_pool;
+};
+
 struct queue_vector {
 	u32 irq;
 	char irq_name[IRQ_NAME_SIZE];
@@ -691,6 +701,11 @@ struct mv_pp2x_ext_buf_struct {
 	u8 *ext_buf_data;
 };
 
+struct mv_pp2x_skb_struct {
+	struct list_head skb_list;
+	struct sk_buff *skb;
+};
+
 struct mv_pp2x_ext_buf_pool {
 	int buf_pool_size;
 	int buf_pool_next_free;
@@ -698,6 +713,13 @@ struct mv_pp2x_ext_buf_pool {
 	struct mv_pp2x_ext_buf_struct *ext_buf_struct;
 };
 
+struct mv_pp2x_skb_pool {
+	int skb_pool_size;
+	int skb_pool_in_use;
+	int skb_pool_next_free;
+	struct mv_pp2x_skb_struct *skb_struct;
+};
+
 static inline struct mv_pp2x_port *mv_pp2x_port_struct_get(struct mv_pp2x *priv,
 							   int port)
 {
diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
index 2e26882..35e6b12 100644
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
@@ -271,6 +271,14 @@ static void mv_pp2x_extra_pool_inc(struct mv_pp2x_ext_buf_pool *ext_buf_pool)
 		ext_buf_pool->buf_pool_next_free++;
 }
 
+static void mv_pp2x_skb_pool_inc(struct mv_pp2x_skb_pool *skb_pool)
+{
+	if (unlikely(skb_pool->skb_pool_next_free == skb_pool->skb_pool_size - 1))
+		skb_pool->skb_pool_next_free = 0;
+	else
+		skb_pool->skb_pool_next_free++;
+}
+
 static u8 mv_pp2x_first_pool_get(struct mv_pp2x *priv)
 {
 	return priv->pp2_cfg.first_bm_pool;
@@ -912,6 +920,51 @@ static inline int mv_pp2_extra_pool_put(struct mv_pp2x_port *port, void *ext_buf
 	return 0;
 }
 
+static inline struct sk_buff *mv_pp2_skb_pool_get(struct mv_pp2x_port *port)
+{
+	struct sk_buff *skb;
+	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
+	struct mv_pp2x_skb_struct *skb_struct;
+
+	if (!list_empty(&cp_pcpu->skb_port_list)) {
+		skb_struct = list_last_entry(&cp_pcpu->skb_port_list,
+					     struct mv_pp2x_skb_struct, skb_list);
+		list_del(&skb_struct->skb_list);
+		cp_pcpu->skb_pool->skb_pool_in_use--;
+
+		skb = skb_struct->skb;
+
+	} else {
+		skb = NULL;
+	}
+
+	return skb;
+}
+
+static inline int mv_pp2_skb_pool_put(struct mv_pp2x_port *port, struct sk_buff *skb,
+				      int cpu)
+{
+	struct mv_pp2x_cp_pcpu *cp_pcpu = per_cpu_ptr(port->priv->pcpu, cpu);
+	struct mv_pp2x_skb_struct *skb_struct;
+
+	if (cp_pcpu->skb_pool->skb_pool_in_use >= cp_pcpu->skb_pool->skb_pool_size) {
+		dev_kfree_skb_any(skb);
+		return 1;
+	}
+	cp_pcpu->skb_pool->skb_pool_in_use++;
+
+	skb_struct =
+		&cp_pcpu->skb_pool->skb_struct[cp_pcpu->skb_pool->skb_pool_next_free];
+	mv_pp2x_skb_pool_inc(cp_pcpu->skb_pool);
+
+	skb_struct->skb = skb;
+
+	list_add(&skb_struct->skb_list,
+		 &cp_pcpu->skb_port_list);
+
+	return 0;
+}
+
 /* Check if there are enough reserved descriptors for transmission.
  * If not, request chunk of reserved descriptors and check again.
  */
@@ -1036,7 +1089,8 @@ static void mv_pp2x_txq_bufs_free(struct mv_pp2x_port *port,
 			skb_rec->head = NULL;
 			atomic_dec(&bm_pool->in_use);
 
-			dev_kfree_skb_any((struct sk_buff *)skb);
+			mv_pp2_skb_pool_put(port, (struct sk_buff *)skb, txq_pcpu->cpu);
+
 			mv_pp2x_txq_inc_get(txq_pcpu);
 			continue;
 		}
@@ -2441,6 +2495,38 @@ static void mv_pp2x_set_skb_hash(struct mv_pp2x_rx_desc *rx_desc, u32 rx_status,
 		skb_set_hash(skb, MVPP2_UNIQUE_HASH, PKT_HASH_TYPE_L3);
 }
 
+/* Function is similar to build_skb routine without sbk kmem_cache_alloc */
+static void mv_pp2x_build_skb(struct sk_buff *skb, unsigned char *data,
+			      unsigned int frag_size)
+{
+	struct skb_shared_info *shinfo;
+	unsigned int size = frag_size ? : ksize(data);
+
+	size -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+
+	memset(skb, 0, offsetof(struct sk_buff, tail));
+	skb->truesize = SKB_TRUESIZE(size);
+	atomic_set(&skb->users, 1);
+	skb->head = data;
+	skb->data = data;
+	skb_reset_tail_pointer(skb);
+	skb->end = skb->tail + size;
+	skb->mac_header = (typeof(skb->mac_header))~0U;
+	skb->transport_header = (typeof(skb->transport_header))~0U;
+
+	/* make sure we initialize shinfo sequentially */
+	shinfo = skb_shinfo(skb);
+	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+	atomic_set(&shinfo->dataref, 1);
+	kmemcheck_annotate_variable(shinfo->destructor_arg);
+
+	if (skb && frag_size) {
+		skb->head_frag = 1;
+		if (page_is_pfmemalloc(virt_to_head_page(data)))
+			skb->pfmemalloc = 1;
+	}
+}
+
 /* Main rx processing */
 static int mv_pp2x_rx(struct mv_pp2x_port *port, struct napi_struct *napi,
 		      int rx_todo, struct mv_pp2x_rx_queue *rxq)
@@ -2528,9 +2614,19 @@ static int mv_pp2x_rx(struct mv_pp2x_port *port, struct napi_struct *napi,
 			atomic_dec(&bm_pool->in_use);
 			continue;
 		}
+		/* Try to get skb from CP skb pool
+		*  If get func return skb -> use mv_pp2x_build_skb to reset skb
+		*  else -> use regular build_skb callback
+		*/
+		skb = mv_pp2_skb_pool_get(port);
 
-		skb = build_skb(data, bm_pool->frag_size > PAGE_SIZE ? 0 :
+		if (skb)
+			mv_pp2x_build_skb(skb, data, bm_pool->frag_size > PAGE_SIZE ? 0 :
+				bm_pool->frag_size);
+		else
+			skb = build_skb(data, bm_pool->frag_size > PAGE_SIZE ? 0 :
 				bm_pool->frag_size);
+
 		if (unlikely(!skb)) {
 			netdev_warn(port->dev, "skb build failed\n");
 			goto err_drop_frame;
@@ -5710,6 +5806,7 @@ static int mv_pp2x_probe(struct platform_device *pdev)
 	u32 cell_index = 0;
 	struct device_node *dn = pdev->dev.of_node;
 	struct device_node *port_node;
+	struct mv_pp2x_cp_pcpu *cp_pcpu;
 
 	priv = devm_kzalloc(&pdev->dev, sizeof(struct mv_pp2x), GFP_KERNEL);
 	if (!priv)
@@ -5769,6 +5866,25 @@ static int mv_pp2x_probe(struct platform_device *pdev)
 		goto err_clk;
 	}
 
+	priv->pcpu = alloc_percpu(struct mv_pp2x_cp_pcpu);
+	if (!priv->pcpu) {
+		err = -ENOMEM;
+		goto  err_clk;
+	}
+
+	/* Init per CPU CP skb list for skb recycling */
+	for_each_present_cpu(cpu) {
+		cp_pcpu = per_cpu_ptr(priv->pcpu, cpu);
+
+		INIT_LIST_HEAD(&cp_pcpu->skb_port_list);
+		cp_pcpu->skb_pool = devm_kzalloc(&pdev->dev,
+			sizeof(struct mv_pp2x_skb_pool), GFP_ATOMIC);
+		cp_pcpu->skb_pool->skb_pool_size = MVPP2_SKB_NUM;
+		cp_pcpu->skb_pool->skb_struct =
+			devm_kzalloc(&pdev->dev,
+				     sizeof(struct mv_pp2x_skb_struct) * MVPP2_SKB_NUM, GFP_ATOMIC);
+	}
+
 	/* Init PP22 rxfhindir table evenly in probe */
 	if (priv->pp2_version == PPV22) {
 		mv_pp22_init_rxfhindir(priv);
-- 
1.7.9.5

