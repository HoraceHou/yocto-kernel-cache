From 90c26e7dd1bf29407a381bf242a34917977fb493 Mon Sep 17 00:00:00 2001
From: Yan Markman <ymarkman@marvell.com>
Date: Thu, 28 Dec 2017 20:19:24 +0200
Subject: [PATCH 1253/1345] fix: net: mvpp2: drain aggr txq over tx-tasklet

commit  818756483a397ad69982af969634404d7b974b13 from
https://github.com/MarvellEmbeddedProcessors/linux-marvell.git

On stop_dev/port the packets pending in aggregated queue should be
sent out (drain aggr queue).

PROBLEM:
This drain must be done for on_each_cpu (which is HW-IRQ context),
preempts a progressing TX-xmit (which is SW-IRQ context).
This preemption damages the TX-logic => need to eliminate context
preemption but serialize the flows.

SOLUTION:
Scedule Drain on tx-tasklet fixes the peempt-priority problem
for stop_dev/tasklet, softirq-fast-path-RX-to-TX and Linux-TX.
(Linux/net/core locks the TX into preemption level ~ softirq).

Change-Id: Ia3ae98d5b2cd3522c69bbdd82f8517ccf9fe4e5c
Signed-off-by: Yan Markman <ymarkman@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/48403
Tested-by: iSoC Platform CI <ykjenk@marvell.com>
Reviewed-by: Omri Itach <omrii@marvell.com>
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c |   92 ++++++++++++--------
 1 file changed, 56 insertions(+), 36 deletions(-)

diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
index 82cb482..6005acd 100644
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
@@ -1293,7 +1293,11 @@ static void mv_pp2x_rxq_drop_pkts(struct mv_pp2x_port *port,
 	u8 *buf_cookie;
 	dma_addr_t buf_phys_addr;
 	struct mv_pp2x_bm_pool *bm_pool;
-	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
+	struct mv_pp2x_cp_pcpu *cp_pcpu;
+
+	preempt_disable();
+	cp_pcpu = this_cpu_ptr(port->priv->pcpu);
+	preempt_enable();
 
 	preempt_disable();
 	rx_received = mv_pp2x_rxq_received(port, rxq->id);
@@ -1965,7 +1969,7 @@ static void mv_pp2x_tx_timer_set(struct mv_pp2x_cp_pcpu *cp_pcpu)
 }
 
 /* Cancel transmit TX timer */
-static void mv_pp2x_tx_timer_kill(struct mv_pp2x_cp_pcpu *cp_pcpu)
+static inline void mv_pp2x_tx_timer_kill(struct mv_pp2x_cp_pcpu *cp_pcpu)
 {
 	if (cp_pcpu->tx_timer_scheduled) {
 		cp_pcpu->tx_timer_scheduled = false;
@@ -1993,6 +1997,17 @@ static void mv_pp2x_tx_proc_cb(unsigned long data)
 		mv_pp2x_timer_set(port_pcpu);
 }
 
+/* TX to HW the pendings in aggregated TXQ; kill deferring TX hrtimer */
+static inline void mv_pp2x_aggr_txq_pend_send(struct mv_pp2x_port *port,
+					      struct mv_pp2x_cp_pcpu *cp_pcpu,
+					      struct mv_pp2x_aggr_tx_queue *aggr_txq)
+{
+	mv_pp2x_tx_timer_kill(cp_pcpu);
+	aggr_txq->hw_count += aggr_txq->sw_count;
+	mv_pp2x_write(&port->priv->hw, MVPP2_AGGR_TXQ_UPDATE_REG, aggr_txq->sw_count);
+	aggr_txq->sw_count = 0;
+}
+
 /* Tasklet transmit procedure */
 static void mv_pp2x_tx_send_proc_cb(unsigned long data)
 {
@@ -3106,10 +3121,7 @@ static inline int mv_pp2_tx_tso(struct sk_buff *skb, struct net_device *dev,
 
 	if (!skb->xmit_more) {
 		/* Transmit TCP segment with bulked descriptors and cancel tx hr timer if exist */
-		aggr_txq->hw_count += aggr_txq->sw_count;
-		mv_pp2x_aggr_txq_pend_desc_add(port, aggr_txq->sw_count);
-		aggr_txq->sw_count = 0;
-		mv_pp2x_tx_timer_kill(cp_pcpu);
+		mv_pp2x_aggr_txq_pend_send(port, cp_pcpu, aggr_txq);
 	}
 
 	txq_pcpu->reserved_num -= total_desc_num;
@@ -3344,14 +3356,10 @@ static int mv_pp2x_tx(struct sk_buff *skb, struct net_device *dev)
 
 	/* Start 50 microseconds timer to transmit */
 	if (!skb->xmit_more) {
-		if (skb->hash == MVPP2_UNIQUE_HASH) {
+		if (skb->hash == MVPP2_UNIQUE_HASH)
 			mv_pp2x_tx_timer_set(cp_pcpu);
-		} else {
-			mv_pp2x_tx_timer_kill(cp_pcpu);
-			aggr_txq->hw_count += aggr_txq->sw_count;
-			mv_pp2x_aggr_txq_pend_desc_add(port, aggr_txq->sw_count);
-			aggr_txq->sw_count = 0;
-		}
+		else
+			mv_pp2x_aggr_txq_pend_send(port, cp_pcpu, aggr_txq);
 	}
 
 out:
@@ -3364,12 +3372,8 @@ static int mv_pp2x_tx(struct sk_buff *skb, struct net_device *dev)
 		u64_stats_update_end(&stats->syncp);
 	} else {
 		/* Transmit bulked descriptors*/
-		if (aggr_txq->sw_count > 0) {
-			mv_pp2x_tx_timer_kill(cp_pcpu);
-			aggr_txq->hw_count += aggr_txq->sw_count;
-			mv_pp2x_aggr_txq_pend_desc_add(port, aggr_txq->sw_count);
-			aggr_txq->sw_count = 0;
-		}
+		if (aggr_txq->sw_count > 0)
+			mv_pp2x_aggr_txq_pend_send(port, cp_pcpu, aggr_txq);
 		dev->stats.tx_dropped++;
 		dev_kfree_skb_any(skb);
 	}
@@ -3700,18 +3704,36 @@ void mv_pp2x_start_dev(struct mv_pp2x_port *port)
 	}
 }
 
-/* Clear aggregated TXQ and kill transmit hrtimer */
-static void mv_pp2x_trans_aggr_txq(void *arg)
+/* Drain pending packets */
+static void mv_pp2x_send_pend_aggr_txq(void *arg)
 {
 	struct mv_pp2x_port *port = arg;
-	struct mv_pp2x_cp_pcpu *cp_pcpu = this_cpu_ptr(port->priv->pcpu);
-	int cpu = smp_processor_id();
-	struct mv_pp2x_aggr_tx_queue *aggr_txq = &port->priv->aggr_txqs[cpu];
+	struct mv_pp2x_aggr_tx_queue *aggr_txq =
+		&port->priv->aggr_txqs[smp_processor_id()];
+	int txq_id;
+	struct mv_pp2x_tx_queue *txq;
+	struct mv_pp2x_txq_pcpu *txq_pcpu;
+	struct mv_pp2x_cp_pcpu *cp_pcpu;
+	bool free_aggr = false;
 
-	mv_pp2x_tx_timer_kill(cp_pcpu);
-	aggr_txq->hw_count += aggr_txq->sw_count;
-	mv_pp2x_aggr_txq_pend_desc_add(port, aggr_txq->sw_count);
-	aggr_txq->sw_count = 0;
+	for (txq_id = 0; txq_id < port->num_tx_queues; txq_id++) {
+		txq = port->txqs[txq_id];
+		txq_pcpu = this_cpu_ptr(txq->pcpu);
+		if (mv_pp2x_txq_free_count(txq_pcpu) != txq->size) {
+			free_aggr = true;
+			break;
+		}
+	}
+
+	if (!aggr_txq->sw_count || !free_aggr)
+		return; /* no pendings */
+
+	/* Schedule Drain over the same tasklet-context
+	 * which the regular TX is using (refer mv_pp2x_tx_send_proc_cb).
+	 * So the Drain from the stop_dev and TX are unpreemptive and correct.
+	 */
+	cp_pcpu = this_cpu_ptr(port->priv->pcpu);
+	tasklet_schedule(&cp_pcpu->tx_tasklet);
 }
 
 /* Set hw internals when stopping port */
@@ -3720,22 +3742,20 @@ void mv_pp2x_stop_dev(struct mv_pp2x_port *port)
 	struct gop_hw *gop = &port->priv->hw.gop;
 	struct mv_mac_data *mac = &port->mac_data;
 
-	/* Stop new packets from arriving to RXQs */
+	/* Stop new packets arriving from RX-interrupts and Linux-TX */
 	mv_pp2x_ingress_disable(port);
+	netif_carrier_off(port->dev);
+	msleep(20);
 
-	mdelay(10);
+	/* Drain pending aggregated TXQ on all CPUs */
+	on_each_cpu(mv_pp2x_send_pend_aggr_txq, port, 1);
+	msleep(200); /* yield and wait for tx-tasklet and HW idle */
 
 	/* Disable interrupts on all CPUs */
 	mv_pp2x_port_interrupts_disable(port);
 
-	/* Drain aggregated TXQ on all CPU's */
-	on_each_cpu(mv_pp2x_trans_aggr_txq, port, 1);
-
 	mv_pp2x_port_napi_disable(port);
-
-	netif_carrier_off(port->dev);
 	netif_tx_stop_all_queues(port->dev);
-
 	mv_pp2x_egress_disable(port);
 
 	if (port->comphy)
-- 
1.7.9.5

