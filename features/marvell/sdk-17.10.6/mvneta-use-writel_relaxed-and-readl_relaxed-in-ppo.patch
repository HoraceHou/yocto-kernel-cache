From 7a6ae2db6e1c724c5c04909537b355d14ca48fb6 Mon Sep 17 00:00:00 2001
From: Yelena <yelena@marvell.com>
Date: Thu, 15 Dec 2016 15:34:03 +0200
Subject: [PATCH 0711/1345] mvneta: use writel_relaxed() and readl_relaxed()
 in ppo

commit  4120e60ca4104faf17475de3688cd1d390db12ca from
https://github.com/MarvellEmbeddedProcessors/linux-marvell.git

	Done to improve driver performance.

Change-Id: I326bac4c9394c4c0cf07b32855998131aef95a94
Signed-off-by: Yelena <yelena@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/34721
Tested-by: iSoC Platform CI <ykjenk@marvell.com>
Reviewed-by: Wilson Ding <dingwei@marvell.com>
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 drivers/net/ethernet/marvell/mvneta.c |   57 +++++++++++++++++++++------------
 1 file changed, 36 insertions(+), 21 deletions(-)

diff --git a/drivers/net/ethernet/marvell/mvneta.c b/drivers/net/ethernet/marvell/mvneta.c
index 6614082..e659517 100644
--- a/drivers/net/ethernet/marvell/mvneta.c
+++ b/drivers/net/ethernet/marvell/mvneta.c
@@ -646,6 +646,18 @@ static u32 mvreg_read(struct mvneta_port *pp, u32 offset)
 	return readl(pp->base + offset);
 }
 
+/* Write helper method */
+static inline void mvreg_relaxed_write(struct mvneta_port *pp, u32 offset, u32 data)
+{
+	writel_relaxed(data, pp->base + offset);
+}
+
+/* Read helper method */
+static inline u32 mvreg_relaxed_read(struct mvneta_port *pp, u32 offset)
+{
+	return readl_relaxed(pp->base + offset);
+}
+
 /* Increment txq get counter */
 static void mvneta_txq_inc_get(struct mvneta_tx_queue *txq)
 {
@@ -771,6 +783,9 @@ static void mvneta_rxq_desc_num_update(struct mvneta_port *pp,
 		return;
 	}
 
+	/* do one write barrier and use relaxed write in loop */
+	__iowmb();
+
 	/* Only 255 descriptors can be added at once */
 	while ((rx_done > 0) || (rx_filled > 0)) {
 		if (rx_done <= 0xff) {
@@ -787,7 +802,7 @@ static void mvneta_rxq_desc_num_update(struct mvneta_port *pp,
 			val |= 0xff << MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT;
 			rx_filled -= 0xff;
 		}
-		mvreg_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id), val);
+		mvreg_relaxed_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id), val);
 	}
 }
 
@@ -1429,10 +1444,10 @@ static void mvneta_percpu_unmask_interrupt(void *arg)
 	/* All the queue are unmasked, but actually only the ones
 	 * mapped to this CPU will be unmasked
 	 */
-	mvreg_write(pp, MVNETA_INTR_NEW_MASK,
-		    MVNETA_RX_INTR_MASK_ALL |
-		    MVNETA_TX_INTR_MASK_ALL |
-		    MVNETA_MISCINTR_INTR_MASK);
+	mvreg_relaxed_write(pp, MVNETA_INTR_NEW_MASK,
+			    MVNETA_RX_INTR_MASK_ALL |
+			    MVNETA_TX_INTR_MASK_ALL |
+			    MVNETA_MISCINTR_INTR_MASK);
 }
 
 static void mvneta_percpu_mask_interrupt(void *arg)
@@ -1442,9 +1457,9 @@ static void mvneta_percpu_mask_interrupt(void *arg)
 	/* All the queue are masked, but actually only the ones
 	 * mapped to this CPU will be masked
 	 */
-	mvreg_write(pp, MVNETA_INTR_NEW_MASK, 0);
-	mvreg_write(pp, MVNETA_INTR_OLD_MASK, 0);
-	mvreg_write(pp, MVNETA_INTR_MISC_MASK, 0);
+	mvreg_relaxed_write(pp, MVNETA_INTR_NEW_MASK, 0);
+	mvreg_relaxed_write(pp, MVNETA_INTR_OLD_MASK, 0);
+	mvreg_relaxed_write(pp, MVNETA_INTR_MISC_MASK, 0);
 }
 
 static void mvneta_percpu_clear_intr_cause(void *arg)
@@ -1454,9 +1469,9 @@ static void mvneta_percpu_clear_intr_cause(void *arg)
 	/* All the queue are cleared, but actually only the ones
 	 * mapped to this CPU will be cleared
 	 */
-	mvreg_write(pp, MVNETA_INTR_NEW_CAUSE, 0);
-	mvreg_write(pp, MVNETA_INTR_MISC_CAUSE, 0);
-	mvreg_write(pp, MVNETA_INTR_OLD_CAUSE, 0);
+	mvreg_relaxed_write(pp, MVNETA_INTR_NEW_CAUSE, 0);
+	mvreg_relaxed_write(pp, MVNETA_INTR_MISC_CAUSE, 0);
+	mvreg_relaxed_write(pp, MVNETA_INTR_OLD_CAUSE, 0);
 }
 
 /* This method sets defaults to the NETA port:
@@ -1742,12 +1757,12 @@ static void mvneta_txq_sent_desc_dec(struct mvneta_port *pp,
 	/* Only 255 TX descriptors can be updated at once */
 	while (sent_desc > 0xff) {
 		val = 0xff << MVNETA_TXQ_DEC_SENT_SHIFT;
-		mvreg_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);
+		mvreg_relaxed_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);
 		sent_desc = sent_desc - 0xff;
 	}
 
 	val = sent_desc << MVNETA_TXQ_DEC_SENT_SHIFT;
-	mvreg_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);
+	mvreg_relaxed_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);
 }
 
 /* Get number of TX descriptors already sent by HW */
@@ -2890,7 +2905,7 @@ static irqreturn_t mvneta_isr(int irq, void *dev_id)
 {
 	struct mvneta_port *pp = (struct mvneta_port *)dev_id;
 
-	mvreg_write(pp, MVNETA_INTR_NEW_MASK, 0);
+	mvreg_relaxed_write(pp, MVNETA_INTR_NEW_MASK, 0);
 	napi_schedule(&pp->napi);
 
 	return IRQ_HANDLED;
@@ -2951,11 +2966,11 @@ static int mvneta_poll(struct napi_struct *napi, int budget)
 	}
 
 	/* Read cause register */
-	cause_rx_tx = mvreg_read(pp, MVNETA_INTR_NEW_CAUSE);
+	cause_rx_tx = mvreg_relaxed_read(pp, MVNETA_INTR_NEW_CAUSE);
 	if (cause_rx_tx & MVNETA_MISCINTR_INTR_MASK) {
-		u32 cause_misc = mvreg_read(pp, MVNETA_INTR_MISC_CAUSE);
+		u32 cause_misc = mvreg_relaxed_read(pp, MVNETA_INTR_MISC_CAUSE);
 
-		mvreg_write(pp, MVNETA_INTR_MISC_CAUSE, 0);
+		mvreg_relaxed_write(pp, MVNETA_INTR_MISC_CAUSE, 0);
 		if (pp->use_inband_status && (cause_misc &
 				(MVNETA_CAUSE_PHY_STATUS_CHANGE |
 				 MVNETA_CAUSE_LINK_CHANGE |
@@ -2994,10 +3009,10 @@ static int mvneta_poll(struct napi_struct *napi, int budget)
 			unsigned long flags;
 
 			local_irq_save(flags);
-			mvreg_write(pp, MVNETA_INTR_NEW_MASK,
-				    MVNETA_RX_INTR_MASK(rxq_number) |
-				    MVNETA_TX_INTR_MASK(txq_number) |
-				    MVNETA_MISCINTR_INTR_MASK);
+			mvreg_relaxed_write(pp, MVNETA_INTR_NEW_MASK,
+					    MVNETA_RX_INTR_MASK(rxq_number) |
+					    MVNETA_TX_INTR_MASK(txq_number) |
+					    MVNETA_MISCINTR_INTR_MASK);
 			local_irq_restore(flags);
 		} else {
 			enable_percpu_irq(pp->dev->irq, 0);
-- 
1.7.9.5

