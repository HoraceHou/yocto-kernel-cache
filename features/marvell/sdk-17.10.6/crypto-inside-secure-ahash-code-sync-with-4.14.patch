From b790ede977a77c741b9ff741aec20074533e82d6 Mon Sep 17 00:00:00 2001
From: Ofer Heifetz <oferh@marvell.com>
Date: Sun, 14 Jan 2018 10:25:53 +0200
Subject: [PATCH 1285/1345] crypto: inside-secure: ahash code sync with 4.14

commit  67d5f1b63760bc546e0dc616fb9b166f4ed694d4 from
https://github.com/MarvellEmbeddedProcessors/linux-marvell.git

code base originates in 4.13, several bugs were fixed by initial
commit:
- fixed hash cache related bugs
- generalize the the import/export routines
- generalize hmac code to work with other SHA types
- fixed corner case issues found with OpenSSL

Change-Id: I1b90c065319f49cd5957541b7eff08aab74f0daa
Signed-off-by: Ofer Heifetz <oferh@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/48930
Tested-by: iSoC Platform CI <ykjenk@marvell.com>
Reviewed-by: Hanna Hawa <hannah@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/51666
Tested-by: Hanna Hawa <hannah@marvell.com>
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 drivers/crypto/inside-secure/hash.c     |  566 +++++++++++++++++--------------
 drivers/crypto/inside-secure/safexcel.c |    4 +-
 drivers/crypto/inside-secure/safexcel.h |    1 +
 3 files changed, 311 insertions(+), 260 deletions(-)

diff --git a/drivers/crypto/inside-secure/hash.c b/drivers/crypto/inside-secure/hash.c
index 39754d0..0872a04 100644
--- a/drivers/crypto/inside-secure/hash.c
+++ b/drivers/crypto/inside-secure/hash.c
@@ -35,11 +35,20 @@ struct safexcel_ahash_req {
 	u32 state[SHA256_DIGEST_SIZE / sizeof(u32)] __aligned(sizeof(u32));
 
 	u64 len;
+	u64 processed;
 
 	u8 cache[SHA256_BLOCK_SIZE] __aligned(sizeof(u32));
 	u8 cache_next[SHA256_BLOCK_SIZE] __aligned(sizeof(u32));
 };
 
+struct safexcel_ahash_export_state {
+	u64 len;
+	u64 processed;
+
+	u32 state[SHA256_DIGEST_SIZE / sizeof(u32)];
+	u8 cache[SHA256_BLOCK_SIZE];
+};
+
 static const u8 sha1_zero_digest[SHA1_DIGEST_SIZE] = {
 	0xda, 0x39, 0xa3, 0xee, 0x5e, 0x6b, 0x4b, 0x0d, 0x32, 0x55,
 	0xbf, 0xef, 0x95, 0x60, 0x18, 0x90, 0xaf, 0xd8, 0x07, 0x09,
@@ -93,7 +102,7 @@ static void safexcel_context_control(struct safexcel_ahash_ctx *ctx,
 	cdesc->control_data.control0 |= ctx->digest;
 
 	if (ctx->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED) {
-		if (req->len) {
+		if (req->processed) {
 			if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA1)
 				cdesc->control_data.control0 |= CONTEXT_CONTROL_SIZE(6);
 			else if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA224 ||
@@ -101,7 +110,6 @@ static void safexcel_context_control(struct safexcel_ahash_ctx *ctx,
 				cdesc->control_data.control0 |= CONTEXT_CONTROL_SIZE(9);
 
 			cdesc->control_data.control1 |= CONTEXT_CONTROL_DIGEST_CNT;
-
 		} else {
 			cdesc->control_data.control0 |= CONTEXT_CONTROL_RESTART_HASH;
 		}
@@ -114,12 +122,12 @@ static void safexcel_context_control(struct safexcel_ahash_ctx *ctx,
 		 * fields. Do this now as we need it to setup the first command
 		 * descriptor.
 		 */
-		if (req->len) {
+		if (req->processed) {
 			for (i = 0; i < digestsize / sizeof(u32); i++)
 				ctx->base.ctxr->data[i] = cpu_to_le32(req->state[i]);
 
 			if (req->finish)
-				ctx->base.ctxr->data[i] = cpu_to_le32(req->len / blocksize);
+				ctx->base.ctxr->data[i] = cpu_to_le32(req->processed / blocksize);
 		}
 	} else if (ctx->digest == CONTEXT_CONTROL_DIGEST_HMAC) {
 		cdesc->control_data.control0 |= CONTEXT_CONTROL_SIZE(10);
@@ -135,11 +143,10 @@ static int safexcel_handle_req_result(struct safexcel_crypto_priv *priv, int rin
 				      bool *should_complete, int *ret)
 {
 	struct safexcel_result_desc *rdesc;
-	int cache_next_len, len;
 	struct ahash_request *areq = ahash_request_cast(async);
 	struct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);
 	struct safexcel_ahash_req *sreq = ahash_request_ctx(areq);
-	int result_sz = sreq->state_sz;
+	int cache_len;
 
 	*ret = 0;
 
@@ -160,18 +167,17 @@ static int safexcel_handle_req_result(struct safexcel_crypto_priv *priv, int rin
 	spin_unlock_bh(&priv->ring[ring].egress_lock);
 
 	if (sreq->finish)
-		result_sz = crypto_ahash_digestsize(ahash);
-	memcpy(sreq->state, areq->result, result_sz);
+		memcpy(areq->result, sreq->state,
+		       crypto_ahash_digestsize(ahash));
 
 	dma_unmap_sg(priv->dev, areq->src,
 		     sg_nents_for_len(areq->src, areq->nbytes), DMA_TO_DEVICE);
 
 	safexcel_free_context(priv, async, sreq->state_sz);
 
-	len = sreq->len;
-	cache_next_len = do_div(len, crypto_ahash_blocksize(ahash));
-	if (cache_next_len)
-		memcpy(sreq->cache, sreq->cache_next, cache_next_len);
+	cache_len = sreq->len - sreq->processed;
+	if (cache_len)
+		memcpy(sreq->cache, sreq->cache_next, cache_len);
 
 	*should_complete = true;
 
@@ -191,28 +197,56 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
 	struct safexcel_command_desc *cdesc, *first_cdesc = NULL;
 	struct safexcel_result_desc *rdesc;
 	struct scatterlist *sg;
-	int i, nents, queued, len = req->len, n_cdesc = 0, ret = 0;
-	int cache_len = do_div(len, crypto_ahash_blocksize(ahash));
-
-	if (req->last_req) {
-		if (!cache_len)
-			queued = len = areq->nbytes;
-		else
-			queued = len = cache_len;
-	} else {
-		queued = len = (cache_len + areq->nbytes) &
-			       ~(crypto_ahash_blocksize(ahash) - 1);
+	int i, nents, queued, len, cache_len, extra, n_cdesc = 0, ret = 0;
+
+	queued = len = req->len - req->processed;
+	if (queued < crypto_ahash_blocksize(ahash))
+		cache_len = queued;
+	else
+		cache_len = queued - areq->nbytes;
+
+	if (!req->last_req) {
+		/* If this is not the last request and the queued data does not
+		 * fit into full blocks, cache it for the next send() call.
+		 */
+		extra = queued & (crypto_ahash_blocksize(ahash) - 1);
+		if (!extra)
+			/* If this is not the last request and the queued data
+			 * is a multiple of a block, cache the last one for now.
+			 */
+			extra = queued - crypto_ahash_blocksize(ahash);
+
+		if (extra) {
+			sg_pcopy_to_buffer(areq->src, sg_nents(areq->src),
+					   req->cache_next, extra,
+					   areq->nbytes - extra);
+
+			queued -= extra;
+			len -= extra;
+
+			if (!queued) {
+				*commands = 0;
+				*results = 0;
+				return 0;
+			}
+		}
 	}
 
 	spin_lock_bh(&priv->ring[ring].egress_lock);
 
 	/* Add a command descriptor for the cached data, if any */
 	if (cache_len) {
-		ctx->base.cache_dma = dma_map_single(priv->dev, req->cache,
-						      cache_len, DMA_TO_DEVICE);
+		ctx->base.cache = kzalloc(cache_len, EIP197_GFP_FLAGS(*async));
+		if (!ctx->base.cache) {
+			ret = -ENOMEM;
+			goto unlock;
+		}
+		memcpy(ctx->base.cache, req->cache, cache_len);
+		ctx->base.cache_dma = dma_map_single(priv->dev, ctx->base.cache,
+						     cache_len, DMA_TO_DEVICE);
 		if (dma_mapping_error(priv->dev, ctx->base.cache_dma)) {
 			ret = -EINVAL;
-			goto unlock;
+			goto free_cache;
 		}
 
 		ctx->base.cache_sz = cache_len;
@@ -223,7 +257,7 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
 						 ctx->base.ctxr_dma);
 		if (IS_ERR(first_cdesc)) {
 			ret = PTR_ERR(first_cdesc);
-			goto free_cache;
+			goto unmap_cache;
 		}
 		n_cdesc++;
 
@@ -273,7 +307,7 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
 	/* Add the token */
 	safexcel_hash_token(first_cdesc, len, req->state_sz);
 
-	ctx->base.result_dma = dma_map_single(priv->dev, areq->result,
+	ctx->base.result_dma = dma_map_single(priv->dev, req->state,
 					      req->state_sz,
 					      DMA_FROM_DEVICE);
 	if (dma_mapping_error(priv->dev, ctx->base.result_dma)) {
@@ -299,7 +333,7 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
 
 	spin_unlock_bh(&priv->ring[ring].egress_lock);
 
-	req->len += areq->nbytes;
+	req->processed += len;
 
 	*commands = n_cdesc;
 	*results = 1;
@@ -313,10 +347,15 @@ static int safexcel_ahash_send_req(struct crypto_async_request *async, int ring,
 
 	for (i = 0; i < n_cdesc; i++)
 		safexcel_ring_rollback_wptr(priv, &priv->ring[ring].cdr);
-free_cache:
-	if (ctx->base.cache_dma)
+unmap_cache:
+	if (ctx->base.cache_dma) {
 		dma_unmap_single(priv->dev, ctx->base.cache_dma,
 				 ctx->base.cache_sz, DMA_TO_DEVICE);
+		ctx->base.cache_sz = 0;
+	}
+free_cache:
+	kfree(ctx->base.cache);
+	ctx->base.cache = NULL;
 
 unlock:
 	spin_unlock_bh(&priv->ring[ring].egress_lock);
@@ -337,7 +376,7 @@ static inline bool safexcel_ahash_needs_inv_get(struct ahash_request *areq)
 			return true;
 
 	if (ctx->base.ctxr->data[state_w_sz] !=
-	    cpu_to_le32(req->len / crypto_ahash_blocksize(ahash)))
+	    cpu_to_le32(req->processed / crypto_ahash_blocksize(ahash)))
 		return true;
 
 	return false;
@@ -491,94 +530,46 @@ static int safexcel_ahash_exit_inv(struct crypto_tfm *tfm)
 	return 0;
 }
 
-static int safexcel_ahash_update(struct ahash_request *areq)
+/* safexcel_ahash_cache: cache data until at least one request can be sent to
+ * the engine, aka. when there is at least 1 block size in the pipe.
+ */
+static int safexcel_ahash_cache(struct ahash_request *areq)
 {
-	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
 	struct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);
-	struct safexcel_crypto_priv *priv = ctx->priv;
-	int queued, len = req->len, ret, ring;
-	int cache_len = do_div(len, crypto_ahash_blocksize(ahash));
+	int queued, cache_len;
 
-	/*
-	 * We're not doing partial updates when performing an hmac request, and
-	 * we're not using the cache. Everything will be handled by the final()
-	 * call.
+	/* cache_len: everyting accepted by the driver but not sent yet,
+	 * tot sz handled by update() - last req sz - tot sz handled by send()
 	 */
-	if (req->hmac && !req->last_req)
-		return 0;
-
-	if (req->last_req) {
-		if (!cache_len)
-			queued = len = areq->nbytes;
-		else
-			queued = len = cache_len;
-	} else {
-		queued = len = cache_len + areq->nbytes;
-	}
-
-	/* If the request is 0 length and is not the last request, do nothing */
-	if (!areq->nbytes && !req->last_req)
-		return 0;
+	cache_len = req->len - areq->nbytes - req->processed;
+	/* queued: everything accepted by the driver which will be handled by
+	 * the next send() calls.
+	 * tot sz handled by update() - tot sz handled by send()
+	 */
+	queued = req->len - req->processed;
 
 	/*
-	 * If we have an overall 0 length request, we only need a "dummy"
-	 * command descriptor.
+	 * In case there isn't enough bytes to proceed (less than a
+	 * block size), cache the data until we have enough.
 	 */
-	if (!queued) {
-		if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA1)
-			memcpy(areq->result, sha1_zero_digest, SHA1_DIGEST_SIZE);
-		else if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA224)
-			memcpy(areq->result, sha224_zero_digest, SHA224_DIGEST_SIZE);
-		else if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA256)
-			memcpy(areq->result, sha256_zero_digest, SHA256_DIGEST_SIZE);
-
-		return 0;
+	if (cache_len + areq->nbytes <= crypto_ahash_blocksize(ahash)) {
+		sg_pcopy_to_buffer(areq->src, sg_nents(areq->src),
+				   req->cache + cache_len,
+				   areq->nbytes, 0);
+		return areq->nbytes;
 	}
 
-	/* If this isn't the last request, we can involve the cache */
-	if (!req->last_req) {
-		u32 extra;
-
-		/*
-		 * In case there isn't enough bytes to proceed (less than a
-		 * block size + 1), cache the data until we have enough.
-		 *
-		 * The last request should have the CONTEXT_CONTROL_FINISH_HASH
-		 * bit set in the first context control word. This means we
-		 * can't cache a request of exactly one block size because we
-		 * wouldn't be able to perform this when called from final():
-		 * the request size would be 0, which isn't supported.
-		 */
-		if (cache_len + areq->nbytes < crypto_ahash_blocksize(ahash) + 1) {
-			sg_pcopy_to_buffer(areq->src, sg_nents(areq->src),
-					   req->cache + cache_len,
-					   areq->nbytes, 0);
-			return 0;
-		}
-
-		/*
-		 * In case we have enough data to proceed, we can send n blocks
-		 * to the hash engine while caching the bytes not in these
-		 * blocks.
-		 */
-		extra = len & (crypto_ahash_blocksize(ahash) - 1);
-
-		/*
-		 * For the exact same reason as stated above, if we have a
-		 * request size being exactly a multiple of the block size, we
-		 * must queue one block in case it's the final one.
-		 */
-		if (!extra)
-			extra = crypto_ahash_blocksize(ahash);
-
+	/* We couldn't cache all the data */
+	return -E2BIG;
+}
 
-		sg_pcopy_to_buffer(areq->src, sg_nents(areq->src),
-				   req->cache_next, extra,
-				   areq->nbytes - extra);
-		len -= extra;
-		queued -= extra;
-	}
+static int safexcel_ahash_enqueue(struct ahash_request *areq)
+{
+	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+	struct safexcel_crypto_priv *priv = ctx->priv;
+	int ret, ring;
 
 	req->needs_inv = false;
 
@@ -591,7 +582,17 @@ static int safexcel_ahash_update(struct ahash_request *areq)
 	 * If it's EIP97 with existing context, the send routine is already set.
 	 */
 	if (ctx->base.ctxr) {
-		if (priv->eip_type == EIP197 && ctx->base.needs_inv) {
+		if (priv->eip_type == EIP197 &&
+		    !ctx->base.needs_inv && req->processed &&
+		    ctx->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED)
+			/* We're still setting needs_inv here, even though it is
+			 * cleared right away, because the needs_inv flag can be
+			 * set in other functions and we want to keep the same
+			 * logic.
+			 */
+			ctx->base.needs_inv = safexcel_ahash_needs_inv_get(areq);
+
+		if (ctx->base.needs_inv) {
 			ctx->base.needs_inv = false;
 			req->needs_inv = true;
 		}
@@ -616,57 +617,105 @@ static int safexcel_ahash_update(struct ahash_request *areq)
 	return ret;
 }
 
+static int safexcel_ahash_update(struct ahash_request *areq)
+{
+	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+	struct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);
+
+	/* If the request is 0 length, do nothing */
+	if (!areq->nbytes)
+		return 0;
+
+	req->len += areq->nbytes;
+
+	safexcel_ahash_cache(areq);
+
+	/*
+	 * We're not doing partial updates when performing an hmac request.
+	 * Everything will be handled by the final() call.
+	 */
+	if (ctx->digest == CONTEXT_CONTROL_DIGEST_HMAC)
+		return 0;
+
+	if (req->hmac)
+		return safexcel_ahash_enqueue(areq);
+
+	if (!req->last_req &&
+	    req->len - req->processed > crypto_ahash_blocksize(ahash))
+		return safexcel_ahash_enqueue(areq);
+
+	return 0;
+}
+
 static int safexcel_ahash_final(struct ahash_request *areq)
 {
 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
-	struct safexcel_crypto_priv *priv = ctx->priv;
 
 	req->last_req = true;
 	req->finish = true;
 
-	/*
-	 * Check if we need to invalidate the context,
-	 * this should be done only for EIP197 (no cache in EIP97).
-	 */
-	if (priv->eip_type == EIP197 && req->len && ctx->base.ctxr &&
-	    ctx->digest == CONTEXT_CONTROL_DIGEST_PRECOMPUTED)
-		ctx->base.needs_inv = safexcel_ahash_needs_inv_get(areq);
+	/* If we have an overall 0 length request */
+	if (!(req->len + areq->nbytes)) {
+		if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA1)
+			memcpy(areq->result, sha1_zero_digest,
+			       SHA1_DIGEST_SIZE);
+		else if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA224)
+			memcpy(areq->result, sha224_zero_digest,
+			       SHA224_DIGEST_SIZE);
+		else if (ctx->alg == CONTEXT_CONTROL_CRYPTO_ALG_SHA256)
+			memcpy(areq->result, sha256_zero_digest,
+			       SHA256_DIGEST_SIZE);
 
-	return safexcel_ahash_update(areq);
+		return 0;
+	}
+
+	return safexcel_ahash_enqueue(areq);
 }
 
 static int safexcel_ahash_finup(struct ahash_request *areq)
 {
+	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+
+	req->last_req = true;
+	req->finish = true;
+
+	safexcel_ahash_update(areq);
 	return safexcel_ahash_final(areq);
 }
 
-static int safexcel_sha1_export(struct ahash_request *areq, void *out)
+static int safexcel_ahash_export(struct ahash_request *areq, void *out)
 {
 	struct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);
 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
-	struct sha1_state *sha1 = out;
-	int len = req->len;
-	int cache_len = do_div(len, crypto_ahash_blocksize(ahash));
+	struct safexcel_ahash_export_state *export = out;
+
+	export->len = req->len;
+	export->processed = req->processed;
 
-	sha1->count = req->len;
-	memcpy(sha1->state, req->state, req->state_sz);
-	memset(sha1->buffer, 0, crypto_ahash_blocksize(ahash));
-	memcpy(sha1->buffer, req->cache, cache_len);
+	memcpy(export->state, req->state, req->state_sz);
+	memcpy(export->cache, req->cache, crypto_ahash_blocksize(ahash));
 
 	return 0;
 }
 
-static int safexcel_sha1_import(struct ahash_request *areq, const void *in)
+static int safexcel_ahash_import(struct ahash_request *areq, const void *in)
 {
+	struct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);
 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
-	const struct sha1_state *sha1 = in;
+	const struct safexcel_ahash_export_state *export = in;
+	int ret;
 
-	memset(req, 0, sizeof(*req));
+	ret = crypto_ahash_init(areq);
+	if (ret)
+		return ret;
+
+	req->len = export->len;
+	req->processed = export->processed;
 
-	req->len = sha1->count;
-	memcpy(req->cache, sha1->buffer, sha1->count);
-	memcpy(req->state, sha1->state, req->state_sz);
+	memcpy(req->cache, export->cache, crypto_ahash_blocksize(ahash));
+	memcpy(req->state, export->state, req->state_sz);
 
 	return 0;
 }
@@ -753,11 +802,11 @@ struct safexcel_alg_template safexcel_alg_sha1 = {
 		.final = safexcel_ahash_final,
 		.finup = safexcel_ahash_finup,
 		.digest = safexcel_sha1_digest,
-		.export = safexcel_sha1_export,
-		.import = safexcel_sha1_import,
+		.export = safexcel_ahash_export,
+		.import = safexcel_ahash_import,
 		.halg = {
 			.digestsize = SHA1_DIGEST_SIZE,
-			.statesize = sizeof(struct sha1_state),
+			.statesize = sizeof(struct safexcel_ahash_export_state),
 			.base = {
 				.cra_name = "sha1",
 				.cra_driver_name = "safexcel-sha1",
@@ -777,11 +826,9 @@ struct safexcel_alg_template safexcel_alg_sha1 = {
 static int safexcel_hmac_sha1_init(struct ahash_request *areq)
 {
 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
-	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
 
 	safexcel_sha1_init(areq);
 	ctx->digest = CONTEXT_CONTROL_DIGEST_HMAC;
-	req->hmac = true;
 	return 0;
 }
 
@@ -811,28 +858,78 @@ static void safexcel_ahash_complete(struct crypto_async_request *req, int error)
 	complete(&result->completion);
 }
 
-static int safexcel_hmac_prepare_pad(struct ahash_request *areq, u8 *pad,
-				     unsigned int blocksize, void *state,
-				     bool finish)
+static int safexcel_hmac_init_pad(struct ahash_request *areq,
+				  unsigned int blocksize, const u8 *key,
+				  unsigned int keylen, u8 *ipad, u8 *opad)
 {
-	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+	struct safexcel_ahash_result result;
 	struct scatterlist sg;
+	int ret, i;
+	u8 *keydup;
+
+	if (keylen <= blocksize) {
+		memcpy(ipad, key, keylen);
+	} else {
+		keydup = kmemdup(key, keylen, GFP_KERNEL);
+		if (!keydup)
+			return -ENOMEM;
+
+		ahash_request_set_callback(areq, CRYPTO_TFM_REQ_MAY_BACKLOG,
+					   safexcel_ahash_complete, &result);
+		sg_init_one(&sg, keydup, keylen);
+		ahash_request_set_crypt(areq, &sg, ipad, keylen);
+		init_completion(&result.completion);
+
+		ret = crypto_ahash_digest(areq);
+		if (ret == -EINPROGRESS) {
+			wait_for_completion_interruptible(&result.completion);
+			ret = result.error;
+		}
+
+		/* Avoid leaking */
+		memzero_explicit(keydup, keylen);
+		kfree(keydup);
+
+		if (ret)
+			return ret;
+
+		keylen = crypto_ahash_digestsize(crypto_ahash_reqtfm(areq));
+	}
+
+	memset(ipad + keylen, 0, blocksize - keylen);
+	memcpy(opad, ipad, blocksize);
+
+	for (i = 0; i < blocksize; i++) {
+		ipad[i] ^= 0x36;
+		opad[i] ^= 0x5c;
+	}
+
+	return 0;
+}
+
+static int safexcel_hmac_init_iv(struct ahash_request *areq,
+				 unsigned int blocksize, u8 *pad, void *state)
+{
 	struct safexcel_ahash_result result;
+	struct safexcel_ahash_req *req;
+	struct scatterlist sg;
 	int ret;
 
-	sg_init_one(&sg, pad, blocksize);
-	init_completion(&result.completion);
-	ahash_request_set_crypt(areq, &sg, pad, blocksize);
 	ahash_request_set_callback(areq, CRYPTO_TFM_REQ_MAY_BACKLOG,
 				   safexcel_ahash_complete, &result);
+	sg_init_one(&sg, pad, blocksize);
+	ahash_request_set_crypt(areq, &sg, pad, blocksize);
+	init_completion(&result.completion);
 
-	ret = safexcel_sha1_init(areq);
+	ret = crypto_ahash_init(areq);
 	if (ret)
 		return ret;
 
-	req->last_req = 1;
-	req->finish = finish;
-	ret = safexcel_ahash_update(areq);
+	req = ahash_request_ctx(areq);
+	req->hmac = true;
+	req->last_req = true;
+
+	ret = crypto_ahash_update(areq);
 	if (ret && ret != -EINPROGRESS && ret != -EBUSY)
 		return ret;
 
@@ -840,105 +937,86 @@ static int safexcel_hmac_prepare_pad(struct ahash_request *areq, u8 *pad,
 	if (result.error)
 		return result.error;
 
-	ret = crypto_ahash_export(areq, state);
-	if (ret)
-		return ret;
-
-	return 0;
+	return crypto_ahash_export(areq, state);
 }
 
-static int safexcel_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 *key,
-				     unsigned int keylen)
+static int safexcel_hmac_setkey(const char *alg, const u8 *key,
+				unsigned int keylen, void *istate, void *ostate)
 {
-	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
-	struct safexcel_crypto_priv *priv = ctx->priv;
 	struct ahash_request *areq;
-	struct crypto_ahash *ahash;
-	struct sha1_state s0, s1;
+	struct crypto_ahash *tfm;
 	unsigned int blocksize;
-	int i, ret = 0;
-	u8 ipad[SHA1_BLOCK_SIZE], opad[SHA1_BLOCK_SIZE];
+	u8 *ipad, *opad;
+	int ret;
 
-	/*
-	 * Prepare the ipad and the opad, which are needed as input digests for
-	 * the hmac operation.
-	 */
-	ahash = crypto_alloc_ahash("safexcel-sha1", CRYPTO_ALG_TYPE_AHASH,
-				   CRYPTO_ALG_TYPE_AHASH_MASK);
-	if (IS_ERR(ahash))
-		return PTR_ERR(ahash);
+	tfm = crypto_alloc_ahash(alg, CRYPTO_ALG_TYPE_AHASH,
+				 CRYPTO_ALG_TYPE_AHASH_MASK);
+	if (IS_ERR(tfm))
+		return PTR_ERR(tfm);
 
-	areq = ahash_request_alloc(ahash, GFP_KERNEL);
+	areq = ahash_request_alloc(tfm, GFP_KERNEL);
 	if (!areq) {
 		ret = -ENOMEM;
 		goto free_ahash;
 	}
 
-	crypto_ahash_clear_flags(ahash, ~0);
-	blocksize = crypto_tfm_alg_blocksize(crypto_ahash_tfm(ahash));
+	crypto_ahash_clear_flags(tfm, ~0);
+	blocksize = crypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));
 
-	if (keylen <= blocksize) {
-		memcpy(ipad, key, keylen);
-	} else if (keylen > blocksize) {
-		/*
-		 * If the key is larger than a block size, we need to hash the
-		 * key before computing ipad and opad.
-		 */
-		struct sha1_state skey;
-		u8 *keydup = kmemdup(key, keylen, GFP_KERNEL);
+	ipad = kzalloc(2 * blocksize, GFP_KERNEL);
+	if (!ipad) {
+		ret = -ENOMEM;
+		goto free_request;
+	}
 
-		if (!keydup) {
-			ret = -ENOMEM;
-			goto free_req;
-		}
+	opad = ipad + blocksize;
 
-		ret = safexcel_hmac_prepare_pad(areq, keydup, keylen, &skey, true);
-		memset(keydup, 0, keylen);
-		kfree(keydup);
-		if (ret)
-			goto free_req;
+	ret = safexcel_hmac_init_pad(areq, blocksize, key, keylen, ipad, opad);
+	if (ret)
+		goto free_ipad;
 
-		memcpy(ipad, skey.state, ARRAY_SIZE(skey.state) * sizeof(u32));
-		keylen = ARRAY_SIZE(skey.state) * sizeof(u32);
-	}
-	memset(ipad + keylen, 0, blocksize - keylen);
-	memcpy(opad, ipad, blocksize);
+	ret = safexcel_hmac_init_iv(areq, blocksize, ipad, istate);
+	if (ret)
+		goto free_ipad;
 
-	for (i = 0; i < blocksize; i++) {
-		ipad[i] ^= 0x36;
-		opad[i] ^= 0x5c;
-	}
+	ret = safexcel_hmac_init_iv(areq, blocksize, opad, ostate);
 
-	ret = safexcel_hmac_prepare_pad(areq, ipad, blocksize, &s0, false);
-	if (ret)
-		goto free_req;
+free_ipad:
+	kfree(ipad);
+free_request:
+	ahash_request_free(areq);
+free_ahash:
+	crypto_free_ahash(tfm);
 
-	ret = safexcel_hmac_prepare_pad(areq, opad, blocksize, &s1, false);
+	return ret;
+}
+
+static int safexcel_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 *key,
+				     unsigned int keylen)
+{
+	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+	struct safexcel_crypto_priv *priv = ctx->priv;
+	struct safexcel_ahash_export_state istate, ostate;
+	int ret, i;
+
+	ret = safexcel_hmac_setkey("safexcel-sha1", key, keylen, &istate, &ostate);
+	if (ret)
+		return ret;
 
-	/*
-	 * For EIP197 we need to Check if the ipad/opad were changed,
-	 * if yes, need to invalidate the context.
-	 */
 	if (priv->eip_type == EIP197 && ctx->base.ctxr) {
-		for (i = 0; i < ARRAY_SIZE(s0.state); i++) {
-			if (ctx->ipad[i] != le32_to_cpu(s0.state[i]) ||
-			    ctx->opad[i] != le32_to_cpu(s1.state[i])) {
+		for (i = 0; i < SHA1_DIGEST_SIZE / sizeof(u32); i++) {
+			if (ctx->ipad[i] != le32_to_cpu(istate.state[i]) ||
+			    ctx->opad[i] != le32_to_cpu(ostate.state[i])) {
 				ctx->base.needs_inv = true;
 				break;
 			}
 		}
 	}
 
-	for (i = 0; i < ARRAY_SIZE(s0.state); i++)
-		ctx->ipad[i] = le32_to_cpu(s0.state[i]);
-	for (i = 0; i < ARRAY_SIZE(s1.state); i++)
-		ctx->opad[i] = le32_to_cpu(s1.state[i]);
+	memcpy(ctx->ipad, &istate.state, SHA1_DIGEST_SIZE);
+	memcpy(ctx->opad, &ostate.state, SHA1_DIGEST_SIZE);
 
-free_req:
-	ahash_request_free(areq);
-free_ahash:
-	crypto_free_ahash(ahash);
-	return ret;
+	return 0;
 }
 
 struct safexcel_alg_template safexcel_alg_hmac_sha1 = {
@@ -950,11 +1028,11 @@ struct safexcel_alg_template safexcel_alg_hmac_sha1 = {
 		.finup = safexcel_ahash_finup,
 		.digest = safexcel_hmac_sha1_digest,
 		.setkey = safexcel_hmac_sha1_setkey,
-		.export = safexcel_sha1_export,
-		.import = safexcel_sha1_import,
+		.export = safexcel_ahash_export,
+		.import = safexcel_ahash_import,
 		.halg = {
 			.digestsize = SHA1_DIGEST_SIZE,
-			.statesize = sizeof(struct sha1_state),
+			.statesize = sizeof(struct safexcel_ahash_export_state),
 			.base = {
 				.cra_name = "hmac(sha1)",
 				.cra_driver_name = "safexcel-hmac-sha1",
@@ -1004,36 +1082,6 @@ static int safexcel_sha256_digest(struct ahash_request *areq)
 	return safexcel_ahash_finup(areq);
 }
 
-static int safexcel_sha256_export(struct ahash_request *areq, void *out)
-{
-	struct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);
-	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
-	struct sha256_state *sha256 = out;
-	int len = req->len;
-	int cache_len = do_div(len, crypto_ahash_blocksize(ahash));
-
-	sha256->count = req->len;
-	memcpy(sha256->state, req->state, req->state_sz);
-	memset(sha256->buf, 0, crypto_ahash_blocksize(ahash));
-	memcpy(sha256->buf, req->cache, cache_len);
-
-	return 0;
-}
-
-static int safexcel_sha256_import(struct ahash_request *areq, const void *in)
-{
-	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
-	const struct sha256_state *sha256 = in;
-
-	memset(req, 0, sizeof(*req));
-
-	req->len = sha256->count;
-	memcpy(req->cache, sha256->buf, sha256->count);
-	memcpy(req->state, sha256->state, req->state_sz);
-
-	return 0;
-}
-
 struct safexcel_alg_template safexcel_alg_sha256 = {
 	.type = SAFEXCEL_ALG_TYPE_AHASH,
 	.alg.ahash = {
@@ -1042,11 +1090,11 @@ struct safexcel_alg_template safexcel_alg_sha256 = {
 		.final = safexcel_ahash_final,
 		.finup = safexcel_ahash_finup,
 		.digest = safexcel_sha256_digest,
-		.export = safexcel_sha256_export,
-		.import = safexcel_sha256_import,
+		.export = safexcel_ahash_export,
+		.import = safexcel_ahash_import,
 		.halg = {
 			.digestsize = SHA256_DIGEST_SIZE,
-			.statesize = sizeof(struct sha256_state),
+			.statesize = sizeof(struct safexcel_ahash_export_state),
 			.base = {
 				.cra_name = "sha256",
 				.cra_driver_name = "safexcel-sha256",
@@ -1104,11 +1152,11 @@ struct safexcel_alg_template safexcel_alg_sha224 = {
 		.final = safexcel_ahash_final,
 		.finup = safexcel_ahash_finup,
 		.digest = safexcel_sha224_digest,
-		.export = safexcel_sha256_export,
-		.import = safexcel_sha256_import,
+		.export = safexcel_ahash_export,
+		.import = safexcel_ahash_import,
 		.halg = {
 			.digestsize = SHA224_DIGEST_SIZE,
-			.statesize = sizeof(struct sha256_state),
+			.statesize = sizeof(struct safexcel_ahash_export_state),
 			.base = {
 				.cra_name = "sha224",
 				.cra_driver_name = "safexcel-sha224",
diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
index 0cbfbc6..1b08f73 100644
--- a/drivers/crypto/inside-secure/safexcel.c
+++ b/drivers/crypto/inside-secure/safexcel.c
@@ -850,9 +850,11 @@ void safexcel_free_context(struct safexcel_crypto_priv *priv,
 		dma_unmap_single(priv->dev, ctx->result_dma, result_sz,
 				 DMA_FROM_DEVICE);
 
-	if (ctx->cache_dma) {
+	if (ctx->cache) {
 		dma_unmap_single(priv->dev, ctx->cache_dma, ctx->cache_sz,
 				 DMA_TO_DEVICE);
+		kfree(ctx->cache);
+		ctx->cache = NULL;
 		ctx->cache_sz = 0;
 	}
 }
diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
index 9f72fed..eeb4bce 100644
--- a/drivers/crypto/inside-secure/safexcel.h
+++ b/drivers/crypto/inside-secure/safexcel.h
@@ -665,6 +665,7 @@ struct safexcel_context {
 
 	/* Used for ahash requests */
 	dma_addr_t result_dma;
+	void *cache;
 	dma_addr_t cache_dma;
 	unsigned int cache_sz;
 };
-- 
1.7.9.5

