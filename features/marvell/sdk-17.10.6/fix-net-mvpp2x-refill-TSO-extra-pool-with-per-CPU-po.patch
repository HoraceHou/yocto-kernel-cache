From 181f9052749f8d27b406c24f3e54d2ff94fd72c9 Mon Sep 17 00:00:00 2001
From: Stefan Chulski <stefanc@marvell.com>
Date: Sun, 8 Jan 2017 17:10:20 +0200
Subject: [PATCH 0715/1345] fix: net: mvpp2x: refill TSO extra pool with per
 CPU pointer

commit  34c11afe24c2b3febeb6e7c2fbc1a52521f3ddaf from
https://github.com/MarvellEmbeddedProcessors/linux-marvell.git

Issue:
 - TSO extra buffer pool was refilled with 'this_cpu_pointer'. This is
   wrong behavior in case of CPU affinity.
   tx_done could be moved to another CPU.

Fix:
 - Use 'per_cpu_pointer' in TSO extra buffer pool refill.

Change-Id: Ib02745b0df818ef5f040104e5b99b876d7285e02
Signed-off-by: Stefan Chulski <stefanc@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/35382
Tested-by: iSoC Platform CI <ykjenk@marvell.com>
Reviewed-by: Omri Itach <omrii@marvell.com>
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c |   14 ++++++++------
 1 file changed, 8 insertions(+), 6 deletions(-)

diff --git a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
index cfcce68..f2ebb83 100644
--- a/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
+++ b/drivers/net/ethernet/marvell/mvpp2x/mv_pp2x_main.c
@@ -840,9 +840,10 @@ static inline void *mv_pp2_extra_pool_get(struct mv_pp2x_port *port)
 	return ext_buf;
 }
 
-static inline int mv_pp2_extra_pool_put(struct mv_pp2x_port *port, void *ext_buf)
+static inline int mv_pp2_extra_pool_put(struct mv_pp2x_port *port, void *ext_buf,
+					int cpu)
 {
-	struct mv_pp2x_port_pcpu *port_pcpu = this_cpu_ptr(port->pcpu);
+	struct mv_pp2x_port_pcpu *port_pcpu = per_cpu_ptr(port->pcpu, cpu);
 	struct mv_pp2x_ext_buf_struct *ext_buf_struct;
 
 	if (port_pcpu->ext_buf_pool->buf_pool_in_use >= port_pcpu->ext_buf_pool->buf_pool_size) {
@@ -969,7 +970,7 @@ static void mv_pp2x_txq_bufs_free(struct mv_pp2x_port *port,
 
 		if (skb & MVPP2_ETH_SHADOW_EXT) {
 			skb &= ~MVPP2_ETH_SHADOW_EXT;
-			mv_pp2_extra_pool_put(port, (void *)skb);
+			mv_pp2_extra_pool_put(port, (void *)skb, txq_pcpu->cpu);
 			mv_pp2x_txq_inc_get(txq_pcpu);
 			continue;
 		}
@@ -983,14 +984,15 @@ static void mv_pp2x_txq_bufs_free(struct mv_pp2x_port *port,
 }
 
 static void mv_pp2x_txq_buf_free(struct mv_pp2x_port *port, uintptr_t skb,
-				 dma_addr_t  buf_phys_addr, int data_size)
+				 dma_addr_t  buf_phys_addr, int data_size,
+				 int cpu)
 {
 	dma_unmap_single(port->dev->dev.parent, buf_phys_addr,
 			 data_size, DMA_TO_DEVICE);
 
 	if (skb & MVPP2_ETH_SHADOW_EXT) {
 		skb &= ~MVPP2_ETH_SHADOW_EXT;
-		mv_pp2_extra_pool_put(port, (void *)skb);
+		mv_pp2_extra_pool_put(port, (void *)skb, cpu);
 		return;
 	}
 
@@ -2853,7 +2855,7 @@ static inline int mv_pp2_tx_tso(struct sk_buff *skb, struct net_device *dev,
 		data_size = txq_pcpu->data_size[txq_pcpu->txq_get_index];
 
 		mv_pp2x_txq_buf_free(port, (uintptr_t)shadow_skb, shadow_buf,
-				     data_size);
+				     data_size, cpu);
 
 		mv_pp2x_txq_prev_desc_get(aggr_txq);
 	}
-- 
1.7.9.5

