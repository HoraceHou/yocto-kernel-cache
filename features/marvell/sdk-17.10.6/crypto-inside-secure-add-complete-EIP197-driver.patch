From a9a71747ad3b7a89ee3341bda1713c9057c3f4cf Mon Sep 17 00:00:00 2001
From: Igal Liberman <igall@marvell.com>
Date: Sun, 19 Mar 2017 11:48:38 +0200
Subject: [PATCH 0889/1345] crypto: inside-secure: add complete EIP197 driver

commit  7e85bbe2380e29b0adfde9a65e794386b7fe3073 from
https://github.com/MarvellEmbeddedProcessors/linux-marvell.git

Currently, the EIP197 performs only basic hardware initialization.
This patch adds the complete EIP197 driver, including run-time
support, ring management, descriptors operations and registration
to Kernel crypto API.

Hash and Cipher support will be added in a separate patches.

Change-Id: I34b339c04cfc5440f116cc51d1e2aca2248d5a6f
Signed-off-by: Igal Liberman <igall@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/37577
Tested-by: iSoC Platform CI <ykjenk@marvell.com>
Reviewed-by: Hanna Hawa <hannah@marvell.com>
Reviewed-by: Omri Itach <omrii@marvell.com>
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 drivers/crypto/inside-secure/Makefile   |    2 +-
 drivers/crypto/inside-secure/ring.c     |  178 ++++++++++
 drivers/crypto/inside-secure/safexcel.c |  552 ++++++++++++++++++++++++++++++-
 drivers/crypto/inside-secure/safexcel.h |  393 +++++++++++++++++++++-
 4 files changed, 1116 insertions(+), 9 deletions(-)
 create mode 100644 drivers/crypto/inside-secure/ring.c

diff --git a/drivers/crypto/inside-secure/Makefile b/drivers/crypto/inside-secure/Makefile
index 82265cb..7a3f1b1 100644
--- a/drivers/crypto/inside-secure/Makefile
+++ b/drivers/crypto/inside-secure/Makefile
@@ -1,2 +1,2 @@
 obj-$(CONFIG_CRYPTO_DEV_SAFEXCEL) += crypto_safexcel.o
-crypto_safexcel-objs := safexcel.o
+crypto_safexcel-objs := safexcel.o ring.o
diff --git a/drivers/crypto/inside-secure/ring.c b/drivers/crypto/inside-secure/ring.c
new file mode 100644
index 0000000..7d0d66c
--- /dev/null
+++ b/drivers/crypto/inside-secure/ring.c
@@ -0,0 +1,178 @@
+/*
+ * Copyright (C) 2016 Marvell
+ *
+ * Antoine Tenart <antoine.tenart@free-electrons.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/dma-mapping.h>
+#include <linux/spinlock.h>
+
+#include "safexcel.h"
+
+/* Initialize the ring descriptors */
+int safexcel_init_ring_descriptors(struct safexcel_crypto_priv *priv,
+				   struct safexcel_ring *cdr,
+				   struct safexcel_ring *rdr)
+{
+	cdr->offset = sizeof(u32) * priv->config.cd_offset;
+	cdr->base = dmam_alloc_coherent(priv->dev,
+					cdr->offset * EIP197_DEFAULT_RING_SIZE,
+					&cdr->base_dma, GFP_KERNEL);
+	if (!cdr->base)
+		return -ENOMEM;
+	cdr->write = cdr->base;
+	cdr->base_end = cdr->base + cdr->offset * EIP197_DEFAULT_RING_SIZE;
+	cdr->read = cdr->base;
+
+	rdr->offset = sizeof(u32) * priv->config.rd_offset;
+	rdr->base = dmam_alloc_coherent(priv->dev,
+					rdr->offset * EIP197_DEFAULT_RING_SIZE,
+					&rdr->base_dma, GFP_KERNEL);
+	if (!rdr->base) {
+		dmam_free_coherent(priv->dev,
+				   cdr->offset * EIP197_DEFAULT_RING_SIZE,
+				   cdr->base, cdr->base_dma);
+		return -ENOMEM;
+	}
+	rdr->write = rdr->base;
+	rdr->base_end = rdr->base + rdr->offset * EIP197_DEFAULT_RING_SIZE;
+	rdr->read = rdr->base;
+
+	return 0;
+}
+
+/* Free the ring descriptors */
+void safexcel_free_ring_descriptors(struct safexcel_crypto_priv *priv,
+				    struct safexcel_ring *cdr,
+				    struct safexcel_ring *rdr)
+{
+	dmam_free_coherent(priv->dev,
+			   cdr->offset * EIP197_DEFAULT_RING_SIZE,
+			   cdr->base, cdr->base_dma);
+	dmam_free_coherent(priv->dev,
+			   rdr->offset * EIP197_DEFAULT_RING_SIZE,
+			   rdr->base, rdr->base_dma);
+}
+
+/* Return the next available descriptor for use (command/result) */
+static void *safexcel_ring_next_wptr(struct safexcel_crypto_priv *priv,
+				     struct safexcel_ring *ring)
+{
+	void *ptr = ring->write;
+
+	if (ring->nr == EIP197_DEFAULT_RING_SIZE - 1)
+		return ERR_PTR(-ENOMEM);
+
+	ring->write += ring->offset;
+	if (ring->write == ring->base_end)
+		ring->write = ring->base;
+
+	ring->nr++;
+	return ptr;
+}
+
+/* Return the last used descriptor (command/result) */
+void *safexcel_ring_next_rptr(struct safexcel_crypto_priv *priv,
+			      struct safexcel_ring *ring)
+{
+	void *ptr = ring->read;
+
+	if (!ring->nr)
+		return ERR_PTR(-ENOENT);
+
+	ring->read += ring->offset;
+	if (ring->read == ring->base_end)
+		ring->read = ring->base;
+
+	ring->nr--;
+	return ptr;
+}
+
+/* Rollback descriptor allocation (in a case of insufficient resources) */
+void safexcel_ring_rollback_wptr(struct safexcel_crypto_priv *priv,
+				 struct safexcel_ring *ring)
+{
+	if (!ring->nr)
+		return;
+
+	if (ring->write == ring->base)
+		ring->write = ring->base_end - ring->offset;
+	else
+		ring->write -= ring->offset;
+
+	ring->nr--;
+}
+
+/* Create a command descriptor */
+struct safexcel_command_desc *safexcel_add_cdesc(struct safexcel_crypto_priv *priv,
+						 int ring_id,
+						 bool first, bool last,
+						 phys_addr_t data, u32 data_len,
+						 u32 full_data_len,
+						 phys_addr_t context) {
+	struct safexcel_command_desc *cdesc;
+	int i;
+
+	cdesc = safexcel_ring_next_wptr(priv, &priv->ring[ring_id].cdr);
+	if (IS_ERR(cdesc))
+		return cdesc;
+
+	memset(cdesc, 0, sizeof(struct safexcel_command_desc));
+
+	cdesc->first_seg = first;
+	cdesc->last_seg = last;
+	cdesc->particle_size = data_len;
+	cdesc->data_lo = lower_32_bits(data);
+	cdesc->data_hi = upper_32_bits(data);
+
+	if (first && context) {
+		struct safexcel_token *token =
+			(struct safexcel_token *)cdesc->control_data.token;
+
+		cdesc->control_data.packet_length = full_data_len;
+		cdesc->control_data.options = EIP197_OPTION_MAGIC_VALUE |
+					      EIP197_OPTION_64BIT_CTX |
+					      EIP197_OPTION_CTX_CTRL_IN_CMD;
+		cdesc->control_data.context_lo = (lower_32_bits(context) &
+						 EIP197_CONTEXT_POINTER_LO_MASK) >>
+						 EIP197_CONTEXT_POINTER_LO_SHIFT;
+		cdesc->control_data.context_hi = upper_32_bits(context);
+
+		/* TODO: large xform HMAC with SHA-384/512 uses
+		 * refresh = EIP197_CONTEXT_SIZE_LARGE (3)
+		 */
+		cdesc->control_data.refresh = EIP197_CONTEXT_SIZE_SMALL;
+
+		for (i = 0; i < EIP197_MAX_TOKENS; i++)
+			eip197_noop_token(&token[i]);
+	}
+
+	return cdesc;
+}
+
+/* Create a result descriptor */
+struct safexcel_result_desc *safexcel_add_rdesc(struct safexcel_crypto_priv *priv,
+						int ring_id,
+						bool first, bool last,
+						phys_addr_t data, u32 len)
+{
+	struct safexcel_result_desc *rdesc;
+
+	rdesc = safexcel_ring_next_wptr(priv, &priv->ring[ring_id].rdr);
+	if (IS_ERR(rdesc))
+		return rdesc;
+
+	memset(rdesc, 0, sizeof(struct safexcel_result_desc));
+
+	rdesc->first_seg = first;
+	rdesc->last_seg = last;
+	rdesc->particle_size = len;
+	rdesc->data_lo = lower_32_bits(data);
+	rdesc->data_hi = upper_32_bits(data);
+
+	return rdesc;
+}
diff --git a/drivers/crypto/inside-secure/safexcel.c b/drivers/crypto/inside-secure/safexcel.c
index d22c251..57c93c2 100644
--- a/drivers/crypto/inside-secure/safexcel.c
+++ b/drivers/crypto/inside-secure/safexcel.c
@@ -362,6 +362,91 @@ static void eip_priv_unit_offset_init(struct safexcel_crypto_priv *priv)
 	unit_off->pe = EIP197_HIA_PE_ADDR;
 }
 
+/* Configure the command descriptor ring manager */
+static int eip_hw_setup_cdesc_rings(struct safexcel_crypto_priv *priv)
+{
+	u32 hdw, cd_size_rnd, val;
+	int i;
+
+	hdw = readl(EIP197_HIA_AIC_G(priv) + EIP197_HIA_OPTIONS);
+	hdw = (hdw & EIP197_xDR_HDW_MASK) >> EIP197_xDR_HDW_OFFSET;
+
+	cd_size_rnd = (priv->config.cd_size + (BIT(hdw) - 1)) >> hdw;
+
+	for (i = 0; i < priv->config.rings; i++) {
+		/* ring base address */
+		writel(lower_32_bits(priv->ring[i].cdr.base_dma),
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_RING_BASE_ADDR_LO);
+		writel(upper_32_bits(priv->ring[i].cdr.base_dma),
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_RING_BASE_ADDR_HI);
+
+		writel(EIP197_xDR_DESC_MODE_64BIT |
+		       (priv->config.cd_offset << EIP197_xDR_DESC_CD_OFFSET) |
+		       priv->config.cd_size,
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_DESC_SIZE);
+		writel(((EIP197_FETCH_COUNT * (cd_size_rnd << hdw)) << EIP197_XDR_CD_FETCH_THRESH) |
+		       (EIP197_FETCH_COUNT * priv->config.cd_offset),
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_CFG);
+
+		/* Configure DMA tx control */
+		val = EIP197_HIA_xDR_CFG_WR_CACHE(WR_CACHE_3BITS);
+		val |= EIP197_HIA_xDR_CFG_RD_CACHE(RD_CACHE_3BITS);
+		writel(val, EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_DMA_CFG);
+
+		/* clear any pending interrupt */
+		writel(EIP197_CDR_INTR_MASK,
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_CDR(i) + EIP197_HIA_xDR_STAT);
+	}
+
+	return 0;
+}
+
+/* Configure the result descriptor ring manager */
+static int eip_hw_setup_rdesc_rings(struct safexcel_crypto_priv *priv)
+{
+	u32 hdw, rd_size_rnd, val;
+	int i;
+
+	hdw = readl(EIP197_HIA_AIC_G(priv) + EIP197_HIA_OPTIONS);
+	hdw = (hdw & EIP197_xDR_HDW_MASK) >> EIP197_xDR_HDW_OFFSET;
+
+	rd_size_rnd = (priv->config.rd_size + (BIT(hdw) - 1)) >> hdw;
+
+	for (i = 0; i < priv->config.rings; i++) {
+		/* ring base address */
+		writel(lower_32_bits(priv->ring[i].rdr.base_dma),
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_RING_BASE_ADDR_LO);
+		writel(upper_32_bits(priv->ring[i].rdr.base_dma),
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_RING_BASE_ADDR_HI);
+
+		writel(EIP197_xDR_DESC_MODE_64BIT |
+		       priv->config.rd_offset << EIP197_xDR_DESC_CD_OFFSET |
+		       priv->config.rd_size,
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_DESC_SIZE);
+
+		writel((EIP197_FETCH_COUNT * (rd_size_rnd << hdw)) << EIP197_XDR_CD_FETCH_THRESH |
+		       (EIP197_FETCH_COUNT * priv->config.rd_offset),
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_CFG);
+
+		/* Configure DMA tx control */
+		val = EIP197_HIA_xDR_CFG_WR_CACHE(WR_CACHE_3BITS);
+		val |= EIP197_HIA_xDR_CFG_RD_CACHE(RD_CACHE_3BITS);
+		val |= EIP197_HIA_xDR_WR_RES_BUF | EIP197_HIA_xDR_WR_CTRL_BUG;
+		writel(val, EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_DMA_CFG);
+
+		/* clear any pending interrupt */
+		writel(EIP197_RDR_INTR_MASK,
+		       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(i) + EIP197_HIA_xDR_STAT);
+
+		/* enable ring interrupt */
+		val = readl(EIP197_HIA_AIC_R(priv) + EIP197_HIA_AIC_R_ENABLE_CTRL(i));
+		val |= EIP197_RDR_IRQ(i);
+		writel(val, EIP197_HIA_AIC_R(priv) + EIP197_HIA_AIC_R_ENABLE_CTRL(i));
+	}
+
+	return 0;
+}
+
 static int eip_hw_init(struct device *dev, struct safexcel_crypto_priv *priv)
 {
 	u32 version, val;
@@ -534,15 +619,401 @@ static int eip_hw_init(struct device *dev, struct safexcel_crypto_priv *priv)
 		return ret;
 	}
 
+	eip_hw_setup_cdesc_rings(priv);
+	eip_hw_setup_rdesc_rings(priv);
+
 	return 0;
 }
 
-static void safexcel_configure(struct safexcel_crypto_priv *priv)
+/* Dequeue crypto API requests and send to the engine */
+void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring)
 {
+	struct crypto_async_request *req, *backlog;
+	struct safexcel_context *ctx;
+	struct safexcel_request *request;
+	int ret, nreq = 0;
+	int cdesc = 0, rdesc = 0;
+	int commands, results;
 	u32 val;
 
+	do {
+		spin_lock_bh(&priv->ring[ring].queue_lock);
+		req = crypto_dequeue_request(&priv->ring[ring].queue);
+		backlog = crypto_get_backlog(&priv->ring[ring].queue);
+		spin_unlock_bh(&priv->ring[ring].queue_lock);
+
+		if (!req)
+			goto finalize;
+
+		if (backlog)
+			backlog->complete(backlog, -EINPROGRESS);
+
+		request = kzalloc(sizeof(*request), EIP197_GFP_FLAGS(*req));
+		if (!request) {
+			ret = -ENOMEM;
+			goto resource_fail;
+		}
+
+		ctx = crypto_tfm_ctx(req->tfm);
+		ret = ctx->send(req, ring, request, &commands, &results);
+		if (ret) {
+			kfree(request);
+			goto resource_fail;
+		}
+
+		spin_lock_bh(&priv->ring[ring].egress_lock);
+		list_add_tail(&request->list, &priv->ring[ring].list);
+		spin_unlock_bh(&priv->ring[ring].egress_lock);
+
+		cdesc += commands;
+		rdesc += results;
+		nreq++;
+	} while (true);
+
+resource_fail:
+	/* resource alloc fail, bail out, complete the request and */
+	/* leave dequeue enabled since we have not cleaned it all  */
+	priv->ring[ring].need_dequeue = true;
+
+	local_bh_disable();
+	req->complete(req, ret);
+	local_bh_enable();
+
+finalize:
+	if (!nreq)
+		return;
+
+	spin_lock_bh(&priv->ring[ring].lock);
+
+	/* Configure when we want an interrupt */
+	val = EIP197_HIA_RDR_THRESH_PKT_MODE | EIP197_HIA_RDR_THRESH_PROC_PKT(nreq);
+	val |= EIP197_HIA_RDR_THRESH_TIMEOUT(0x80);
+	writel_relaxed(val, EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_THRESH);
+
+	/* let the RDR know we have pending descriptors */
+	writel_relaxed((rdesc * priv->config.rd_offset) << EIP197_xDR_PREP_RD_COUNT_INCR_OFFSET,
+	       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PREP_COUNT);
+
+	/* let the CDR know we have pending descriptors */
+	writel((cdesc * priv->config.cd_offset) << EIP197_xDR_PREP_RD_COUNT_INCR_OFFSET,
+	       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_CDR(ring) + EIP197_HIA_xDR_PREP_COUNT);
+
+	spin_unlock_bh(&priv->ring[ring].lock);
+}
+
+/* Select the ring which will be used for the operation */
+inline int safexcel_select_ring(struct safexcel_crypto_priv *priv)
+{
+	return (atomic_inc_return(&priv->ring_used) % priv->config.rings);
+}
+
+/* Default IRQ handler */
+static irqreturn_t safexcel_irq_default(int irq, void *data)
+{
+	struct safexcel_crypto_priv *priv = data;
+
+	dev_err(priv->dev, "Got an interrupt handled by the default handler.\n");
+
+	return IRQ_NONE;
+}
+
+/* Global IRQ handler */
+static irqreturn_t safexcel_irq_global(int irq, void *data)
+{
+	struct safexcel_crypto_priv *priv = data;
+	u32 status = readl(EIP197_HIA_AIC_G(priv) + EIP197_HIA_AIC_G_ENABLED_STAT);
+
+	writel(status, EIP197_HIA_AIC_G(priv) + EIP197_HIA_AIC_G_ACK);
+
+	return IRQ_HANDLED;
+}
+
+/* Free crypto API result mapping */
+void safexcel_free_context(struct safexcel_crypto_priv *priv,
+				  struct crypto_async_request *req,
+				  int result_sz)
+{
+	struct safexcel_context *ctx = crypto_tfm_ctx(req->tfm);
+
+	if (ctx->result_dma)
+		dma_unmap_single(priv->dev, ctx->result_dma, result_sz,
+				 DMA_FROM_DEVICE);
+
+	if (ctx->cache_dma) {
+		dma_unmap_single(priv->dev, ctx->cache_dma, ctx->cache_sz,
+				 DMA_TO_DEVICE);
+		ctx->cache_sz = 0;
+	}
+}
+
+/* Acknoledge and release the used descriptors */
+void safexcel_complete(struct safexcel_crypto_priv *priv, int ring)
+{
+	struct safexcel_command_desc *cdesc;
+
+	/* Acknowledge the command descriptors */
+	do {
+		cdesc = safexcel_ring_next_rptr(priv, &priv->ring[ring].cdr);
+		if (IS_ERR(cdesc)) {
+			dev_err(priv->dev,
+				"Could not retrieve the command descriptor\n");
+			return;
+		}
+	} while (!cdesc->last_seg);
+}
+
+/* Context completion cache invalidation */
+void safexcel_inv_complete(struct crypto_async_request *req, int error)
+{
+	struct safexcel_inv_result *result = req->data;
+
+	if (error == -EINPROGRESS)
+		return;
+
+	result->error = error;
+	complete(&result->completion);
+}
+
+/* Context cache invalidation */
+int safexcel_invalidate_cache(struct crypto_async_request *async,
+			      struct safexcel_context *ctx,
+			      struct safexcel_crypto_priv *priv,
+			      dma_addr_t ctxr_dma,
+			      int ring, struct safexcel_request *request)
+{
+	struct safexcel_command_desc *cdesc;
+	struct safexcel_result_desc *rdesc;
+	phys_addr_t ctxr_phys;
+	int ret;
+
+	ctxr_phys = dma_to_phys(priv->dev, ctxr_dma);
+
+	spin_lock_bh(&priv->ring[ring].egress_lock);
+
+	/* prepare command descriptor */
+	cdesc = safexcel_add_cdesc(priv, ring, true, true,
+				   0, 0, 0, ctxr_phys);
+
+	if (IS_ERR(cdesc)) {
+		ret = PTR_ERR(cdesc);
+		goto unlock;
+	}
+
+	cdesc->control_data.type = CONTEXT_CONTROL_TYPE_AUTONOMUS_TOKEN;
+	cdesc->control_data.options = 0;
+	cdesc->control_data.refresh = 0;
+	cdesc->control_data.control0 = CONTEXT_CONTROL_INV_TR <<
+				       CONTEXT_CONTROL_HW_SERVICES_OFFSET;
+
+	/* prepare result descriptor */
+	rdesc = safexcel_add_rdesc(priv, ring, true, true, ctxr_phys, 0);
+
+	if (IS_ERR(rdesc)) {
+		ret = PTR_ERR(rdesc);
+		goto cdesc_rollback;
+	}
+
+	request->req = async;
+
+	spin_unlock_bh(&priv->ring[ring].egress_lock);
+
+	return 0;
+
+cdesc_rollback:
+	safexcel_ring_rollback_wptr(priv, &priv->ring[ring].cdr);
+
+unlock:
+	spin_unlock_bh(&priv->ring[ring].egress_lock);
+	return ret;
+}
+
+/* Generic result handler */
+static inline void safexcel_handle_result_descriptor(struct safexcel_crypto_priv *priv,
+						     int ring)
+{
+	struct safexcel_request *sreq;
+	struct safexcel_context *ctx;
+	int ret, i, nreq, ndesc = 0;
+	bool should_complete;
+
+	nreq = readl(EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
+	nreq = (nreq >> EIP197_xDR_PROC_xD_PKT_OFFSET) & EIP197_xDR_PROC_xD_PKT_MASK;
+
+	if (!nreq)
+		return;
+
+	for (i = 0; i < nreq; i++) {
+		spin_lock_bh(&priv->ring[ring].egress_lock);
+		sreq = list_first_entry(&priv->ring[ring].list, struct safexcel_request, list);
+		list_del(&sreq->list);
+		spin_unlock_bh(&priv->ring[ring].egress_lock);
+
+		WARN_ON(!virt_addr_valid(sreq->req->tfm));
+
+		ctx = crypto_tfm_ctx(sreq->req->tfm);
+		ndesc = ctx->handle_result(priv, ring, sreq->req,
+					   &should_complete, &ret);
+		if (ndesc < 0) {
+			dev_err(priv->dev, "failed to handle result (%d)", ndesc);
+			return;
+		}
+
+		if (should_complete) {
+			local_bh_disable();
+			sreq->req->complete(sreq->req, ret);
+			local_bh_enable();
+		}
+
+		kfree(sreq);
+	}
+
+	writel(EIP197_HIA_RDR_THRESH_PKT_MODE | EIP197_xDR_PROC_xD_PKT(i),
+	       EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_PROC_COUNT);
+}
+
+/* Result worker routine */
+static void safexcel_handle_result_work(struct work_struct *work)
+{
+	struct safexcel_work_data *data = container_of(work, struct safexcel_work_data, work);
+	struct safexcel_crypto_priv *priv = data->priv;
+
+	safexcel_handle_result_descriptor(priv, data->ring);
+
+	if (priv->ring[data->ring].need_dequeue) {
+		priv->ring[data->ring].need_dequeue = false;
+		safexcel_dequeue(data->priv, data->ring);
+	}
+}
+
+struct safexcel_ring_irq_data {
+	struct safexcel_crypto_priv *priv;
+	int ring;
+};
+
+/* Ring IRQ handler */
+static irqreturn_t safexcel_irq_ring(int irq, void *data)
+{
+	struct safexcel_ring_irq_data *irq_data = data;
+	struct safexcel_crypto_priv *priv = irq_data->priv;
+	int ring = irq_data->ring;
+	u32 status, stat;
+
+	status = readl(EIP197_HIA_AIC_R(priv) + EIP197_HIA_AIC_R_ENABLED_STAT(ring));
+
+	if (!status)
+		return IRQ_NONE;
+
+	/* CDR interrupts */
+	if (status & EIP197_CDR_IRQ(ring)) {
+		stat = readl_relaxed(EIP197_HIA_AIC_xDR(priv) + EIP197_HIA_CDR(ring) + EIP197_HIA_xDR_STAT);
+
+		if (unlikely(stat & EIP197_xDR_ERR)) {
+			/*
+			 * Fatal error, the CDR is unusable and must be
+			 * reinitialized. This should not happen under
+			 * normal circumstances.
+			 */
+			dev_err(priv->dev, "CDR: fatal error.");
+		}
+
+		/* ACK the interrupts */
+		writel_relaxed(stat & 0xff, EIP197_HIA_AIC_xDR(priv) +
+			       EIP197_HIA_CDR(ring) + EIP197_HIA_xDR_STAT);
+	}
+
+	/* RDR interrupts */
+	if (status & EIP197_RDR_IRQ(ring)) {
+		stat = readl_relaxed(EIP197_HIA_AIC_xDR(priv) +
+				     EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_STAT);
+
+		if (unlikely(stat & EIP197_xDR_ERR)) {
+			/*
+			 * Fatal error, the CDR is unusable and must be
+			 * reinitialized. This should not happen under
+			 * normal circumstances.
+			 */
+			dev_err(priv->dev, "RDR: fatal error.");
+		}
+
+		if (likely(stat & EIP197_xDR_THRESH)) {
+			writel_relaxed(0, EIP197_HIA_AIC_xDR(priv) +
+				       EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_THRESH);
+			queue_work(priv->ring[ring].workqueue, &priv->ring[ring].work_data.work);
+		} else if (unlikely(stat & EIP197_xDR_TIMEOUT)) {
+			queue_work(priv->ring[ring].workqueue, &priv->ring[ring].work_data.work);
+		}
+
+		/* ACK the interrupts */
+		writel_relaxed(stat & 0xff, EIP197_HIA_AIC_xDR(priv) +
+			       EIP197_HIA_RDR(ring) + EIP197_HIA_xDR_STAT);
+	}
+
+	/* ACK the interrupts */
+	writel(status, EIP197_HIA_AIC_R(priv) + EIP197_HIA_AIC_R_ACK(ring));
+
+	return IRQ_HANDLED;
+}
+
+/* Register gloabl interrupts */
+static int safexcel_request_irq(struct platform_device *pdev, const char *name,
+				irq_handler_t handler,
+				struct safexcel_crypto_priv *priv)
+{
+	int ret, irq = platform_get_irq_byname(pdev, name);
+
+	if (irq < 0) {
+		dev_err(&pdev->dev, "unable to get IRQ '%s'\n", name);
+		return irq;
+	}
+
+	ret = devm_request_irq(&pdev->dev, irq, handler, 0,
+			       dev_name(&pdev->dev), priv);
+	if (ret) {
+		dev_err(&pdev->dev, "unable to request IRQ %d\n", irq);
+		return ret;
+	}
+
+	return irq;
+}
+
+/* Register ring interrupts */
+static int safexcel_request_ring_irq(struct platform_device *pdev, const char *name,
+				     irq_handler_t handler,
+				     struct safexcel_ring_irq_data *ring_irq_priv)
+{
+	int ret, irq = platform_get_irq_byname(pdev, name);
+
+	if (irq < 0) {
+		dev_err(&pdev->dev, "unable to get IRQ '%s'\n", name);
+		return irq;
+	}
+
+	ret = devm_request_irq(&pdev->dev, irq, handler, 0,
+			       dev_name(&pdev->dev), ring_irq_priv);
+	if (ret) {
+		dev_err(&pdev->dev, "unable to request IRQ %d\n", irq);
+		return ret;
+	}
+
+	return irq;
+}
+
+static void safexcel_configure(struct safexcel_crypto_priv *priv)
+{
+	u32 val, mask;
+
+	val = readl(EIP197_HIA_AIC_G(priv) + EIP197_HIA_OPTIONS);
+	val = (val & EIP197_xDR_HDW_MASK) >> EIP197_xDR_HDW_OFFSET;
+	mask = BIT(val) - 1;
+
 	val = readl(EIP197_HIA_AIC_G(priv) + EIP197_HIA_OPTIONS);
 	priv->config.rings = (val & GENMASK(3, 0));
+
+	priv->config.cd_size = (sizeof(struct safexcel_command_desc) / sizeof(u32));
+	priv->config.cd_offset = (priv->config.cd_size + mask) & ~mask;
+
+	priv->config.rd_size = (sizeof(struct safexcel_result_desc) / sizeof(u32));
+	priv->config.rd_offset = (priv->config.rd_size + mask) & ~mask;
 }
 
 static int safexcel_probe(struct platform_device *pdev)
@@ -550,7 +1021,7 @@ static int safexcel_probe(struct platform_device *pdev)
 	struct device *dev = &pdev->dev;
 	struct resource *res;
 	struct safexcel_crypto_priv *priv;
-	int ret;
+	int i, ret;
 
 	priv = devm_kzalloc(dev, sizeof(struct safexcel_crypto_priv),
 			    GFP_KERNEL);
@@ -582,20 +1053,84 @@ static int safexcel_probe(struct platform_device *pdev)
 			return -EPROBE_DEFER;
 	}
 
+	priv->context_pool = dmam_pool_create("safexcel-context", dev,
+					      sizeof(struct safexcel_context_record),
+					      1, 0);
+	if (!priv->context_pool) {
+		ret = -ENOMEM;
+		goto err_clk;
+	}
+
 	safexcel_configure(priv);
 
-	platform_set_drvdata(pdev, priv);
+	ret = safexcel_request_irq(pdev, "eip_out", safexcel_irq_global, priv);
+	if (ret < 0)
+		goto err_pool;
+
+	ret = safexcel_request_irq(pdev, "eip_addr", safexcel_irq_default, priv);
+	if (ret < 0)
+		goto err_pool;
+
+	for (i = 0; i < priv->config.rings; i++) {
+		char irq_name[6] = {0}; /* "ringX\0" */
+		char wq_name[9] = {0}; /* "wq_ringX\0" */
+		int irq;
+		struct safexcel_ring_irq_data *ring_irq;
+
+		ret = safexcel_init_ring_descriptors(priv,
+						     &priv->ring[i].cdr,
+						     &priv->ring[i].rdr);
+		if (ret)
+			goto err_pool;
+
+		ring_irq = devm_kzalloc(dev, sizeof(struct safexcel_ring_irq_data),
+					GFP_KERNEL);
+		if (!ring_irq) {
+			ret = -ENOMEM;
+			goto err_pool;
+		}
+
+		ring_irq->priv = priv;
+		ring_irq->ring = i;
+
+		snprintf(irq_name, 6, "ring%d", i);
+		irq = safexcel_request_ring_irq(pdev, irq_name, safexcel_irq_ring,
+						ring_irq);
+
+		if (irq < 0)
+			goto err_pool;
 
-	spin_lock_init(&priv->lock);
+		priv->ring[i].work_data.priv = priv;
+		priv->ring[i].work_data.ring = i;
+		INIT_WORK(&priv->ring[i].work_data.work, safexcel_handle_result_work);
+
+		snprintf(wq_name, 9, "wq_ring%d", i);
+		priv->ring[i].workqueue = create_singlethread_workqueue(wq_name);
+		if (!priv->ring[i].workqueue) {
+			ret = -ENOMEM;
+			goto err_pool;
+		}
+
+		INIT_LIST_HEAD(&priv->ring[i].list);
+		spin_lock_init(&priv->ring[i].lock);
+		spin_lock_init(&priv->ring[i].egress_lock);
+		spin_lock_init(&priv->ring[i].queue_lock);
+		crypto_init_queue(&priv->ring[i].queue, EIP197_DEFAULT_RING_SIZE);
+	}
+	atomic_set(&priv->ring_used, 0);
+
+	platform_set_drvdata(pdev, priv);
 
 	ret = eip_hw_init(dev, priv);
 	if (ret) {
 		dev_err(dev, "EIP h/w init failed (%d)\n", ret);
-		goto err_clk;
+		goto err_pool;
 	}
 
 	return 0;
 
+err_pool:
+	dma_pool_destroy(priv->context_pool);
 err_clk:
 	clk_disable_unprepare(priv->clk);
 	return ret;
@@ -605,9 +1140,16 @@ static int safexcel_probe(struct platform_device *pdev)
 static int safexcel_remove(struct platform_device *pdev)
 {
 	struct safexcel_crypto_priv *priv = platform_get_drvdata(pdev);
+	int i;
 
 	clk_disable_unprepare(priv->clk);
 
+	for (i = 0; i < priv->config.rings; i++) {
+		safexcel_free_ring_descriptors(priv, &priv->ring[i].cdr,
+					       &priv->ring[i].rdr);
+		destroy_workqueue(priv->ring[i].workqueue);
+	}
+
 	return 0;
 }
 
diff --git a/drivers/crypto/inside-secure/safexcel.h b/drivers/crypto/inside-secure/safexcel.h
index b484cef..b493f9f 100644
--- a/drivers/crypto/inside-secure/safexcel.h
+++ b/drivers/crypto/inside-secure/safexcel.h
@@ -26,7 +26,15 @@
 #define EIP197_HIA_VERSION_BE				0x35ca
 
 /* Static configuration */
-#define EIP197_DEFAULT_RING_SIZE			512
+#define EIP197_DEFAULT_RING_SIZE			300
+#define EIP197_MAX_TOKENS				5
+#define EIP197_MAX_RINGS				4
+
+#define EIP197_FETCH_COUNT				1
+#define EIP197_MAX_BATCH_SZ				8
+
+#define EIP197_GFP_FLAGS(base)	((base).flags & CRYPTO_TFM_REQ_MAY_SLEEP ? \
+				 GFP_KERNEL : GFP_ATOMIC)
 
 /* READ and WRITE cache control */
 #define RD_CACHE_3BITS				0x5
@@ -53,11 +61,17 @@
 #define EIP197_TRC_RAM_WC				3840
 
 /* Transformation Record Cache address */
- #define EIP197_TRC_PARAMS				0xf0820
+#define EIP197_TRC_CTRL					0xf0800
+#define EIP197_TRC_LASTRES				0xf0804
+#define EIP197_TRC_REGINDEX				0xf0808
+#define EIP197_TRC_PARAMS				0xf0820
 #define EIP197_TRC_FREECHAIN				0xf0824
 #define EIP197_TRC_PARAMS2				0xf0828
 #define EIP197_TRC_ECCCTRL				0xf0830
 #define EIP197_TRC_ECCSTAT				0xf0834
+#define EIP197_TRC_ECCADMINSTAT				0xf0838
+#define EIP197_TRC_ECCDATASTAT				0xf083c
+#define EIP197_TRC_ECCDATA				0xf0840
 
 /* Classification regs */
 #define EIP197_CS_RAM_CTRL				0xf7ff0
@@ -69,6 +83,7 @@
 
 /* EIP-96 PRNG */
 /* Registers   */
+#define EIP197_PE_EIP96_PRNG_STAT			0x01040
 #define EIP197_PE_EIP96_PRNG_CTRL			0x01044
 #define EIP197_PE_EIP96_PRNG_SEED_L			0x01048
 #define EIP197_PE_EIP96_PRNG_SEED_H			0x0104c
@@ -126,23 +141,69 @@
 #define EIP197_HIA_xDR_OFF(r)				((r) * 0x1000)
 #define EIP197_HIA_CDR(r)				(EIP197_HIA_xDR_OFF(r))
 #define EIP197_HIA_RDR(r)				(0x800 + EIP197_HIA_xDR_OFF(r))
+#define EIP197_HIA_xDR_RING_BASE_ADDR_LO		0x0
+#define EIP197_HIA_xDR_RING_BASE_ADDR_HI		0x4
 #define EIP197_HIA_xDR_RING_SIZE			0x18
+#define EIP197_HIA_xDR_DESC_SIZE			0x1c
 #define EIP197_HIA_xDR_CFG				0x20
+#define EIP197_HIA_xDR_DMA_CFG				0x24
+#define EIP197_HIA_xDR_THRESH				0x28
 #define EIP197_HIA_xDR_PREP_COUNT			0x2c
 #define EIP197_HIA_xDR_PROC_COUNT			0x30
 #define EIP197_HIA_xDR_PREP_PNTR			0x34
 #define EIP197_HIA_xDR_PROC_PNTR			0x38
+#define EIP197_HIA_xDR_STAT				0x3c
+
+/* EIP197_HIA_xDR_DESC_SIZE */
+#define EIP197_xDR_DESC_MODE_64BIT			BIT(31)
+#define EIP197_xDR_DESC_CD_OFFSET			16
+
+/* EIP197_DIA_xDR_CFG */
+#define EIP197_XDR_CD_FETCH_THRESH			16
+
+/* EIP197_HIA_xDR_DMA_CFG */
+#define EIP197_HIA_xDR_WR_RES_BUF			BIT(22)
+#define EIP197_HIA_xDR_WR_CTRL_BUG			BIT(23)
+#define EIP197_HIA_xDR_WR_OWN_BUF			BIT(24)
+#define EIP197_HIA_xDR_CFG_WR_CACHE(n)			(((n) & 0x7) << 23)
+#define EIP197_HIA_xDR_CFG_RD_CACHE(n)			(((n) & 0x7) << 29)
+
+/* EIP197_HIA_CDR_THRESH */
+#define EIP197_HIA_CDR_THRESH_PROC_PKT(n)		((n) << 0)
+#define EIP197_HIA_CDR_THRESH_PROC_MODE			BIT(22)
+#define EIP197_HIA_CDR_THRESH_PKT_MODE			BIT(23)
+#define EIP197_HIA_CDR_THRESH_TIMEOUT(n)		((n) << 24) /* x256 clk cycles */
+
+/* EIP197_HIA_RDR_THRESH */
+#define EIP197_HIA_RDR_THRESH_PROC_PKT(n)		((n) << 0)
+#define EIP197_HIA_RDR_THRESH_PKT_MODE			BIT(23)
+#define EIP197_HIA_RDR_THRESH_TIMEOUT(n)		((n) << 24) /* x256 clk cycles */
 
 /* EIP197_HIA_xDR_PREP_COUNT */
 #define EIP197_xDR_PREP_CLR_COUNT			BIT(31)
+#define EIP197_xDR_PREP_RD_COUNT_INCR_OFFSET		2
 
 /* EIP197_HIA_xDR_PROC_COUNT */
+#define EIP197_xDR_PROC_xD_PKT_OFFSET			24
+#define EIP197_xDR_PROC_xD_PKT_MASK			(GENMASK(6, 0))
+#define EIP197_xDR_PROC_xD_COUNT(n)			((n) << 2)
+#define EIP197_xDR_PROC_xD_PKT(n)			((n) << EIP197_xDR_PROC_xD_PKT_OFFSET)
 #define EIP197_xDR_PROC_CLR_COUNT			BIT(31)
 
+/* EIP197_HIA_xDR_STAT */
+#define EIP197_xDR_DMA_ERR				BIT(0)
+#define EIP197_xDR_PREP_CMD_THRES			BIT(1)
+#define EIP197_xDR_ERR					BIT(2)
+#define EIP197_xDR_THRESH				BIT(4)
+#define EIP197_xDR_TIMEOUT				BIT(5)
+#define EIP197_CDR_INTR_MASK				(GENMASK(5, 0))
+#define EIP197_RDR_INTR_MASK				(GENMASK(7, 0))
+
 #define EIP197_HIA_RA_PE_CTRL_RESET			BIT(31)
 #define EIP197_HIA_RA_PE_CTRL_EN			BIT(30)
 
 /* Register offsets */
+
 /* unit offsets */
 #define EIP197_HIA_AIC_ADDR				0x90000
 #define EIP197_HIA_AIC_G_ADDR				0x90000
@@ -157,27 +218,44 @@
 #define EIP197_HIA_GC					0xf0000
 
 #define EIP197_HIA_AIC_R_OFF(r)			((r) * 0x1000)
+#define EIP197_HIA_AIC_R_ENABLE_CTRL(r)		(0xe008 - EIP197_HIA_AIC_R_OFF(r))
+#define EIP197_HIA_AIC_R_ENABLED_STAT(r)	(0xe010 - EIP197_HIA_AIC_R_OFF(r))
+#define EIP197_HIA_AIC_R_ACK(r)			(0xe010 - EIP197_HIA_AIC_R_OFF(r))
 #define EIP197_HIA_AIC_R_ENABLE_CLR(r)		(0xe014 - EIP197_HIA_AIC_R_OFF(r))
 
 #define EIP197_HIA_RA_PE_CTRL			0x010
 
 #define EIP197_HIA_DFE_CFG			0x000
 #define EIP197_HIA_DFE_THR_CTRL			0x000
+#define EIP197_HIA_DFE_THR_STAT			0x004
 
 #define EIP197_HIA_DSE_CFG			0x000
 #define EIP197_HIA_DSE_THR_CTRL			0x000
 #define EIP197_HIA_DSE_THR_STAT			0x004
 
 #define EIP197_HIA_AIC_G_ENABLE_CTRL		0xf808
+#define EIP197_HIA_AIC_G_ENABLED_STAT		0xf810
 #define EIP197_HIA_AIC_G_ACK			0xf810
 #define EIP197_HIA_MST_CTRL			0xfff4
 #define EIP197_HIA_OPTIONS			0xfff8
 #define EIP197_HIA_VERSION			0xfffc
 #define EIP197_PE_IN_DBUF_THRES			0x0000
 #define EIP197_PE_IN_TBUF_THRES			0x0100
+#define EIP197_FUNCTION_EN			0x1004
+#define EIP197_CONTEXT_CTRL			0x11008
 #define EIP197_PE_OUT_DBUF_THRES		0x1c00
+#define EIP197_OPTIONS				0x1fff8
+#define EIP197_IP_VERSION			0x1fffc
 #define EIP197_MST_CTRL				0xfff4
 
+/* EIP197_HIA_OPTIONS */
+#define EIP197_xDR_HDW_OFFSET			25
+#define EIP197_xDR_HDW_MASK			(GENMASK(27, 25))
+
+/* EIP197_HIA_AIC_R_ENABLE_CTRL */
+#define EIP197_CDR_IRQ(n)			BIT((n) * 2)
+#define EIP197_RDR_IRQ(n)			BIT((n) * 2 + 1)
+
 /* EIP197_HIA_DFE/DSE_CFG */
 #define EIP197_HIA_DxE_CFG_MIN_DATA_SIZE(n)	((n) << 0)
 #define EIP197_HIA_DxE_CFG_DATA_CACHE_CTRL(n)	(((n) & 0x7) << 4)
@@ -192,6 +270,12 @@
 #define EIP197_DxE_THR_CTRL_EN			BIT(30)
 #define EIP197_DxE_THR_CTRL_RESET_PE		BIT(31)
 
+/* EIP197_HIA_AIC_G_ENABLED_STAT */
+#define EIP197_G_IRQ_DFE(n)			BIT((n) << 1)
+#define EIP197_G_IRQ_DSE(n)			BIT(((n) << 1) + 1)
+#define EIP197_G_IRQ_RING			BIT(16)
+#define EIP197_G_IRQ_PE(n)			BIT((n) + 20)
+
 /* EIP197_HIA_MST_CTRL */
 #define EIP197_HIA_SLAVE_BYTE_SWAP			BIT(24)
 #define EIP197_HIA_SLAVE_NO_BYTE_SWAP		BIT(25)
@@ -204,9 +288,210 @@
 #define EIP197_PE_OUT_DBUF_THRES_MIN(n)		((n) << 0)
 #define EIP197_PE_OUT_DBUF_THRES_MAX(n)		((n) << 4)
 
+/* Remove */
+/* EIP197_FUNCTION_EN */
+#define EIP197_FUNCTION_RSVD			(BIT(6) | BIT(15) | BIT(20) | BIT(23))
+#define EIP197_PROTOCOL_HASH_ONLY		BIT(0)
+#define EIP197_PROTOCOL_ENCRYPT_ONLY		BIT(1)
+#define EIP197_PROTOCOL_HASH_ENCRYPT		BIT(2)
+#define EIP197_PROTOCOL_HASH_DECRYPT		BIT(3)
+#define EIP197_PROTOCOL_ENCRYPT_HASH		BIT(4)
+#define EIP197_PROTOCOL_DECRYPT_HASH		BIT(5)
+#define EIP197_ALG_ARC4				BIT(7)
+#define EIP197_ALG_AES_ECB			BIT(8)
+#define EIP197_ALG_AES_CBC			BIT(9)
+#define EIP197_ALG_AES_CTR_ICM			BIT(10)
+#define EIP197_ALG_AES_OFB			BIT(11)
+#define EIP197_ALG_AES_CFB			BIT(12)
+#define EIP197_ALG_DES_ECB			BIT(13)
+#define EIP197_ALG_DES_CBC			BIT(14)
+#define EIP197_ALG_DES_OFB			BIT(16)
+#define EIP197_ALG_DES_CFB			BIT(17)
+#define EIP197_ALG_3DES_ECB			BIT(18)
+#define EIP197_ALG_3DES_CBC			BIT(19)
+#define EIP197_ALG_3DES_OFB			BIT(21)
+#define EIP197_ALG_3DES_CFB			BIT(22)
+#define EIP197_ALG_MD5				BIT(24)
+#define EIP197_ALG_HMAC_MD5			BIT(25)
+#define EIP197_ALG_SHA1				BIT(26)
+#define EIP197_ALG_HMAC_SHA1			BIT(27)
+#define EIP197_ALG_SHA2				BIT(28)
+#define EIP197_ALG_HMAC_SHA2			BIT(29)
+#define EIP197_ALG_AES_XCBC_MAC			BIT(30)
+#define EIP197_ALG_GCM_HASH			BIT(31)
+
+/* EIP197_CONTEXT_CTRL */
+#define EIP197_CONTEXT_SIZE(n)			(n)
+#define EIP197_ADDRESS_MODE			BIT(8)
+#define EIP197_CONTROL_MODE			BIT(9)
+
+/* Context Control */
+struct safexcel_context_record {
+	u32 control0;
+	u32 control1;
+
+	__le32 data[12];
+} __packed;
+
+/* control0 */
+#define CONTEXT_CONTROL_TYPE_NULL_OUT		0
+#define CONTEXT_CONTROL_TYPE_NULL_IN		0x1
+#define CONTEXT_CONTROL_TYPE_HASH_OUT		0x2
+#define CONTEXT_CONTROL_TYPE_HASH_IN		0x3
+#define CONTEXT_CONTROL_TYPE_CRYPTO_OUT		0x4
+#define CONTEXT_CONTROL_TYPE_CRYPTO_IN		0x5
+#define CONTEXT_CONTROL_TYPE_ENCRYPT_HASH_OUT	0x6
+#define CONTEXT_CONTROL_TYPE_DECRYPT_HASH_IN	0x7
+#define CONTEXT_CONTROL_TYPE_HASH_ENCRYPT_OUT	0x14
+#define CONTEXT_CONTROL_TYPE_HASH_DECRYPT_OUT	0x15
+#define CONTEXT_CONTROL_RESTART_HASH		BIT(4)
+#define CONTEXT_CONTROL_NO_FINISH_HASH		BIT(5)
+#define CONTEXT_CONTROL_SIZE(n)			((n) << 8)
+#define CONTEXT_CONTROL_KEY_EN			BIT(16)
+#define CONTEXT_CONTROL_CRYPTO_ALG_AES128	(0x5 << 17)
+#define CONTEXT_CONTROL_CRYPTO_ALG_AES192	(0x6 << 17)
+#define CONTEXT_CONTROL_CRYPTO_ALG_AES256	(0x7 << 17)
+#define CONTEXT_CONTROL_DIGEST_PRECOMPUTED	(0x1 << 21)
+#define CONTEXT_CONTROL_DIGEST_HMAC		(0x3 << 21)
+#define CONTEXT_CONTROL_CRYPTO_ALG_SHA1		(0x2 << 23)
+#define CONTEXT_CONTROL_CRYPTO_ALG_SHA224	(0x4 << 23)
+#define CONTEXT_CONTROL_CRYPTO_ALG_SHA256	(0x3 << 23)
+
+#define CONTEXT_CONTROL_TYPE_AUTONOMUS_TOKEN	3
+#define CONTEXT_CONTROL_HW_SERVICES_OFFSET	24
+#define CONTEXT_CONTROL_INV_TR			0x6
+
+/* control1 */
+#define CONTEXT_CONTROL_CRYPTO_MODE_ECB		(0 << 0)
+#define CONTEXT_CONTROL_CRYPTO_MODE_CBC		(1 << 0)
+#define CONTEXT_CONTROL_IV0			BIT(5)
+#define CONTEXT_CONTROL_IV1			BIT(6)
+#define CONTEXT_CONTROL_IV2			BIT(7)
+#define CONTEXT_CONTROL_IV3			BIT(8)
+#define CONTEXT_CONTROL_DIGEST_CNT		BIT(9)
+#define CONTEXT_CONTROL_COUNTER_MODE		BIT(10)
+#define CONTEXT_CONTROL_HASH_STORE		BIT(19)
+
+
+/* Result data */
+struct result_data_desc {
+	u32 packet_length:17;
+	u32 error_code:15;
+
+	u8 bypass_length:4;
+	u8 e15:1;
+	u16 rsvd0;
+	u8 hash_bytes:1;
+	u8 hash_length:6;
+	u8 generic_bytes:1;
+	u8 checksum:1;
+	u8 next_header:1;
+	u8 length:1;
+
+	u16 application_id;
+	u16 rsvd1;
+
+	u32 rsvd2;
+} __packed;
+
+
+/* Basic Result Descriptor format */
+struct safexcel_result_desc {
+	u32 particle_size:17;
+	u8 rsvd0:3;
+	u8 descriptor_overflow:1;
+	u8 buffer_overflow:1;
+	u8 last_seg:1;
+	u8 first_seg:1;
+	u16 result_size:8;
+
+	u32 rsvd1;
+
+	u32 data_lo;
+	u32 data_hi;
+
+	struct result_data_desc result_data;
+} __packed;
+
+struct safexcel_token {
+	u32 packet_length:17;
+	u8 stat:2;
+	u16 instructions:9;
+	u8 opcode:4;
+} __packed;
+
+#define EIP197_TOKEN_STAT_LAST_HASH		BIT(0)
+#define EIP197_TOKEN_STAT_LAST_PACKET		BIT(1)
+#define EIP197_TOKEN_OPCODE_DIRECTION		0x0
+#define EIP197_TOKEN_OPCODE_INSERT		0x2
+#define EIP197_TOKEN_OPCODE_NOOP		EIP197_TOKEN_OPCODE_INSERT
+#define EIP197_TOKEN_OPCODE_BYPASS		GENMASK(3, 0)
+
+static inline void eip197_noop_token(struct safexcel_token *token)
+{
+	token->opcode = EIP197_TOKEN_OPCODE_NOOP;
+	token->packet_length = BIT(2);
+}
+
+/* Instructions */
+#define EIP197_TOKEN_INS_INSERT_HASH_DIGEST	0x1c
+#define EIP197_TOKEN_INS_TYPE_OUTPUT		BIT(5)
+#define EIP197_TOKEN_INS_TYPE_HASH		BIT(6)
+#define EIP197_TOKEN_INS_TYPE_CRYTO		BIT(7)
+#define EIP197_TOKEN_INS_LAST			BIT(8)
+
+/* Context size */
+#define EIP197_CONTEXT_SIZE_SMALL		2
+#define EIP197_CONTEXT_SIZE_LARGE		3
+
+/* Context LO pointer */
+#define EIP197_CONTEXT_POINTER_LO_MASK		(GENMASK(31, 2))
+#define EIP197_CONTEXT_POINTER_LO_SHIFT		2
+
+/* Processing Engine Control Data  */
+struct safexcel_control_data_desc {
+	u32 packet_length:17;
+	u16 options:13;
+	u8 type:2;
+
+	u16 application_id;
+	u16 rsvd;
+
+	u8 refresh:2;
+	u32 context_lo:30;
+	u32 context_hi;
+
+	u32 control0;
+	u32 control1;
+
+	u32 token[EIP197_MAX_TOKENS];
+} __packed;
+
+#define EIP197_OPTION_MAGIC_VALUE	BIT(0)
+#define EIP197_OPTION_64BIT_CTX		BIT(1)
+#define EIP197_OPTION_CTX_CTRL_IN_CMD	BIT(8)
+#define EIP197_OPTION_4_TOKEN_IV_CMD	(GENMASK(11, 9))
+
+/* Basic Command Descriptor format */
+struct safexcel_command_desc {
+	u32 particle_size:17;
+	u8 rsvd0:5;
+	u8 last_seg:1;
+	u8 first_seg:1;
+	u16 additional_cdata_size:8;
+
+	u32 rsvd1;
+
+	u32 data_lo;
+	u32 data_hi;
+
+	struct safexcel_control_data_desc control_data;
+} __packed;
+
 /*
  * Internal structures & functions
  */
+
 enum eip197_fw {
 	IFPP_FW = 0,
 	IPUE_FW,
@@ -217,6 +502,25 @@ enum safexcel_eip_type {
 	EIP197,
 };
 
+struct safexcel_ring {
+	void *base;
+	void *base_end;
+	dma_addr_t base_dma;
+
+	/* write and read pointers */
+	void *write;
+	void *read;
+
+	/* number of elements used in the ring */
+	unsigned nr;
+	unsigned offset;
+};
+
+struct safexcel_request {
+	struct list_head list;
+	struct crypto_async_request *req;
+};
+
 /* internal unit register offset */
 struct safexcel_unit_offset {
 	u32 hia_aic;
@@ -253,6 +557,12 @@ struct safexcel_config {
 	u32 rd_offset;
 };
 
+struct safexcel_work_data {
+	struct work_struct work;
+	struct safexcel_crypto_priv *priv;
+	int ring;
+};
+
 struct safexcel_crypto_priv {
 	void __iomem *base;
 	struct safexcel_unit_offset unit_off;
@@ -261,7 +571,84 @@ struct safexcel_crypto_priv {
 	enum safexcel_eip_type eip_type;
 	struct safexcel_config config;
 
-	spinlock_t lock;
+	/* context DMA pool */
+	struct dma_pool *context_pool;
+
+	atomic_t ring_used;
+
+	struct {
+		spinlock_t lock;
+		spinlock_t egress_lock;
+
+		struct list_head list;
+		struct workqueue_struct *workqueue;
+		struct safexcel_work_data work_data;
+
+		/* command/result rings */
+		struct safexcel_ring cdr;
+		struct safexcel_ring rdr;
+
+		spinlock_t queue_lock;
+		struct crypto_queue queue;
+		bool need_dequeue;
+	} ring[EIP197_MAX_RINGS];
 };
 
+struct safexcel_context {
+	int (*send)(struct crypto_async_request *req, int ring,
+		    struct safexcel_request *request, int *commands,
+		    int *results);
+	int (*handle_result)(struct safexcel_crypto_priv *priv, int ring,
+			     struct crypto_async_request *req, bool *complete,
+			     int *ret);
+	struct safexcel_context_record *ctxr;
+	dma_addr_t ctxr_dma;
+
+	int ring;
+	bool needs_inv;
+	bool exit_inv;
+
+	/* Used for ahash requests */
+	dma_addr_t result_dma;
+	dma_addr_t cache_dma;
+	unsigned int cache_sz;
+};
+struct safexcel_inv_result {
+	struct completion completion;
+	int error;
+};
+
+void safexcel_dequeue(struct safexcel_crypto_priv *priv, int ring);
+void safexcel_complete(struct safexcel_crypto_priv *priv, int ring);
+void safexcel_free_context(struct safexcel_crypto_priv *priv,
+				  struct crypto_async_request *req,
+				  int result_sz);
+int safexcel_invalidate_cache(struct crypto_async_request *async,
+			      struct safexcel_context *ctx,
+			      struct safexcel_crypto_priv *priv,
+			      dma_addr_t ctxr_dma,
+			      int ring, struct safexcel_request *request);
+int safexcel_init_ring_descriptors(struct safexcel_crypto_priv *priv,
+				   struct safexcel_ring *cdr,
+				   struct safexcel_ring *rdr);
+void safexcel_free_ring_descriptors(struct safexcel_crypto_priv *priv,
+				    struct safexcel_ring *cdr,
+				    struct safexcel_ring *rdr);
+int safexcel_select_ring(struct safexcel_crypto_priv *priv);
+void *safexcel_ring_next_rptr(struct safexcel_crypto_priv *priv,
+			      struct safexcel_ring *ring);
+void safexcel_ring_rollback_wptr(struct safexcel_crypto_priv *priv,
+				 struct safexcel_ring *ring);
+struct safexcel_command_desc *safexcel_add_cdesc(struct safexcel_crypto_priv *priv,
+						 int ring_id,
+						 bool first, bool last,
+						 phys_addr_t data, u32 len,
+						 u32 full_data_len,
+						 phys_addr_t context);
+struct safexcel_result_desc *safexcel_add_rdesc(struct safexcel_crypto_priv *priv,
+						 int ring_id,
+						bool first, bool last,
+						phys_addr_t data, u32 len);
+void safexcel_inv_complete(struct crypto_async_request *req, int error);
+
 #endif
-- 
1.7.9.5

