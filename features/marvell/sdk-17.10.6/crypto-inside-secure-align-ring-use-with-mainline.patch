From 33be95782917a521737ebea7b11d8459d1401857 Mon Sep 17 00:00:00 2001
From: Ofer Heifetz <oferh@marvell.com>
Date: Tue, 24 Oct 2017 21:50:24 +0300
Subject: [PATCH 1260/1345] crypto: inside-secure: align ring use with
 mainline

commit  54daacfa1bb3dc6b46bd4ff9b3bfef2906a4e3f0 from
https://github.com/MarvellEmbeddedProcessors/linux-marvell.git

simplify the code as in mainline, use local ring variable.

Change-Id: Icfacb99e1c3d9f3a355cf47df70ec633a768b777
Signed-off-by: Ofer Heifetz <oferh@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/45569
Tested-by: iSoC Platform CI <ykjenk@marvell.com>
Reviewed-by: Igal Liberman <igall@marvell.com>
Reviewed-by: Hanna Hawa <hannah@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/51641
Signed-off-by: Meng Li <Meng.Li@windriver.com>
---
 drivers/crypto/inside-secure/cipher.c |   36 ++++++++++++++++++---------------
 drivers/crypto/inside-secure/hash.c   |   36 ++++++++++++++++++---------------
 2 files changed, 40 insertions(+), 32 deletions(-)

diff --git a/drivers/crypto/inside-secure/cipher.c b/drivers/crypto/inside-secure/cipher.c
index 31a6f5c..9767e78 100644
--- a/drivers/crypto/inside-secure/cipher.c
+++ b/drivers/crypto/inside-secure/cipher.c
@@ -340,16 +340,17 @@ static int safexcel_handle_inv_result(struct safexcel_crypto_priv *priv,
 	}
 
 	ctx->base.ring = safexcel_select_ring(priv);
+	ring = ctx->base.ring;
 
-	spin_lock_bh(&priv->ring[ctx->base.ring].queue_lock);
-	enq_ret = ablkcipher_enqueue_request(&priv->ring[ctx->base.ring].queue, req);
-	spin_unlock_bh(&priv->ring[ctx->base.ring].queue_lock);
+	spin_lock_bh(&priv->ring[ring].queue_lock);
+	enq_ret = ablkcipher_enqueue_request(&priv->ring[ring].queue, req);
+	spin_unlock_bh(&priv->ring[ring].queue_lock);
 
 	if (enq_ret != -EINPROGRESS)
 		*ret = enq_ret;
 
-	queue_work(priv->ring[ctx->base.ring].workqueue,
-		   &priv->ring[ctx->base.ring].work_data.work);
+	queue_work(priv->ring[ring].workqueue,
+		   &priv->ring[ring].work_data.work);
 
 	*should_complete = false;
 
@@ -424,6 +425,7 @@ static int safexcel_cipher_exit_inv(struct crypto_tfm *tfm)
 	struct ablkcipher_request *req = (void *)__req_desc;
 	struct safexcel_cipher_req *sreq = ablkcipher_request_ctx(req);
 	struct safexcel_inv_result result = { 0 };
+	int ring = ctx->base.ring;
 	int ret;
 
 	memset(req, 0, sizeof(struct ablkcipher_request));
@@ -438,12 +440,12 @@ static int safexcel_cipher_exit_inv(struct crypto_tfm *tfm)
 	ctx->base.exit_inv = true;
 	sreq->needs_inv = true;
 
-	spin_lock_bh(&priv->ring[ctx->base.ring].queue_lock);
-	ret = ablkcipher_enqueue_request(&priv->ring[ctx->base.ring].queue, req);
-	spin_unlock_bh(&priv->ring[ctx->base.ring].queue_lock);
+	spin_lock_bh(&priv->ring[ring].queue_lock);
+	ret = ablkcipher_enqueue_request(&priv->ring[ring].queue, req);
+	spin_unlock_bh(&priv->ring[ring].queue_lock);
 
-	queue_work(priv->ring[ctx->base.ring].workqueue,
-		   &priv->ring[ctx->base.ring].work_data.work);
+	queue_work(priv->ring[ring].workqueue,
+		   &priv->ring[ring].work_data.work);
 
 	wait_for_completion_interruptible(&result.completion);
 
@@ -464,7 +466,7 @@ static int safexcel_aes(struct ablkcipher_request *req,
 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(req->base.tfm);
 	struct safexcel_cipher_req *sreq = ablkcipher_request_ctx(req);
 	struct safexcel_crypto_priv *priv = ctx->priv;
-	int ret;
+	int ret, ring;
 
 	sreq->direction = dir;
 	sreq->needs_inv = false;
@@ -492,12 +494,14 @@ static int safexcel_aes(struct ablkcipher_request *req,
 			return -ENOMEM;
 	}
 
-	spin_lock_bh(&priv->ring[ctx->base.ring].queue_lock);
-	ret = ablkcipher_enqueue_request(&priv->ring[ctx->base.ring].queue, req);
-	spin_unlock_bh(&priv->ring[ctx->base.ring].queue_lock);
+	ring = ctx->base.ring;
+
+	spin_lock_bh(&priv->ring[ring].queue_lock);
+	ret = ablkcipher_enqueue_request(&priv->ring[ring].queue, req);
+	spin_unlock_bh(&priv->ring[ring].queue_lock);
 
-	queue_work(priv->ring[ctx->base.ring].workqueue,
-		   &priv->ring[ctx->base.ring].work_data.work);
+	queue_work(priv->ring[ring].workqueue,
+		   &priv->ring[ring].work_data.work);
 
 	return ret;
 }
diff --git a/drivers/crypto/inside-secure/hash.c b/drivers/crypto/inside-secure/hash.c
index 3fc2981a..8cc7358 100644
--- a/drivers/crypto/inside-secure/hash.c
+++ b/drivers/crypto/inside-secure/hash.c
@@ -364,16 +364,17 @@ static int safexcel_handle_inv_result(struct safexcel_crypto_priv *priv,
 	}
 
 	ctx->base.ring = safexcel_select_ring(priv);
+	ring = ctx->base.ring;
 
-	spin_lock_bh(&priv->ring[ctx->base.ring].queue_lock);
-	enq_ret = ahash_enqueue_request(&priv->ring[ctx->base.ring].queue, areq);
-	spin_unlock_bh(&priv->ring[ctx->base.ring].queue_lock);
+	spin_lock_bh(&priv->ring[ring].queue_lock);
+	enq_ret = ahash_enqueue_request(&priv->ring[ring].queue, areq);
+	spin_unlock_bh(&priv->ring[ring].queue_lock);
 
 	if (enq_ret != -EINPROGRESS)
 		*ret = enq_ret;
 
-	queue_work(priv->ring[ctx->base.ring].workqueue,
-		   &priv->ring[ctx->base.ring].work_data.work);
+	queue_work(priv->ring[ring].workqueue,
+		   &priv->ring[ring].work_data.work);
 
 	*should_complete = false;
 
@@ -445,6 +446,7 @@ static int safexcel_ahash_exit_inv(struct crypto_tfm *tfm)
 	AHASH_REQUEST_ON_STACK(req, __crypto_ahash_cast(tfm));
 	struct safexcel_ahash_req *sreq = ahash_request_ctx(req);
 	struct safexcel_inv_result result = { 0 };
+	int ring = ctx->base.ring;
 	int ret;
 
 	memset(req, 0, sizeof(struct ahash_request));
@@ -459,12 +461,12 @@ static int safexcel_ahash_exit_inv(struct crypto_tfm *tfm)
 	ctx->base.exit_inv = true;
 	sreq->needs_inv = true;
 
-	spin_lock_bh(&priv->ring[ctx->base.ring].queue_lock);
-	ret = ahash_enqueue_request(&priv->ring[ctx->base.ring].queue, req);
-	spin_unlock_bh(&priv->ring[ctx->base.ring].queue_lock);
+	spin_lock_bh(&priv->ring[ring].queue_lock);
+	ret = ahash_enqueue_request(&priv->ring[ring].queue, req);
+	spin_unlock_bh(&priv->ring[ring].queue_lock);
 
-	queue_work(priv->ring[ctx->base.ring].workqueue,
-		   &priv->ring[ctx->base.ring].work_data.work);
+	queue_work(priv->ring[ring].workqueue,
+		   &priv->ring[ring].work_data.work);
 
 	wait_for_completion_interruptible(&result.completion);
 
@@ -483,7 +485,7 @@ static int safexcel_ahash_update(struct ahash_request *areq)
 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
 	struct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);
 	struct safexcel_crypto_priv *priv = ctx->priv;
-	int queued, len = req->len, ret;
+	int queued, len = req->len, ret, ring;
 	int cache_len = do_div(len, crypto_ahash_blocksize(ahash));
 
 	/*
@@ -590,12 +592,14 @@ static int safexcel_ahash_update(struct ahash_request *areq)
 			return -ENOMEM;
 	}
 
-	spin_lock_bh(&priv->ring[ctx->base.ring].queue_lock);
-	ret = ahash_enqueue_request(&priv->ring[ctx->base.ring].queue, areq);
-	spin_unlock_bh(&priv->ring[ctx->base.ring].queue_lock);
+	ring = ctx->base.ring;
 
-	queue_work(priv->ring[ctx->base.ring].workqueue,
-		   &priv->ring[ctx->base.ring].work_data.work);
+	spin_lock_bh(&priv->ring[ring].queue_lock);
+	ret = ahash_enqueue_request(&priv->ring[ring].queue, areq);
+	spin_unlock_bh(&priv->ring[ring].queue_lock);
+
+	queue_work(priv->ring[ring].workqueue,
+		   &priv->ring[ring].work_data.work);
 
 	return ret;
 }
-- 
1.7.9.5

